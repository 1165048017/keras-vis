<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Documentation for keras-vis, neural network visualization toolkit.">
  
  <link rel="shortcut icon" href="./img/favicon.ico">
  <title>Home - Keras-vis Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="./css/theme.css" type="text/css" />
  <link rel="stylesheet" href="./css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="./css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Home";
    var mkdocs_page_input_path = "index.md";
    var mkdocs_page_url = "/";
  </script>
  
  <script src="./js/jquery-2.1.1.min.js"></script>
  <script src="./js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="./js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="." class="icon icon-home"> Keras-vis Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href=".">Home</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#keras-visualization-toolkit">Keras Visualization Toolkit</a></li>
                
                    <li><a class="toctree-l4" href="#getting-started">Getting Started</a></li>
                
                    <li><a class="toctree-l4" href="#installation">Installation</a></li>
                
                    <li><a class="toctree-l4" href="#visualizations">Visualizations</a></li>
                
                    <li><a class="toctree-l4" href="#generating-animated-gif-of-optimization-progress">Generating animated gif of optimization progress</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Core API</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="vis.losses/">losses</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="vis.regularizers/">regularizers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="vis.optimizer/">optimizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="vis.visualization/">visualization</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Utils</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="vis.utils.utils/">utils</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="vis.utils.vggnet/">vggnet</a>
        
    </li>

        
    </ul>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href=".">Keras-vis Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".">Docs</a> &raquo;</li>
    
      
    
    <li>Home</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="keras-visualization-toolkit">Keras Visualization Toolkit</h1>
<p><a href="https://github.com/raghakot/keras-vis/blob/master/LICENSE"><img alt="license" src="https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000" /></a>
<a href="https://keras-vis.herokuapp.com/"><img alt="Slack" src="https://img.shields.io/badge/slack-discussion-E01563.svg" /></a></p>
<p>keras-vis is a high-level toolkit for visualizing input images via guided backprop. 
There are several repositories out there to visualize: </p>
<ul>
<li>Activation maximization</li>
<li>Saliency maps</li>
<li>Caricaturization (deep dream)</li>
<li>Texture/Artistic style transfer</li>
<li>Any other guided image backprop</li>
</ul>
<p>This toolkit generalizes all of the above as energy minimization problem. 
Compatible with both theano and tensorflow backends. </p>
<p>Read the documentation at <a href="https://raghakot.github.io/keras-vis">https://raghakot.github.io/keras-vis</a>. 
Join the slack channel for questions/discussions.</p>
<h2 id="getting-started">Getting Started</h2>
<p>In image backprop problems, the goal is to generate an input image that minimizes some loss function.
Setting up an image backprop problem is easy.</p>
<p><strong>Define weighted loss function</strong>
Various useful loss functions are defined in <a href="https://raghakot.github.io/keras-vis/vis.losses">losses</a>.
A custom loss function can be defined by implementing <a href="https://raghakot.github.io/keras-vis/vis.losses/#lossbuild_loss">Loss.build_loss</a>.</p>
<pre><code class="python">from vis.losses import ActivationMaximization
from vis.regularizers import TotalVariation, LPNorm

filter_indices = [1, 2, 3]

# Tuple consists of (loss_function, weight)
# Add regularizers as needed.
losses = [
    (ActivationMaximization(keras_layer, filter_indices), 1),
    (LPNorm(), 10),
    (TotalVariation(), 10)
]
</code></pre>

<p><strong>Configure optimizer to minimize weighted loss</strong>
In order to generate natural looking images, image search space is constrained using regularization penalties. 
Some common regularizers are defined in <a href="https://raghakot.github.io/keras-vis/vis.regularizers">regularizers</a>.
Like loss functions, custom regularizer can be defined by implementing 
<a href="https://raghakot.github.io/keras-vis/vis.losses/#lossbuild_loss">Loss.build_loss</a>.</p>
<pre><code class="python">from vis.optimizer import Optimizer

optimizer = Optimizer(img_input_layer, losses)
opt_img, grads = optimizer.minimize()
</code></pre>

<p>Concrete examples of various visualizations can be found in 
<a href="https://github.com/raghakot/keras-vis/tree/master/examples">examples folder</a>.</p>
<h2 id="installation">Installation</h2>
<p>1) Install <a href="https://github.com/fchollet/keras/blob/master/README.md#installation">keras</a> 
with theano or tensorflow backend
2) Install OpenCV </p>
<pre><code class="bash">sudo apt-get install python-opencv
</code></pre>

<p>3) Install keras-vis</p>
<blockquote>
<p>From sources</p>
</blockquote>
<pre><code class="bash">sudo python setup.py install
</code></pre>

<blockquote>
<p>PyPI package</p>
</blockquote>
<pre><code class="bash">sudo pip install keras-vis
</code></pre>

<h2 id="visualizations">Visualizations</h2>
<p>Neural nets are black boxes. How can we be sure that they are learning the right thing? If the neural net generates a
wrong prediction, how could we diagnose the issue? In the recent years, several approaches for understanding and 
visualizing Convolutional Networks have been developed in the literature.</p>
<h3 id="conv-filter-visualization">Conv filter visualization</h3>
<p>Each conv layer has several learned 'template matching' filters that maximize their output when a similar template 
pattern is found in the input image. This makes the first conv net layer highly interpretable by simply visualizing 
their weights as it is operating over raw pixels.</p>
<p>Subsequent conv filters operate over the outputs of previous conv filters (which indicate the presence or absence of 
some templates), so visualizing them directly is not very interpretable.</p>
<p>One way of interpreting them is to generate an input image that maximizes the filter output. With keras-vis, setting
this up is easy. Lets visualize the second conv layer of vggnet (named as 'block1_conv2').</p>
<pre><code class="python">import cv2
from vis.utils.vggnet import VGG16
from vis.visualization import visualize_activation

# Build the VGG16 network with ImageNet weights
model = VGG16(weights='imagenet', include_top=True)
print('Model loaded.')

# The name of the layer we want to visualize
# (see model definition in vggnet.py)
layer_name = 'block1_conv2'
layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])

vis_img = visualize_activation(model.input, layer_dict[layer_name])
cv2.imshow(layer_name, vis_img)
cv2.waitKey(0)
</code></pre>

<p>This generates the following stitched image representing input image(s) that maximize the filter_idx output.
They mostly seem to match for specific color and directional patterns.</p>
<p><img alt="block1_conv2 filters" src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/block1_conv2_filters.jpg?raw=true" title="conv_1 filters" /></p>
<p>Lets visualize the remaining conv filters (first few) by iterating over different <code>layer_name</code> values.</p>
<h4 id="block2_conv2-random-sample-of-the-128-filters">block2_conv2: random sample of the 128 filters</h4>
<p><img alt="block2_conv2 filters" src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/block2_conv2_filters.jpg?raw=true" title="conv_2 filters" /></p>
<h4 id="block3_conv3-random-sample-of-the-256-filters">block3_conv3: random sample of the 256 filters</h4>
<p><img alt="block3_conv3 filters" src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/block3_conv3_filters.jpg?raw=true" title="conv_3 filters" /></p>
<h4 id="block3_conv4-random-sample-of-the-512-filters">block3_conv4: random sample of the 512 filters</h4>
<p><img alt="block4_conv3 filters" src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/block4_conv3_filters.jpg?raw=true" title="conv_4 filters" /></p>
<h4 id="block3_conv5-random-sample-of-the-512-filters">block3_conv5: random sample of the 512 filters</h4>
<p><img alt="block5_conv3 filters" src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/block5_conv3_filters.jpg?raw=true" title="conv_5 filters" /></p>
<p>Some of the 'block5_conv3' filters failed to converge. This is because regularization losses (total variation and 
LP norm) are overtaking activation maximization loss (set <code>verbose=True</code> to observe). </p>
<p>Whenever activation maximization fails to converge, total variation regularization is the typical culprit. 
It is easier to minimize total variation from a random image (just have to create blobbier color structures), 
and this sets the input image in a bad local minima that makes it difficult to optimize for activation maximization. 
We can turn off total variation by setting <code>tv_weight=0</code>. This generates most of the previously unconverged filters.</p>
<p><img alt="block5_conv3 filters_no_tv" src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/block5_conv3_filters_no_tv.jpg?raw=true" title="conv_5 filters_no_tv" /></p>
<p>By this layer, we can clearly notice templates for complex patterns such as flower buds / corals 
(filters 67, 84 respectively). Notice that images are not as coherent due to lack of total variation loss.</p>
<p>A good strategy in these situations might be to seed the optimization with image output generated via tv_weight=0
and add the tv_weight back. Lets specifically look at filter 67.</p>
<pre><code class="python">layer_name = 'block5_conv3'

no_tv_seed_img = visualize_activation(model.input, layer_dict[layer_name], filter_indices=[67],
                                      tv_weight=0, verbose=True)
post_tv_img = visualize_activation(model.input, layer_dict[layer_name], filter_indices=[67],
                                   tv_weight=1, seed_img=no_tv_seed_img, verbose=True, max_iter=300)
cv2.imshow(layer_name, post_tv_img)
cv2.waitKey(0)
</code></pre>

<p>As expected, this generates a blobbier and smoother image:</p>
<p><img alt="filter_67" src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/filter_67.png?raw=true" title="filter_75" /></p>
<h3 id="dense-layer-visualization">Dense layer visualization</h3>
<p>Given an input image, conv net can classify whether it is a cat, bird etc. How can we be sure that it is capturing 
the correct notion of what it means to be a bird? Suppose that all the training images of 'bird' class contains a tree
with leaves. How do we know that the conv net is indeed 'looking' at the bird as opposed to leaves and classifying it 
as a bird?</p>
<p>One way to peer into the black box is to ask the reverse question - Generate an input image that maximizes the final
<code>Dense</code> layer output corresponding to bird class. Lets try this for 'ouzel' (imagenet output category: 20)</p>
<pre><code class="python">import cv2
from vis.utils.vggnet import VGG16
from vis.visualization import visualize_activation

# Build the VGG16 network with ImageNet weights
model = VGG16(weights='imagenet', include_top=True)
print('Model loaded.')

# The name of the layer we want to visualize
# (see model definition in vggnet.py)
layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])
layer_name = 'predictions'

# Generate three different images of the same output index.
img = visualize_activation(model.input, layer_dict[layer_name],
                           filter_indices=[20, 20, 20], max_iter=500)
cv2.imshow(layer_name, img)
cv2.waitKey(0)
</code></pre>

<p>and out comes this..</p>
<p><img alt="ouzel_vis" src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/ouzel_vis.png?raw=true" title="ouzel_vis" /></p>
<p>Not only has the conv net captured what it means to be an ouzel, but it also seems to encode for different orientations 
and scales, a further proof of rotational and scale invariance. </p>
<p>Lets do this for a few more random imagenet categories.</p>
<p><img alt="random_imagenet" src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/random_imagenet.png?raw=true" title="random_imagenet" /></p>
<p>If you squint really hard, we can sort of see that most images are more or less accurate representations of the 
corresponding class.</p>
<p>You might notice that in most of these visualizations, the same pattern tends to repeat all over the image 
with different orientations and scales. Why is this the case? If you think about it, it is essentially the consequence
of activation maximization loss. Multiple copies of 'template pattern' all over the image should increase the output value.</p>
<p>If we want more natural looking images, we need a better 'natural image' regularizer that penalizes this sort of 
behavior. Instead of hand crafting the regularizer, we can use the negative of 'discriminator' output probability 
(since we want to maximize probability that the image is real) of a generative adversarial network (GAN). </p>
<p>See this article for details about GANs in general: <a href="https://arxiv.org/abs/1511.06434">Unsupervised Representation Learning with Deep Convolutional 
Generative Adversarial Networks</a></p>
<p><em>GAN regularizer is currently a work in progress. Check back in a few days.</em></p>
<p>At this point, it might be fun to see the effect of total variation regularizer. The following images are generated with
<code>tv_weight=0</code></p>
<p><img alt="random_imagenet_no_tv" src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/random_imagenet_no_tv.png?raw=true" title="random_imagenet_no_tv" /></p>
<p>Total variation regularizer definitely helps in creating more natural looking images. I am excited to see what
a GAN discriminator could do.</p>
<h3 id="saliency-maps">Saliency Maps</h3>
<h3 id="more-will-be-added-soon-wip">More will be added soon (WIP...)</h3>
<h2 id="generating-animated-gif-of-optimization-progress">Generating animated gif of optimization progress</h2>
<p>It is possible to generate an animated gif of optimization progress. Below is an example for activation maximization
of 'ouzel' class (output_index: 20).</p>
<pre><code class="python">from vis.utils.vggnet import VGG16
from vis.optimizer import Optimizer
from vis.losses import ActivationMaximization
from vis.regularizers import TotalVariation, LPNorm

# Build the VGG16 network with ImageNet weights
model = VGG16(weights='imagenet', include_top=True)
print('Model loaded.')

# The name of the layer we want to visualize
# (see model definition in vggnet.py)
layer_name = 'predictions'
layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])
output_class = [20]

losses = [
    (ActivationMaximization(layer_dict[layer_name], output_class), 1),
    (LPNorm(), 10),
    (TotalVariation(), 1)
]
opt = Optimizer(model.input, losses)

# Jitter is used as a regularizer to create crisper images, but it makes gif animation ugly.
opt.minimize(max_iter=500, verbose=True, jitter=0,
             progress_gif_path='opt_progress')
</code></pre>

<p><img alt="opt_progress" src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/opt_progress.gif?raw=true" title="Optimization progress" /></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="vis.losses/" class="btn btn-neutral float-right" title="losses">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
      
        <span style="margin-left: 15px"><a href="vis.losses/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="./js/theme.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>

<!--
MkDocs version : 0.16.1
Build Date UTC : 2016-12-22 21:23:06
-->
