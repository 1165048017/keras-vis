{
    "docs": [
        {
            "location": "/", 
            "text": "Keras Visualization Toolkit\n\n\n\n\n\n\n\n\nkeras-vis is a high-level toolkit for visualizing and debugging your trained keras neural net models. Currently\nsupported visualizations include:\n\n\n\n\nActivation maximization\n\n\nSaliency maps\n\n\nClass activation maps\n\n\n\n\nAll visualizations by default support N-dimensional image inputs. i.e., it generalizes to N-dim image inputs \nto your model.\n\n\nThe toolkit generalizes all of the above as energy minimization problems with a clean, easy to use, \nand extendable interface. Compatible with both theano and tensorflow backends with 'channels_first', 'channels_last' \ndata format.\n\n\nQuick links\n\n\n\n\nRead the documentation at \nhttps://raghakot.github.io/keras-vis\n. \n\n\nJoin the slack \nchannel\n for questions/discussions.\n\n\nWe are tracking new features/tasks in \nwaffle.io\n. Would love it if you lend us \na hand and submit PRs.\n\n\n\n\nGetting Started\n\n\nIn image backprop problems, the goal is to generate an input image that minimizes some loss function.\nSetting up an image backprop problem is easy.\n\n\nDefine weighted loss function\n\n\nVarious useful loss functions are defined in \nlosses\n.\nA custom loss function can be defined by implementing \nLoss.build_loss\n.\n\n\nfrom vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\n\nfilter_indices = [1, 2, 3]\n\n# Tuple consists of (loss_function, weight)\n# Add regularizers as needed.\nlosses = [\n    (ActivationMaximization(keras_layer, filter_indices), 1),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]\n\n\n\n\nConfigure optimizer to minimize weighted loss\n\n\nIn order to generate natural looking images, image search space is constrained using regularization penalties. \nSome common regularizers are defined in \nregularizers\n.\nLike loss functions, custom regularizer can be defined by implementing \n\nLoss.build_loss\n.\n\n\nfrom vis.optimizer import Optimizer\n\noptimizer = Optimizer(model.input, losses)\nopt_img, grads, _ = optimizer.minimize()\n\n\n\n\nConcrete examples of various supported visualizations can be found in \n\nexamples folder\n.\n\n\nInstallation\n\n\n1) Install \nkeras\n \nwith theano or tensorflow backend. Note that this library requires keras \n 2.0\n\n\n2) Install OpenCV \n\n\nsudo apt-get install python-opencv\n\n\n\n\n3) Install keras-vis\n\n\n\n\nFrom sources\n\n\n\n\nsudo python setup.py install\n\n\n\n\n\n\nPyPI package\n\n\n\n\nsudo pip install keras-vis\n\n\n\n\nVisualizations\n\n\nNeural nets are black boxes. In the recent years, several approaches for understanding and visualizing Convolutional \nNetworks have been developed in the literature. They give us a way to peer into the black boxes, \ndiagnose mis-classifications, and assess whether the network is over/under fitting. \n\n\nGuided backprop can also be used to create \ntrippy art\n, neural/texture \n\nstyle transfer\n among the list of other growing applications.\n\n\nVarious visualizations, documented in their own pages, are summarized here.\n\n\n\n\n\nConv filter visualization\n\n\n\n\nConvolutional filters learn 'template matching' filters that maximize the output when a similar template \npattern is found in the input image. Visualize those templates via Activation Maximization.\n\n\n\n\n\nDense layer visualization\n\n\n\n\nHow can we assess whether a network is over/under fitting or generalizing well?\n\n\n\n\n\nAttention Maps\n\n\n\n\nHow can we assess whether a network is attending to correct parts of the image in order to generate a decision?\n\n\n\n\n\nGenerating animated gif of optimization progress\n\n\nIt is possible to generate an animated gif of optimization progress by leveraging \n\ncallbacks\n. Following example shows how to visualize the \nactivation maximization for 'ouzel' class (output_index: 20).\n\n\nfrom vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\nfrom vis.modifiers import Jitter\nfrom vis.optimizer import Optimizer\n\nfrom vis.callbacks import GifGenerator\nfrom vis.utils.vggnet import VGG16\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\noutput_class = [20]\n\nlosses = [\n    (ActivationMaximization(layer_dict[layer_name], output_class), 2),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]\nopt = Optimizer(model.input, losses)\nopt.minimize(max_iter=500, verbose=True, image_modifiers=[Jitter()], callbacks=[GifGenerator('opt_progress')])\n\n\n\n\n\nNotice how the output jitters around? This is because we used \nJitter\n, \na kind of \nImageModifier\n that is known to produce \ncrisper activation maximization images. As an exercise, try:\n\n\n\n\nWithout Jitter\n\n\nVarying various loss weights", 
            "title": "Home"
        }, 
        {
            "location": "/#keras-visualization-toolkit", 
            "text": "keras-vis is a high-level toolkit for visualizing and debugging your trained keras neural net models. Currently\nsupported visualizations include:   Activation maximization  Saliency maps  Class activation maps   All visualizations by default support N-dimensional image inputs. i.e., it generalizes to N-dim image inputs \nto your model.  The toolkit generalizes all of the above as energy minimization problems with a clean, easy to use, \nand extendable interface. Compatible with both theano and tensorflow backends with 'channels_first', 'channels_last' \ndata format.", 
            "title": "Keras Visualization Toolkit"
        }, 
        {
            "location": "/#quick-links", 
            "text": "Read the documentation at  https://raghakot.github.io/keras-vis .   Join the slack  channel  for questions/discussions.  We are tracking new features/tasks in  waffle.io . Would love it if you lend us \na hand and submit PRs.", 
            "title": "Quick links"
        }, 
        {
            "location": "/#getting-started", 
            "text": "In image backprop problems, the goal is to generate an input image that minimizes some loss function.\nSetting up an image backprop problem is easy.  Define weighted loss function  Various useful loss functions are defined in  losses .\nA custom loss function can be defined by implementing  Loss.build_loss .  from vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\n\nfilter_indices = [1, 2, 3]\n\n# Tuple consists of (loss_function, weight)\n# Add regularizers as needed.\nlosses = [\n    (ActivationMaximization(keras_layer, filter_indices), 1),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]  Configure optimizer to minimize weighted loss  In order to generate natural looking images, image search space is constrained using regularization penalties. \nSome common regularizers are defined in  regularizers .\nLike loss functions, custom regularizer can be defined by implementing  Loss.build_loss .  from vis.optimizer import Optimizer\n\noptimizer = Optimizer(model.input, losses)\nopt_img, grads, _ = optimizer.minimize()  Concrete examples of various supported visualizations can be found in  examples folder .", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#installation", 
            "text": "1) Install  keras  \nwith theano or tensorflow backend. Note that this library requires keras   2.0  2) Install OpenCV   sudo apt-get install python-opencv  3) Install keras-vis   From sources   sudo python setup.py install   PyPI package   sudo pip install keras-vis", 
            "title": "Installation"
        }, 
        {
            "location": "/#visualizations", 
            "text": "Neural nets are black boxes. In the recent years, several approaches for understanding and visualizing Convolutional \nNetworks have been developed in the literature. They give us a way to peer into the black boxes, \ndiagnose mis-classifications, and assess whether the network is over/under fitting.   Guided backprop can also be used to create  trippy art , neural/texture  style transfer  among the list of other growing applications.  Various visualizations, documented in their own pages, are summarized here.", 
            "title": "Visualizations"
        }, 
        {
            "location": "/#conv-filter-visualization", 
            "text": "Convolutional filters learn 'template matching' filters that maximize the output when a similar template \npattern is found in the input image. Visualize those templates via Activation Maximization.", 
            "title": "Conv filter visualization"
        }, 
        {
            "location": "/#dense-layer-visualization", 
            "text": "How can we assess whether a network is over/under fitting or generalizing well?", 
            "title": "Dense layer visualization"
        }, 
        {
            "location": "/#attention-maps", 
            "text": "How can we assess whether a network is attending to correct parts of the image in order to generate a decision?", 
            "title": "Attention Maps"
        }, 
        {
            "location": "/#generating-animated-gif-of-optimization-progress", 
            "text": "It is possible to generate an animated gif of optimization progress by leveraging  callbacks . Following example shows how to visualize the \nactivation maximization for 'ouzel' class (output_index: 20).  from vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\nfrom vis.modifiers import Jitter\nfrom vis.optimizer import Optimizer\n\nfrom vis.callbacks import GifGenerator\nfrom vis.utils.vggnet import VGG16\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\noutput_class = [20]\n\nlosses = [\n    (ActivationMaximization(layer_dict[layer_name], output_class), 2),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]\nopt = Optimizer(model.input, losses)\nopt.minimize(max_iter=500, verbose=True, image_modifiers=[Jitter()], callbacks=[GifGenerator('opt_progress')])  Notice how the output jitters around? This is because we used  Jitter , \na kind of  ImageModifier  that is known to produce \ncrisper activation maximization images. As an exercise, try:   Without Jitter  Varying various loss weights", 
            "title": "Generating animated gif of optimization progress"
        }, 
        {
            "location": "/visualizations/conv_filters/", 
            "text": "Overview\n\n\nEach conv layer has several learned 'template matching' filters that maximize their output when a similar template \npattern is found in the input image. This makes the first conv net layer highly interpretable by simply visualizing \ntheir weights as it is operating over raw pixels.\n\n\nSubsequent conv filters operate over the outputs of previous conv filters (which indicate the presence or absence of \nsome templates), so visualizing them directly is not very interpretable.\n\n\nOne way of interpreting them is to generate an input image that maximizes the filter output. With keras-vis, setting\nthis up is easy. Lets visualize the second conv layer of vggnet (named as 'block1_conv2').\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom vis.utils import utils\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_activation, get_num_filters\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'block1_conv2'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Visualize all filters in this layer.\nfilters = np.arange(get_num_filters(model.layers[layer_idx]))\n\n# Generate input image for each filter. Here `text` field is used to overlay `filter_value` on top of the image.\nvis_images = [visualize_activation(model, layer_idx, filter_indices=idx, text=str(idx))\n              for idx in filters]\n\n# Generate stitched image palette with 8 cols.\nstitched = utils.stitch_images(vis_images, cols=8)    \nplt.axis('off')\nplt.imshow(stitched)\nplt.title(layer_name)\nplt.show()\n\n\n\n\n\nThis generates the following stitched image representing input image(s) that maximize the filter_idx output.\nThey mostly seem to match for specific color and directional patterns.\n\n\n\n\nLets visualize the remaining conv filters (first few) by iterating over different \nlayer_name\n values.\n\n\nblock2_conv2: random sample of the 128 filters\n\n\n\n\nblock3_conv3: random sample of the 256 filters\n\n\n\n\nblock3_conv4: random sample of the 512 filters\n\n\n\n\nblock3_conv5: random sample of the 512 filters\n\n\n\n\nSome of the 'block5_conv3' filters failed to converge. This is because regularization losses (total variation and \nLP norm) are overtaking activation maximization loss (set \nverbose=True\n to observe). \n\n\nWhenever activation maximization fails to converge, total variation regularization is the typical culprit. \nIt is easier to minimize total variation from a random image (just have to create blobbier color structures), \nand this sets the input image in a bad local minima that makes it difficult to optimize for activation maximization. \nWe can turn off total variation by setting \ntv_weight=0\n. This generates most of the previously unconverged filters.\n\n\n\n\nBy this layer, we can clearly notice templates for complex patterns such as flower buds / corals \n(filters 67, 84 respectively). Notice that images are not as coherent due to lack of total variation loss.\n\n\nA good strategy in these situations might be to seed the optimization with image output generated via tv_weight=0\nand add the tv_weight back. Lets specifically look at filter 67.\n\n\nlayer_name = 'block5_conv3'\n\nno_tv_seed_img = visualize_activation(model, layer_idx, filter_indices=[67],\n                                      tv_weight=0, verbose=True)\npost_tv_img = visualize_activation(model, layer_idx, filter_indices=[67],\n                                   tv_weight=1, seed_img=no_tv_seed_img, verbose=True, max_iter=300)\nplt.axis('off')\nplt.imshow(post_tv_img)\nplt.title(layer_name)\nplt.show()\n\n\n\n\n\nAs expected, this generates a blobbier and smoother image:", 
            "title": "Convolutional Filters"
        }, 
        {
            "location": "/visualizations/conv_filters/#overview", 
            "text": "Each conv layer has several learned 'template matching' filters that maximize their output when a similar template \npattern is found in the input image. This makes the first conv net layer highly interpretable by simply visualizing \ntheir weights as it is operating over raw pixels.  Subsequent conv filters operate over the outputs of previous conv filters (which indicate the presence or absence of \nsome templates), so visualizing them directly is not very interpretable.  One way of interpreting them is to generate an input image that maximizes the filter output. With keras-vis, setting\nthis up is easy. Lets visualize the second conv layer of vggnet (named as 'block1_conv2').  import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom vis.utils import utils\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_activation, get_num_filters\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'block1_conv2'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Visualize all filters in this layer.\nfilters = np.arange(get_num_filters(model.layers[layer_idx]))\n\n# Generate input image for each filter. Here `text` field is used to overlay `filter_value` on top of the image.\nvis_images = [visualize_activation(model, layer_idx, filter_indices=idx, text=str(idx))\n              for idx in filters]\n\n# Generate stitched image palette with 8 cols.\nstitched = utils.stitch_images(vis_images, cols=8)    \nplt.axis('off')\nplt.imshow(stitched)\nplt.title(layer_name)\nplt.show()  This generates the following stitched image representing input image(s) that maximize the filter_idx output.\nThey mostly seem to match for specific color and directional patterns.   Lets visualize the remaining conv filters (first few) by iterating over different  layer_name  values.", 
            "title": "Overview"
        }, 
        {
            "location": "/visualizations/conv_filters/#block2_conv2-random-sample-of-the-128-filters", 
            "text": "", 
            "title": "block2_conv2: random sample of the 128 filters"
        }, 
        {
            "location": "/visualizations/conv_filters/#block3_conv3-random-sample-of-the-256-filters", 
            "text": "", 
            "title": "block3_conv3: random sample of the 256 filters"
        }, 
        {
            "location": "/visualizations/conv_filters/#block3_conv4-random-sample-of-the-512-filters", 
            "text": "", 
            "title": "block3_conv4: random sample of the 512 filters"
        }, 
        {
            "location": "/visualizations/conv_filters/#block3_conv5-random-sample-of-the-512-filters", 
            "text": "Some of the 'block5_conv3' filters failed to converge. This is because regularization losses (total variation and \nLP norm) are overtaking activation maximization loss (set  verbose=True  to observe).   Whenever activation maximization fails to converge, total variation regularization is the typical culprit. \nIt is easier to minimize total variation from a random image (just have to create blobbier color structures), \nand this sets the input image in a bad local minima that makes it difficult to optimize for activation maximization. \nWe can turn off total variation by setting  tv_weight=0 . This generates most of the previously unconverged filters.   By this layer, we can clearly notice templates for complex patterns such as flower buds / corals \n(filters 67, 84 respectively). Notice that images are not as coherent due to lack of total variation loss.  A good strategy in these situations might be to seed the optimization with image output generated via tv_weight=0\nand add the tv_weight back. Lets specifically look at filter 67.  layer_name = 'block5_conv3'\n\nno_tv_seed_img = visualize_activation(model, layer_idx, filter_indices=[67],\n                                      tv_weight=0, verbose=True)\npost_tv_img = visualize_activation(model, layer_idx, filter_indices=[67],\n                                   tv_weight=1, seed_img=no_tv_seed_img, verbose=True, max_iter=300)\nplt.axis('off')\nplt.imshow(post_tv_img)\nplt.title(layer_name)\nplt.show()  As expected, this generates a blobbier and smoother image:", 
            "title": "block3_conv5: random sample of the 512 filters"
        }, 
        {
            "location": "/visualizations/dense/", 
            "text": "Overview\n\n\nHow can we assess whether a network is over/under fitting or generalizing well? Given an input image, \nconv net can classify whether it is a cat, bird etc. How can we be sure that it is capturing the correct notion of \nwhat it means to be a bird?\n\n\nOne way to answer these questions is to pose the reverse question:\n\n\n\n\nGenerate an input image that maximizes the final \nDense\n layer output corresponding to bird class. \n\n\n\n\nLets try this for 'ouzel' (imagenet output category: 20)\n\n\nfrom matplotlib import pyplot as plt\n\nfrom vis.utils import utils\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_activation\n\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Generate three different images of the same output index.\nvis_images = [visualize_activation(model, layer_idx, filter_indices=idx, text=str(idx), max_iter=500)\n              for idx in [20, 20, 20]]\nstitched = utils.stitch_images(vis_images)    \nplt.axis('off')\nplt.imshow(stitched)\nplt.title(layer_name)\nplt.show()\n\n\n\n\n\nand out comes this..\n\n\n\n\nNot only has the conv net captured what it means to be an ouzel, but it also seems to encode for different orientations \nand scales, a further proof of rotational and scale invariance. \n\n\nLets do this for a few more random imagenet categories.\n\n\n\n\nIf you squint really hard, we can sort of see that most images are more or less accurate representations of the \ncorresponding class.\n\n\nGenerating more natural looking images\n\n\nYou might notice that in most of these visualizations, the same pattern tends to repeat all over the image \nwith different orientations and scales. Why is this the case? If you think about it, it is essentially the consequence\nof using \nActivationMaximization\n loss. Multiple copies of 'template pattern' \nall over the image would certainly maximize the output value.\n\n\nIf we want more natural looking images, we need a better \nnatural image prior\n. A natural image prior is something that\ncaptures the degree of naturalness. By default \nvisualize_activation\n uses:\n\n\n\n\nTotalVariation\n regularizer to Prefer blobbier images. i.e., not bobby images emit higher loss values.\n\n\nLPNorm\n regularizer to limit the color range.\n\n\n\n\nIf we set \ntv_weight=0\n, i.e., turn off total variation regularization, the following is generated:\n\n\n\n\nTotal variation regularizer definitely helps, but in order to get even more natural looking images, we need a better \nimage prior that penalizes unnatural images. Instead of hand-crafting these losses, perhaps the best approach is to \nuse a generative adversarial network (GAN) discriminator. A GAN discriminator is trained to emit probability that the \ninput image is real/fake. To learn more about GANs in general, read: \nUnsupervised Representation Learning with Deep Convolutional \nGenerative Adversarial Networks\n\n\nI am currently in the process of building a GAN regularizer. Stay tuned!", 
            "title": "Dense Layers"
        }, 
        {
            "location": "/visualizations/dense/#overview", 
            "text": "How can we assess whether a network is over/under fitting or generalizing well? Given an input image, \nconv net can classify whether it is a cat, bird etc. How can we be sure that it is capturing the correct notion of \nwhat it means to be a bird?  One way to answer these questions is to pose the reverse question:   Generate an input image that maximizes the final  Dense  layer output corresponding to bird class.    Lets try this for 'ouzel' (imagenet output category: 20)  from matplotlib import pyplot as plt\n\nfrom vis.utils import utils\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_activation\n\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Generate three different images of the same output index.\nvis_images = [visualize_activation(model, layer_idx, filter_indices=idx, text=str(idx), max_iter=500)\n              for idx in [20, 20, 20]]\nstitched = utils.stitch_images(vis_images)    \nplt.axis('off')\nplt.imshow(stitched)\nplt.title(layer_name)\nplt.show()  and out comes this..   Not only has the conv net captured what it means to be an ouzel, but it also seems to encode for different orientations \nand scales, a further proof of rotational and scale invariance.   Lets do this for a few more random imagenet categories.   If you squint really hard, we can sort of see that most images are more or less accurate representations of the \ncorresponding class.", 
            "title": "Overview"
        }, 
        {
            "location": "/visualizations/dense/#generating-more-natural-looking-images", 
            "text": "You might notice that in most of these visualizations, the same pattern tends to repeat all over the image \nwith different orientations and scales. Why is this the case? If you think about it, it is essentially the consequence\nof using  ActivationMaximization  loss. Multiple copies of 'template pattern' \nall over the image would certainly maximize the output value.  If we want more natural looking images, we need a better  natural image prior . A natural image prior is something that\ncaptures the degree of naturalness. By default  visualize_activation  uses:   TotalVariation  regularizer to Prefer blobbier images. i.e., not bobby images emit higher loss values.  LPNorm  regularizer to limit the color range.   If we set  tv_weight=0 , i.e., turn off total variation regularization, the following is generated:   Total variation regularizer definitely helps, but in order to get even more natural looking images, we need a better \nimage prior that penalizes unnatural images. Instead of hand-crafting these losses, perhaps the best approach is to \nuse a generative adversarial network (GAN) discriminator. A GAN discriminator is trained to emit probability that the \ninput image is real/fake. To learn more about GANs in general, read:  Unsupervised Representation Learning with Deep Convolutional \nGenerative Adversarial Networks  I am currently in the process of building a GAN regularizer. Stay tuned!", 
            "title": "Generating more natural looking images"
        }, 
        {
            "location": "/visualizations/attention/", 
            "text": "Overview\n\n\nSuppose that all the training images of 'bird' class contains a tree with leaves. How do we know that the conv net is \nindeed leveraging bird-related pixels as opposed to some other features such as the tree or leaves in the image. \n\n\nAttention maps are a family of methods that try to answer these questions by generating a heatmap over input \nimage that most contributed towards maximizing the probability of an output class.\n\n\nSaliency maps\n\n\nSaliency maps was first introduced in the paper: \n\nDeep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n\n\nThe idea is pretty simple. We compute the gradient of output category with respect to input image. This should tell us\nhow output category value changes with respect to a small change in input image pixels. All the positive values\nin the gradients tell us that a small change to that pixel will increase the output value. \nHence, visualizing these gradients, which are the same shape as the image should provide some intuition of attention.\n\n\nkeras-vis abstracts all of this under the hood with \nvisualize_saliency\n. Lets\ntry to visualize attention over images with: \ntiger, penguin, dumbbell, speedboat, spider\n. Note there is no guarantee\nthat these image urls haven't expired. Update them as needed.\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.imagenet_utils import preprocess_input\n\nfrom vis.utils import utils\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_saliency\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Images corresponding to tiger, penguin, dumbbell, speedboat, spider\nimage_paths = [\n    \nhttp://www.tigerfdn.com/wp-content/uploads/2016/05/How-Much-Does-A-Tiger-Weigh.jpg\n,\n    \nhttp://www.slate.com/content/dam/slate/articles/health_and_science/wild_things/2013/10/131025_WILD_AdeliePenguin.jpg.CROP.promo-mediumlarge.jpg\n,\n    \nhttps://www.kshs.org/cool2/graphics/dumbbell1lg.jpg\n,\n    \nhttp://tampaspeedboatadventures.com/wp-content/uploads/2010/10/DSC07011.jpg\n,\n    \nhttp://ichef-1.bbci.co.uk/news/660/cpsprodpb/1C24/production/_85540270_85540265.jpg\n\n]\n\nheatmaps = []\nfor path in image_paths:\n    seed_img = utils.load_img(path, target_size=(224, 224))\n    x = np.expand_dims(img_to_array(seed_img), axis=0)\n    x = preprocess_input(x)\n    pred_class = np.argmax(model.predict(x))\n\n    # Here we are asking it to show attention such that prob of `pred_class` is maximized.\n    heatmap = visualize_saliency(model, layer_idx, [pred_class], seed_img)\n    heatmaps.append(heatmap)\n\nplt.axis('off')\nplt.imshow(utils.stitch_images(heatmaps))\nplt.title('Saliency map')\nplt.show()\n\n\n\n\n\nThis generates heatmaps overlayed on top of images. overlay can be turned off using \noverlay=False\n.\n\n\n\n\nThis mostly looks pretty accurate! Note that the heatmap looks pretty sparse as the \nDense\n layers destroys a lot of spatial \ninformation. This visualization should look a lot better if we used 1 X 1 convolutions to \n\nmimic\n the dense layer.\n\n\nClass activation maps\n\n\nAs you might expect, since the inception of saliency maps by Simonyan et al, various other techniques have been developed \nto improve upon these visualizations. One problem with saliency maps is that it is not class discriminative; i.e., there\nis some overlap in heatmaps between, say the 'dog' and 'cat' class. Notable methods to solve this problem includes:  \n\n\n\n\nOcculusion maps\n\n\nClass Activation maps\n\n\n\n\nIn keras-vis, we however adopt the \ngrad-CAM\n method as it solves the inefficiency\nproblem with occlusion maps and architectural constraint problem with CAM.\n\n\nGenerating grad-CAM visualization is simple, just replace \nvisualize_saliency\n with \n\nvisualize_cam\n in the above code. This generates the following:\n\n\n\n\nCompared to saliency, notice how this excludes the spider the in \nspider_web\n prediction. I personally feel that grad-CAM \nis more helpful in diagnosing issues with conv-nets, especially for Kaggle competitions.", 
            "title": "Attention Maps"
        }, 
        {
            "location": "/visualizations/attention/#overview", 
            "text": "Suppose that all the training images of 'bird' class contains a tree with leaves. How do we know that the conv net is \nindeed leveraging bird-related pixels as opposed to some other features such as the tree or leaves in the image.   Attention maps are a family of methods that try to answer these questions by generating a heatmap over input \nimage that most contributed towards maximizing the probability of an output class.", 
            "title": "Overview"
        }, 
        {
            "location": "/visualizations/attention/#saliency-maps", 
            "text": "Saliency maps was first introduced in the paper:  Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps  The idea is pretty simple. We compute the gradient of output category with respect to input image. This should tell us\nhow output category value changes with respect to a small change in input image pixels. All the positive values\nin the gradients tell us that a small change to that pixel will increase the output value. \nHence, visualizing these gradients, which are the same shape as the image should provide some intuition of attention.  keras-vis abstracts all of this under the hood with  visualize_saliency . Lets\ntry to visualize attention over images with:  tiger, penguin, dumbbell, speedboat, spider . Note there is no guarantee\nthat these image urls haven't expired. Update them as needed.  import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.imagenet_utils import preprocess_input\n\nfrom vis.utils import utils\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_saliency\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Images corresponding to tiger, penguin, dumbbell, speedboat, spider\nimage_paths = [\n     http://www.tigerfdn.com/wp-content/uploads/2016/05/How-Much-Does-A-Tiger-Weigh.jpg ,\n     http://www.slate.com/content/dam/slate/articles/health_and_science/wild_things/2013/10/131025_WILD_AdeliePenguin.jpg.CROP.promo-mediumlarge.jpg ,\n     https://www.kshs.org/cool2/graphics/dumbbell1lg.jpg ,\n     http://tampaspeedboatadventures.com/wp-content/uploads/2010/10/DSC07011.jpg ,\n     http://ichef-1.bbci.co.uk/news/660/cpsprodpb/1C24/production/_85540270_85540265.jpg \n]\n\nheatmaps = []\nfor path in image_paths:\n    seed_img = utils.load_img(path, target_size=(224, 224))\n    x = np.expand_dims(img_to_array(seed_img), axis=0)\n    x = preprocess_input(x)\n    pred_class = np.argmax(model.predict(x))\n\n    # Here we are asking it to show attention such that prob of `pred_class` is maximized.\n    heatmap = visualize_saliency(model, layer_idx, [pred_class], seed_img)\n    heatmaps.append(heatmap)\n\nplt.axis('off')\nplt.imshow(utils.stitch_images(heatmaps))\nplt.title('Saliency map')\nplt.show()  This generates heatmaps overlayed on top of images. overlay can be turned off using  overlay=False .   This mostly looks pretty accurate! Note that the heatmap looks pretty sparse as the  Dense  layers destroys a lot of spatial \ninformation. This visualization should look a lot better if we used 1 X 1 convolutions to  mimic  the dense layer.", 
            "title": "Saliency maps"
        }, 
        {
            "location": "/visualizations/attention/#class-activation-maps", 
            "text": "As you might expect, since the inception of saliency maps by Simonyan et al, various other techniques have been developed \nto improve upon these visualizations. One problem with saliency maps is that it is not class discriminative; i.e., there\nis some overlap in heatmaps between, say the 'dog' and 'cat' class. Notable methods to solve this problem includes:     Occulusion maps  Class Activation maps   In keras-vis, we however adopt the  grad-CAM  method as it solves the inefficiency\nproblem with occlusion maps and architectural constraint problem with CAM.  Generating grad-CAM visualization is simple, just replace  visualize_saliency  with  visualize_cam  in the above code. This generates the following:   Compared to saliency, notice how this excludes the spider the in  spider_web  prediction. I personally feel that grad-CAM \nis more helpful in diagnosing issues with conv-nets, especially for Kaggle competitions.", 
            "title": "Class activation maps"
        }, 
        {
            "location": "/vis.losses/", 
            "text": "Source:\n \nvis/losses.py#L0\n\n\n\n\nLoss\n\n\nAbstract class for defining the loss function to be minimized.\nThe loss function should be built by defining \nbuild_loss\n function.\n\n\nThe attribute \nname\n should be defined to identify loss function with verbose outputs.\nDefaults to 'Unnamed Loss' if not overridden.\n\n\n\n\nLoss.\n__init__\n\n\n__init__(self)\n\n\n\n\n\n\nLoss.build_loss\n\n\nbuild_loss(self)\n\n\n\n\nImplement this function to build the loss function expression. \nAny additional arguments required to build this loss function may be passed in via \n__init__\n.\n\n\nIdeally, the function expression must be compatible with both theano/tensorflow backends with\n'th' or 'tf' image dim ordering. \nutils.slicer\n can be used to define backend agnostic slices\n(just define it for theano, it will automatically shuffle indices for tensorflow).\n\n\n# theano slice\nconv_layer[:, filter_idx, ...]\n\n# TF slice\nconv_layer[..., filter_idx]\n\n# Backend agnostic slice\nconv_layer[utils.slicer[:, filter_idx, ...]]\n\n\n\n\nutils.get_img_shape\n and\n\nutils.get_img_indices\n are other optional utilities that make this easier.\n\n\nReturns:\n\n\nThe loss expression.\n\n\n\n\nActivationMaximization\n\n\nA loss function that maximizes the activation of a set of filters within a particular layer.\n\n\nTypically this loss is used to ask the reverse question - What kind of input image would increase the networks\nconfidence, for say, dog class. This helps determine what the network might be internalizing as being the 'dog'\nimage space.\n\n\nOne might also use this to generate an input image that maximizes both 'dog' and 'human' outputs on the final\n\nkeras.layers.Dense\n layer.\n\n\n\n\nActivationMaximization.\n__init__\n\n\n__init__(self, layer, filter_indices)\n\n\n\n\nArgs:\n\n\n\n\nlayer\n:  The keras layer whose filters need to be maximized. This can either be a convolutional layer\n  or a dense layer.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  For \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\n\n\nIf you are optimizing final \nkeras.layers.Dense\n layer to maximize class output, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nActivationMaximization.build_loss\n\n\nbuild_loss(self)", 
            "title": "losses"
        }, 
        {
            "location": "/vis.losses/#loss", 
            "text": "Abstract class for defining the loss function to be minimized.\nThe loss function should be built by defining  build_loss  function.  The attribute  name  should be defined to identify loss function with verbose outputs.\nDefaults to 'Unnamed Loss' if not overridden.", 
            "title": "Loss"
        }, 
        {
            "location": "/vis.losses/#loss__init__", 
            "text": "__init__(self)", 
            "title": "Loss.__init__"
        }, 
        {
            "location": "/vis.losses/#lossbuild_loss", 
            "text": "build_loss(self)  Implement this function to build the loss function expression. \nAny additional arguments required to build this loss function may be passed in via  __init__ .  Ideally, the function expression must be compatible with both theano/tensorflow backends with\n'th' or 'tf' image dim ordering.  utils.slicer  can be used to define backend agnostic slices\n(just define it for theano, it will automatically shuffle indices for tensorflow).  # theano slice\nconv_layer[:, filter_idx, ...]\n\n# TF slice\nconv_layer[..., filter_idx]\n\n# Backend agnostic slice\nconv_layer[utils.slicer[:, filter_idx, ...]]  utils.get_img_shape  and utils.get_img_indices  are other optional utilities that make this easier.  Returns:  The loss expression.", 
            "title": "Loss.build_loss"
        }, 
        {
            "location": "/vis.losses/#activationmaximization", 
            "text": "A loss function that maximizes the activation of a set of filters within a particular layer.  Typically this loss is used to ask the reverse question - What kind of input image would increase the networks\nconfidence, for say, dog class. This helps determine what the network might be internalizing as being the 'dog'\nimage space.  One might also use this to generate an input image that maximizes both 'dog' and 'human' outputs on the final keras.layers.Dense  layer.", 
            "title": "ActivationMaximization"
        }, 
        {
            "location": "/vis.losses/#activationmaximization__init__", 
            "text": "__init__(self, layer, filter_indices)  Args:   layer :  The keras layer whose filters need to be maximized. This can either be a convolutional layer\n  or a dense layer.  filter_indices :  filter indices within the layer to be maximized.\n  For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.   If you are optimizing final  keras.layers.Dense  layer to maximize class output, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.", 
            "title": "ActivationMaximization.__init__"
        }, 
        {
            "location": "/vis.losses/#activationmaximizationbuild_loss", 
            "text": "build_loss(self)", 
            "title": "ActivationMaximization.build_loss"
        }, 
        {
            "location": "/vis.regularizers/", 
            "text": "Source:\n \nvis/regularizers.py#L0\n\n\n\n\nnormalize\n\n\nnormalize(img, tensor)\n\n\n\n\nNormalizes the tensor with respect to image dimensions. This makes regularizer weight factor more or less\nuniform across various input image dimensions.\n\n\nArgs:\n\n\n\n\nimg\n:  An tensor of shape: \n(samples, channels, image_dims...)\n if data_format='channels_first' or\n  \n(samples, image_dims..., channels)\n if data_format='channels_last'.\n\n\ntensor\n:  The tensor to normalize\n\n\n\n\nReturns:\n\n\nThe normalized tensor.\n\n\n\n\nTotalVariation\n\n\n\n\nTotalVariation.\n__init__\n\n\n__init__(self, img_input, beta=2.0)\n\n\n\n\nTotal variation regularizer encourages blobbier and coherent image structures, akin to natural images.\nSee \nsection 3.2.2\n in\n\nVisualizing deep convolutional neural networks using natural pre-images\n\nfor details.\n\n\nArgs:\n\n\n\n\nimg_input\n:  An image tensor of shape: \n(samples, channels, image_dims...)\n if data_format='channels_first' or\n  \n(samples, image_dims..., channels)\n if data_format='channels_last'.\n\n\nbeta\n:  Smaller values of beta give sharper but 'spikier' images.\n  Values \n\\in [1.5, 3.0]\n are recommended as a reasonable compromise. (Default value = 2.)\n\n\n\n\n\n\nTotalVariation.build_loss\n\n\nbuild_loss(self)\n\n\n\n\nImplements the N-dim version of function\n\nTV^{\\beta}(x) = \\sum_{whc} \\left ( \\left ( x(h, w+1, c) - x(h, w, c) \\right )^{2} +\n\\left ( x(h+1, w, c) - x(h, w, c) \\right )^{2} \\right )^{\\frac{\\beta}{2}}\n\nto return total variation for all images in the batch.\n\n\n\n\nLPNorm\n\n\n\n\nLPNorm.\n__init__\n\n\n__init__(self, img_input, p=6.0)\n\n\n\n\nBuilds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded.\n  i.e., prevents pixels from taking on very large values.\n\n\nArgs:\n\n\n\n\nimg_input\n:  4D image input tensor to the model of shape: \n(samples, channels, rows, cols)\n\n  if data_format='channels_first' or \n(samples, rows, cols, channels)\n if data_format='channels_last'.\n\n\np\n:  The pth norm to use. If p = float('inf'), infinity-norm will be used.\n\n\n\n\n\n\nLPNorm.build_loss\n\n\nbuild_loss(self)", 
            "title": "regularizers"
        }, 
        {
            "location": "/vis.regularizers/#normalize", 
            "text": "normalize(img, tensor)  Normalizes the tensor with respect to image dimensions. This makes regularizer weight factor more or less\nuniform across various input image dimensions.  Args:   img :  An tensor of shape:  (samples, channels, image_dims...)  if data_format='channels_first' or\n   (samples, image_dims..., channels)  if data_format='channels_last'.  tensor :  The tensor to normalize   Returns:  The normalized tensor.", 
            "title": "normalize"
        }, 
        {
            "location": "/vis.regularizers/#totalvariation", 
            "text": "", 
            "title": "TotalVariation"
        }, 
        {
            "location": "/vis.regularizers/#totalvariation__init__", 
            "text": "__init__(self, img_input, beta=2.0)  Total variation regularizer encourages blobbier and coherent image structures, akin to natural images.\nSee  section 3.2.2  in Visualizing deep convolutional neural networks using natural pre-images \nfor details.  Args:   img_input :  An image tensor of shape:  (samples, channels, image_dims...)  if data_format='channels_first' or\n   (samples, image_dims..., channels)  if data_format='channels_last'.  beta :  Smaller values of beta give sharper but 'spikier' images.\n  Values  \\in [1.5, 3.0]  are recommended as a reasonable compromise. (Default value = 2.)", 
            "title": "TotalVariation.__init__"
        }, 
        {
            "location": "/vis.regularizers/#totalvariationbuild_loss", 
            "text": "build_loss(self)  Implements the N-dim version of function TV^{\\beta}(x) = \\sum_{whc} \\left ( \\left ( x(h, w+1, c) - x(h, w, c) \\right )^{2} +\n\\left ( x(h+1, w, c) - x(h, w, c) \\right )^{2} \\right )^{\\frac{\\beta}{2}} \nto return total variation for all images in the batch.", 
            "title": "TotalVariation.build_loss"
        }, 
        {
            "location": "/vis.regularizers/#lpnorm", 
            "text": "", 
            "title": "LPNorm"
        }, 
        {
            "location": "/vis.regularizers/#lpnorm__init__", 
            "text": "__init__(self, img_input, p=6.0)  Builds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded.\n  i.e., prevents pixels from taking on very large values.  Args:   img_input :  4D image input tensor to the model of shape:  (samples, channels, rows, cols) \n  if data_format='channels_first' or  (samples, rows, cols, channels)  if data_format='channels_last'.  p :  The pth norm to use. If p = float('inf'), infinity-norm will be used.", 
            "title": "LPNorm.__init__"
        }, 
        {
            "location": "/vis.regularizers/#lpnormbuild_loss", 
            "text": "build_loss(self)", 
            "title": "LPNorm.build_loss"
        }, 
        {
            "location": "/vis.modifiers/", 
            "text": "Source:\n \nvis/modifiers.py#L0\n\n\n\n\nImageModifier\n\n\nAbstract class for defining an image modifier. An image modifier can be used with the\n\nOptimizer.minimize\n to make \npre\n and \npost\n image changes with the\ngradient descent update step.\n\n\nmodifier.pre(img)\n# gradient descent update to img\nmodifier.post(img)\n\n\n\n\n\n\nImageModifier.post\n\n\npost(self, img)\n\n\n\n\nImplement post gradient descent update modification to the image. If post-processing is not desired,\nsimply ignore the implementation. It returns the unmodified \nimg\n by default.\n\n\nArgs:\n\n\n\n\nimg\n:  An N-dim numpy array of shape: \n(samples, channels, image_dims...)\n if data_format='channels_first' or\n  \n(samples, image_dims..., channels)\n if data_format='channels_last'.\n\n\n\n\nReturns:\n\n\nThe modified post image.\n\n\n\n\nImageModifier.pre\n\n\npre(self, img)\n\n\n\n\nImplement pre gradient descent update modification to the image. If pre-processing is not desired,\nsimply ignore the implementation. It returns the unmodified \nimg\n by default.\n\n\nArgs:\n\n\n\n\nimg\n:  An N-dim numpy array of shape: \n(samples, channels, image_dims...)\n if data_format='channels_first' or\n  \n(samples, image_dims..., channels)\n if data_format='channels_last'.\n\n\n\n\nReturns:\n\n\nThe modified pre image.\n\n\n\n\nJitter\n\n\n\n\nJitter.\n__init__\n\n\n__init__(self, jitter=16)\n\n\n\n\nImplements an image modifier that introduces random jitter in \npre\n and undoes in \npost\n.\nJitter has been shown to produce crisper activation maximization images.\n\n\nArgs:\n\n\n\n\njitter\n:  Number of pixels to jitter in rows and cols dimensions.\n\n\n\n\n\n\nJitter.post\n\n\npost(self, img)\n\n\n\n\nImplement post gradient descent update modification to the image. If post-processing is not desired,\nsimply ignore the implementation. It returns the unmodified \nimg\n by default.\n\n\nArgs:\n\n\n\n\nimg\n:  An N-dim numpy array of shape: \n(samples, channels, image_dims...)\n if data_format='channels_first' or\n  \n(samples, image_dims..., channels)\n if data_format='channels_last'.\n\n\n\n\nReturns:\n\n\nThe modified post image.\n\n\n\n\nJitter.pre\n\n\npre(self, img)", 
            "title": "modifiers"
        }, 
        {
            "location": "/vis.modifiers/#imagemodifier", 
            "text": "Abstract class for defining an image modifier. An image modifier can be used with the Optimizer.minimize  to make  pre  and  post  image changes with the\ngradient descent update step.  modifier.pre(img)\n# gradient descent update to img\nmodifier.post(img)", 
            "title": "ImageModifier"
        }, 
        {
            "location": "/vis.modifiers/#imagemodifierpost", 
            "text": "post(self, img)  Implement post gradient descent update modification to the image. If post-processing is not desired,\nsimply ignore the implementation. It returns the unmodified  img  by default.  Args:   img :  An N-dim numpy array of shape:  (samples, channels, image_dims...)  if data_format='channels_first' or\n   (samples, image_dims..., channels)  if data_format='channels_last'.   Returns:  The modified post image.", 
            "title": "ImageModifier.post"
        }, 
        {
            "location": "/vis.modifiers/#imagemodifierpre", 
            "text": "pre(self, img)  Implement pre gradient descent update modification to the image. If pre-processing is not desired,\nsimply ignore the implementation. It returns the unmodified  img  by default.  Args:   img :  An N-dim numpy array of shape:  (samples, channels, image_dims...)  if data_format='channels_first' or\n   (samples, image_dims..., channels)  if data_format='channels_last'.   Returns:  The modified pre image.", 
            "title": "ImageModifier.pre"
        }, 
        {
            "location": "/vis.modifiers/#jitter", 
            "text": "", 
            "title": "Jitter"
        }, 
        {
            "location": "/vis.modifiers/#jitter__init__", 
            "text": "__init__(self, jitter=16)  Implements an image modifier that introduces random jitter in  pre  and undoes in  post .\nJitter has been shown to produce crisper activation maximization images.  Args:   jitter :  Number of pixels to jitter in rows and cols dimensions.", 
            "title": "Jitter.__init__"
        }, 
        {
            "location": "/vis.modifiers/#jitterpost", 
            "text": "post(self, img)  Implement post gradient descent update modification to the image. If post-processing is not desired,\nsimply ignore the implementation. It returns the unmodified  img  by default.  Args:   img :  An N-dim numpy array of shape:  (samples, channels, image_dims...)  if data_format='channels_first' or\n   (samples, image_dims..., channels)  if data_format='channels_last'.   Returns:  The modified post image.", 
            "title": "Jitter.post"
        }, 
        {
            "location": "/vis.modifiers/#jitterpre", 
            "text": "pre(self, img)", 
            "title": "Jitter.pre"
        }, 
        {
            "location": "/vis.optimizer/", 
            "text": "Source:\n \nvis/optimizer.py#L0\n\n\n\n\nOptimizer\n\n\n\n\nOptimizer.\n__init__\n\n\n__init__(self, img_input, losses, wrt=None, norm_grads=True)\n\n\n\n\nCreates an optimizer that minimizes weighted loss function.\n\n\nArgs:\n\n\n\n\nimg_input\n:  An image input tensor of shape: \n(samples, channels, image_dims...)\n if \n  data_format='channels_first' or \n(samples, image_dims..., channels)\n if data_format='channels_last'.\n\n\nlosses\n:  List of (\nLoss\n, weight) tuples.\n\n\nwrt\n:  Short for, with respect to. This instructs the optimizer that the aggregate loss from \nlosses\n\n  should be minimized with respect to \nwrt\n. \nwrt\n can be any tensor that is part of the model graph.\n  Default value is set to None which means that loss will simply be minimized with respect to \nimg_input\n.\n\n\nnorm_grads\n:  True to normalize gradients. Normalization avoids very small or large gradients and ensures \n  a smooth gradient gradient descent process. If you want the actual gradient, set this to false.\n\n\n\n\n\n\nOptimizer.minimize\n\n\nminimize(self, seed_img=None, max_iter=200, image_modifiers=None, callbacks=None, verbose=True)\n\n\n\n\nPerforms gradient descent on the input image with respect to defined losses.\n\n\nArgs:\n\n\n\n\nseed_img\n:  An N-dim numpy array of shape: \n(samples, channels, image_dims...)\n \n  if data_format='channels_first' or \n(samples, image_dims..., channels)\n if data_format='channels_last'. \n  Seeded with random noise if set to None. (Default value = None)\n\n\nmax_iter\n:  The maximum number of gradient descent iterations. (Default value = 200)\n\n\nimage_modifiers\n:  A list of \n../vis/modifiers/#ImageModifier\n instances specifying \npre\n and\n  \npost\n image processing steps with the gradient descent update step. \npre\n is applied in list order while\n  \npost\n is applied in reverse order. For example, \nimage_modifiers = [f, g]\n means that\n  \npre_img = g(f(img))\n and \npost_img = f(g(img))\n\n\ncallbacks\n:  A list of \n../vis/callbacks/#OptimizerCallback\n to trigger during optimization.\n\n\nverbose\n:  Logs individual losses at the end of every gradient descent iteration.\n  Very useful to estimate loss weight factor. (Default value = True)\n\n\n\n\nReturns:\n\n\nThe tuple of \n(optimized_image, grads with respect to wrt, wrt_value)\n after gradient descent iterations.", 
            "title": "optimizer"
        }, 
        {
            "location": "/vis.optimizer/#optimizer", 
            "text": "", 
            "title": "Optimizer"
        }, 
        {
            "location": "/vis.optimizer/#optimizer__init__", 
            "text": "__init__(self, img_input, losses, wrt=None, norm_grads=True)  Creates an optimizer that minimizes weighted loss function.  Args:   img_input :  An image input tensor of shape:  (samples, channels, image_dims...)  if \n  data_format='channels_first' or  (samples, image_dims..., channels)  if data_format='channels_last'.  losses :  List of ( Loss , weight) tuples.  wrt :  Short for, with respect to. This instructs the optimizer that the aggregate loss from  losses \n  should be minimized with respect to  wrt .  wrt  can be any tensor that is part of the model graph.\n  Default value is set to None which means that loss will simply be minimized with respect to  img_input .  norm_grads :  True to normalize gradients. Normalization avoids very small or large gradients and ensures \n  a smooth gradient gradient descent process. If you want the actual gradient, set this to false.", 
            "title": "Optimizer.__init__"
        }, 
        {
            "location": "/vis.optimizer/#optimizerminimize", 
            "text": "minimize(self, seed_img=None, max_iter=200, image_modifiers=None, callbacks=None, verbose=True)  Performs gradient descent on the input image with respect to defined losses.  Args:   seed_img :  An N-dim numpy array of shape:  (samples, channels, image_dims...)  \n  if data_format='channels_first' or  (samples, image_dims..., channels)  if data_format='channels_last'. \n  Seeded with random noise if set to None. (Default value = None)  max_iter :  The maximum number of gradient descent iterations. (Default value = 200)  image_modifiers :  A list of  ../vis/modifiers/#ImageModifier  instances specifying  pre  and\n   post  image processing steps with the gradient descent update step.  pre  is applied in list order while\n   post  is applied in reverse order. For example,  image_modifiers = [f, g]  means that\n   pre_img = g(f(img))  and  post_img = f(g(img))  callbacks :  A list of  ../vis/callbacks/#OptimizerCallback  to trigger during optimization.  verbose :  Logs individual losses at the end of every gradient descent iteration.\n  Very useful to estimate loss weight factor. (Default value = True)   Returns:  The tuple of  (optimized_image, grads with respect to wrt, wrt_value)  after gradient descent iterations.", 
            "title": "Optimizer.minimize"
        }, 
        {
            "location": "/vis.visualization/", 
            "text": "Source:\n \nvis/visualization.py#L0\n\n\n\n\nget_num_filters\n\n\nget_num_filters(layer)\n\n\n\n\nReturns: Total number of filters within \nlayer\n.\n\n\nFor \nkeras.layers.Dense\n layer, this is the total number of outputs.\n\n\n\n\nvisualize_activation\n\n\nvisualize_activation(model, layer_idx, filter_indices=None, seed_img=None, text=None, \\\n    act_max_weight=1, lp_norm_weight=10, tv_weight=10, **optimizer_params)\n\n\n\n\nGenerates stitched input image(s) over all \nfilter_indices\n in the given \nlayer\n that maximize\nthe filter output activation.\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance. The model input shape must be: \n(samples, channels, image_dims...)\n \n  if data_format='channels_first' or \n(samples, image_dims..., channels)\n if data_format='channels_last'.\n\n\nlayer_idx\n:  The layer index within \nmodel.layers\n whose filters needs to be visualized.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  For \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\n\n\nIf you are visualizing final \nkeras.layers.Dense\n layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\nfilter indices within the layer to be maximized.\n  If None, all filters are visualized. (Default value = None)\n\n\nAn input image is generated for each entry in \nfilter_indices\n. The entry can also be an array.\n  For example, \nfilter_indices = [[1, 2], 3, [4, 5, 6]]\n would generate three input images. The first one\n  would maximize output of filters 1, 2, 3 jointly. A fun use of this might be to generate a dog-fish\n  image by maximizing 'dog' and 'fish' output in final \nDense\n layer.\n\n\nFor \nkeras.layers.Dense\n layers, \nfilter_idx\n is interpreted as the output index.\n\n\nIf you are visualizing final \nkeras.layers.Dense\n layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nseed_img\n:  Seeds the optimization with a starting image. Initialized with a random value when set to None.\n  (Default value = None)\n\n\ntext\n:  The text to overlay on top of the generated image. (Default Value = None)\n\n\nact_max_weight\n:  The weight param for \nActivationMaximization\n loss. Not used if 0 or None. (Default value = 1)\n\n\nlp_norm_weight\n:  The weight param for \nLPNorm\n regularization loss. Not used if 0 or None. (Default value = 10)\n\n\ntv_weight\n:  The weight param for \nTotalVariation\n regularization loss. Not used if 0 or None. (Default value = 10)\n\n\noptimizer_params\n:  The **kwargs for optimizer \nparams\n. Will default to\n  reasonable values when required keys are not found.\n\n\n\n\nExample:\n\n\nIf you wanted to visualize the input image that would maximize the output index 22, say on\nfinal \nkeras.layers.Dense\n layer, then, \nfilter_indices = [22]\n, \nlayer = dense_layer\n.\n\n\nIf \nfilter_indices = [22, 23]\n, then it should generate an input image that shows features of both classes.\n\n\nReturns:\n\n\nStitched image output visualizing input images that maximize the filter output(s). (Default value = 10)\n\n\n\n\nvisualize_saliency\n\n\nvisualize_saliency(model, layer_idx, filter_indices, seed_img, alpha=0.5)\n\n\n\n\nGenerates an attention heatmap over the \nseed_img\n for maximizing \nfilter_indices\n output in the given \nlayer\n.\nFor a full description of saliency, see the paper:\n\nDeep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance. The model input shape must be: \n(samples, channels, image_dims...)\n \n  if data_format='channels_first' or \n(samples, image_dims..., channels)\n if data_format='channels_last'.\n\n\nlayer_idx\n:  The layer index within \nmodel.layers\n whose filters needs to be visualized.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  For \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\n\n\nIf you are visualizing final \nkeras.layers.Dense\n layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nseed_img\n:  The input image for which activation map needs to be visualized.\n\n\nalpha\n:  The alpha value of image as overlayed onto the heatmap. \n  This value needs to be between [0, 1], with 0 being heatmap only to 1 being image only (Default value = 0.5)\n\n\n\n\nExample:\n\n\nIf you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal \nkeras.layers.Dense\n layer, then, \nfilter_indices = [22]\n, \nlayer = dense_layer\n.\n\n\nOne could also set filter indices to more than one value. For example, \nfilter_indices = [22, 23]\n should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.\n\n\nReturns:\n\n\nThe heatmap image, overlayed with \nseed_img\n using \nalpha\n, indicating image regions that, when changed, \nwould contribute the most towards maximizing the output of \nfilter_indices\n.\n\n\n\n\nvisualize_cam\n\n\nvisualize_cam(model, layer_idx, filter_indices, seed_img, penultimate_layer_idx=None, alpha=0.5)\n\n\n\n\nGenerates a gradient based class activation map (CAM) as described in paper\n\nGrad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization\n.\nUnlike \nclass activation mapping\n, which requires minor changes to\nnetwork architecture in some instances, grad-CAM has a more general applicability.\n\n\nCompared to saliency maps, grad-CAM is class discriminative; i.e., the 'cat' explanation exclusively highlights\ncat regions and not the 'dog' region and vice-versa.\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance. The model input shape must be: \n(samples, channels, image_dims...)\n \n  if data_format='channels_first' or \n(samples, image_dims..., channels)\n if data_format='channels_last'.\n\n\nlayer_idx\n:  The layer index within \nmodel.layers\n whose filters needs to be visualized.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  For \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\n\n\nIf you are visualizing final \nkeras.layers.Dense\n layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nseed_img\n:  The input image for which activation map needs to be visualized.\n\n\npenultimate_layer_idx\n:  The pre-layer to \nlayer_idx\n whose feature maps should be used to compute gradients\n  wrt filter output. If not provided, it is set to the nearest penultimate \nConvolutional\n or \nPooling\n layer.\n\n\nalpha\n:  The alpha value of image as overlayed onto the heatmap. \n  This value needs to be between [0, 1], with 0 being heatmap only to 1 being image only (Default value = 0.5)\n\n\n\n\nExample:\n\n\nIf you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal \nkeras.layers.Dense\n layer, then, \nfilter_indices = [22]\n, \nlayer = dense_layer\n.\n\n\nOne could also set filter indices to more than one value. For example, \nfilter_indices = [22, 23]\n should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.\n\n\nNotes:\n\n\nThis technique deprecates occlusion maps as it gives similar results, but with one-pass gradient computation\nas opposed inefficient sliding window approach.\n\n\nReturns:\n\n\nThe heatmap image, overlayed with \nseed_img\n using \nalpha\n, indicating image regions that, when changed, \nwould contribute the most towards maximizing the output of \nfilter_indices\n.", 
            "title": "visualization"
        }, 
        {
            "location": "/vis.visualization/#get_num_filters", 
            "text": "get_num_filters(layer)  Returns: Total number of filters within  layer .  For  keras.layers.Dense  layer, this is the total number of outputs.", 
            "title": "get_num_filters"
        }, 
        {
            "location": "/vis.visualization/#visualize_activation", 
            "text": "visualize_activation(model, layer_idx, filter_indices=None, seed_img=None, text=None, \\\n    act_max_weight=1, lp_norm_weight=10, tv_weight=10, **optimizer_params)  Generates stitched input image(s) over all  filter_indices  in the given  layer  that maximize\nthe filter output activation.  Args:   model :  The  keras.models.Model  instance. The model input shape must be:  (samples, channels, image_dims...)  \n  if data_format='channels_first' or  (samples, image_dims..., channels)  if data_format='channels_last'.  layer_idx :  The layer index within  model.layers  whose filters needs to be visualized.  filter_indices :  filter indices within the layer to be maximized.\n  For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.   If you are visualizing final  keras.layers.Dense  layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.  filter indices within the layer to be maximized.\n  If None, all filters are visualized. (Default value = None)  An input image is generated for each entry in  filter_indices . The entry can also be an array.\n  For example,  filter_indices = [[1, 2], 3, [4, 5, 6]]  would generate three input images. The first one\n  would maximize output of filters 1, 2, 3 jointly. A fun use of this might be to generate a dog-fish\n  image by maximizing 'dog' and 'fish' output in final  Dense  layer.  For  keras.layers.Dense  layers,  filter_idx  is interpreted as the output index.  If you are visualizing final  keras.layers.Dense  layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.   seed_img :  Seeds the optimization with a starting image. Initialized with a random value when set to None.\n  (Default value = None)  text :  The text to overlay on top of the generated image. (Default Value = None)  act_max_weight :  The weight param for  ActivationMaximization  loss. Not used if 0 or None. (Default value = 1)  lp_norm_weight :  The weight param for  LPNorm  regularization loss. Not used if 0 or None. (Default value = 10)  tv_weight :  The weight param for  TotalVariation  regularization loss. Not used if 0 or None. (Default value = 10)  optimizer_params :  The **kwargs for optimizer  params . Will default to\n  reasonable values when required keys are not found.   Example:  If you wanted to visualize the input image that would maximize the output index 22, say on\nfinal  keras.layers.Dense  layer, then,  filter_indices = [22] ,  layer = dense_layer .  If  filter_indices = [22, 23] , then it should generate an input image that shows features of both classes.  Returns:  Stitched image output visualizing input images that maximize the filter output(s). (Default value = 10)", 
            "title": "visualize_activation"
        }, 
        {
            "location": "/vis.visualization/#visualize_saliency", 
            "text": "visualize_saliency(model, layer_idx, filter_indices, seed_img, alpha=0.5)  Generates an attention heatmap over the  seed_img  for maximizing  filter_indices  output in the given  layer .\nFor a full description of saliency, see the paper: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps  Args:   model :  The  keras.models.Model  instance. The model input shape must be:  (samples, channels, image_dims...)  \n  if data_format='channels_first' or  (samples, image_dims..., channels)  if data_format='channels_last'.  layer_idx :  The layer index within  model.layers  whose filters needs to be visualized.  filter_indices :  filter indices within the layer to be maximized.\n  For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.   If you are visualizing final  keras.layers.Dense  layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.   seed_img :  The input image for which activation map needs to be visualized.  alpha :  The alpha value of image as overlayed onto the heatmap. \n  This value needs to be between [0, 1], with 0 being heatmap only to 1 being image only (Default value = 0.5)   Example:  If you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal  keras.layers.Dense  layer, then,  filter_indices = [22] ,  layer = dense_layer .  One could also set filter indices to more than one value. For example,  filter_indices = [22, 23]  should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.  Returns:  The heatmap image, overlayed with  seed_img  using  alpha , indicating image regions that, when changed, \nwould contribute the most towards maximizing the output of  filter_indices .", 
            "title": "visualize_saliency"
        }, 
        {
            "location": "/vis.visualization/#visualize_cam", 
            "text": "visualize_cam(model, layer_idx, filter_indices, seed_img, penultimate_layer_idx=None, alpha=0.5)  Generates a gradient based class activation map (CAM) as described in paper Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization .\nUnlike  class activation mapping , which requires minor changes to\nnetwork architecture in some instances, grad-CAM has a more general applicability.  Compared to saliency maps, grad-CAM is class discriminative; i.e., the 'cat' explanation exclusively highlights\ncat regions and not the 'dog' region and vice-versa.  Args:   model :  The  keras.models.Model  instance. The model input shape must be:  (samples, channels, image_dims...)  \n  if data_format='channels_first' or  (samples, image_dims..., channels)  if data_format='channels_last'.  layer_idx :  The layer index within  model.layers  whose filters needs to be visualized.  filter_indices :  filter indices within the layer to be maximized.\n  For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.   If you are visualizing final  keras.layers.Dense  layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.   seed_img :  The input image for which activation map needs to be visualized.  penultimate_layer_idx :  The pre-layer to  layer_idx  whose feature maps should be used to compute gradients\n  wrt filter output. If not provided, it is set to the nearest penultimate  Convolutional  or  Pooling  layer.  alpha :  The alpha value of image as overlayed onto the heatmap. \n  This value needs to be between [0, 1], with 0 being heatmap only to 1 being image only (Default value = 0.5)   Example:  If you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal  keras.layers.Dense  layer, then,  filter_indices = [22] ,  layer = dense_layer .  One could also set filter indices to more than one value. For example,  filter_indices = [22, 23]  should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.  Notes:  This technique deprecates occlusion maps as it gives similar results, but with one-pass gradient computation\nas opposed inefficient sliding window approach.  Returns:  The heatmap image, overlayed with  seed_img  using  alpha , indicating image regions that, when changed, \nwould contribute the most towards maximizing the output of  filter_indices .", 
            "title": "visualize_cam"
        }, 
        {
            "location": "/vis.utils.utils/", 
            "text": "Source:\n \nvis/utils/utils.py#L0\n\n\nGlobal Variables\n\n\n\n\nslicer\n\n\n\n\n\n\nset_random_seed\n\n\nset_random_seed(seed_value=1337)\n\n\n\n\nSets random seed value for reproducibility.\n\n\nArgs:\n\n\n\n\nseed_value\n:  The seed value to use. (Default Value = infamous 1337)\n\n\n\n\n\n\nreverse_enumerate\n\n\nreverse_enumerate(iterable)\n\n\n\n\nEnumerate over an iterable in reverse order while retaining proper indexes, without creating any copies.\n\n\n\n\nlistify\n\n\nlistify(value)\n\n\n\n\nEnsures that the value is a list. If it is not a list, it creates a new list with \nvalue\n as an item.\n\n\n\n\nrandom_array\n\n\nrandom_array(shape, mean=128.0, std=20.0)\n\n\n\n\nCreates a uniformly distributed random array with the given mean and std.\n\n\nArgs:\n\n\n\n\nshape\n:  The desired shape\n\n\nmean\n:  The desired mean (Default value = 128)\n\n\nstd\n:  The desired std (Default value = 20)\n\n\n\n\nReturns: Random numpy array of given \nshape\n uniformly distributed with desired \nmean\n and \nstd\n.\n\n\n\n\ndeprocess_image\n\n\ndeprocess_image(img)\n\n\n\n\nUtility function to convert optimized image output into a valid image.\n\n\nArgs:\n\n\n\n\nimg\n:  N dim numpy image array with shape: \n(channels, image_dims...)\n if data_format='channels_first' or\n  \n(image_dims..., channels)\n if data_format='channels_last'.\n\n\n\n\nReturns:\n\n\nA valid image output.\n\n\n\n\nstitch_images\n\n\nstitch_images(images, margin=5, cols=5)\n\n\n\n\nUtility function to stitch images together with a \nmargin\n.\n\n\nArgs:\n\n\n\n\nimages\n:  The array of 2D images to stitch.\n\n\nmargin\n:  The black border margin size between images (Default value = 5)\n\n\ncols\n:  Max number of image cols. New row is created when number of images exceed the column size.\n  (Default value = 5)\n\n\n\n\nReturns:\n\n\nA single numpy image array comprising of input images.\n\n\n\n\nget_img_shape\n\n\nget_img_shape(img)\n\n\n\n\nReturns image shape in a backend agnostic manner.\n\n\nArgs:\n\n\n\n\nimg\n:  An image tensor of shape: \n(channels, image_dims...)\n if data_format='channels_first' or\n  \n(image_dims..., channels)\n if data_format='channels_last'.\n\n\n\n\nReturns:\n\n\nTuple containing image shape information in \n(samples, channels, image_dims...)\n order.\n\n\n\n\nload_img\n\n\nload_img(path, grayscale=False, target_size=None)\n\n\n\n\nUtility function to load an image from disk.\n\n\nArgs:\n\n\n\n\npath\n:  The image file path.\n\n\ngrayscale\n:  True to convert to grayscale image (Default value = False)\n\n\ntarget_size\n:  (w, h) to resize. (Default value = None)\n\n\n\n\nReturns:\n\n\nThe loaded numpy image.\n\n\n\n\nget_imagenet_label\n\n\nget_imagenet_label(indices, join=\n, \n)\n\n\n\n\nUtility function to return the image net label for the final \ndense\n layer output index.\n\n\nArgs:\n\n\n\n\nindices\n:  Could be a single value or an array of indices whose labels needs looking up.\n\n\njoin\n:  When multiple indices are passed, the output labels are joined using this value. (Default Value = ', ')\n\n\n\n\nReturns:\n\n\nImage net label corresponding to the image category.\n\n\n\n\ndraw_text\n\n\ndraw_text(img, text, position=(10, 10), font=\nFreeSans.ttf\n, font_size=14, color=(0, 0, 0))\n\n\n\n\nDraws text over the image. Requires PIL.\n\n\nArgs:\n\n\n\n\nimg\n:  The image to use.\n\n\ntext\n:  The text string to overlay.\n\n\nposition\n:  The text (x, y) position. (Default value = (10, 10)) \n\n\nfont\n:  The ttf or open type font to use. (Default value = 'FreeSans.ttf')\n\n\nfont_size\n:  The text font size. (Default value = 12)\n\n\ncolor\n:  The (r, g, b) values for text color. (Default value = (0, 0, 0))\n\n\n\n\nReturns: Image overlayed with text.\n\n\n\n\nbgr2rgb\n\n\nbgr2rgb(img)\n\n\n\n\nConverts an RGB image to BGR and vice versa\n\n\nArgs:\n\n\n\n\nimg\n:  Numpy array in RGB or BGR format\n\n\n\n\nReturns: The converted image format", 
            "title": "utils"
        }, 
        {
            "location": "/vis.utils.utils/#global-variables", 
            "text": "slicer", 
            "title": "Global Variables"
        }, 
        {
            "location": "/vis.utils.utils/#set_random_seed", 
            "text": "set_random_seed(seed_value=1337)  Sets random seed value for reproducibility.  Args:   seed_value :  The seed value to use. (Default Value = infamous 1337)", 
            "title": "set_random_seed"
        }, 
        {
            "location": "/vis.utils.utils/#reverse_enumerate", 
            "text": "reverse_enumerate(iterable)  Enumerate over an iterable in reverse order while retaining proper indexes, without creating any copies.", 
            "title": "reverse_enumerate"
        }, 
        {
            "location": "/vis.utils.utils/#listify", 
            "text": "listify(value)  Ensures that the value is a list. If it is not a list, it creates a new list with  value  as an item.", 
            "title": "listify"
        }, 
        {
            "location": "/vis.utils.utils/#random_array", 
            "text": "random_array(shape, mean=128.0, std=20.0)  Creates a uniformly distributed random array with the given mean and std.  Args:   shape :  The desired shape  mean :  The desired mean (Default value = 128)  std :  The desired std (Default value = 20)   Returns: Random numpy array of given  shape  uniformly distributed with desired  mean  and  std .", 
            "title": "random_array"
        }, 
        {
            "location": "/vis.utils.utils/#deprocess_image", 
            "text": "deprocess_image(img)  Utility function to convert optimized image output into a valid image.  Args:   img :  N dim numpy image array with shape:  (channels, image_dims...)  if data_format='channels_first' or\n   (image_dims..., channels)  if data_format='channels_last'.   Returns:  A valid image output.", 
            "title": "deprocess_image"
        }, 
        {
            "location": "/vis.utils.utils/#stitch_images", 
            "text": "stitch_images(images, margin=5, cols=5)  Utility function to stitch images together with a  margin .  Args:   images :  The array of 2D images to stitch.  margin :  The black border margin size between images (Default value = 5)  cols :  Max number of image cols. New row is created when number of images exceed the column size.\n  (Default value = 5)   Returns:  A single numpy image array comprising of input images.", 
            "title": "stitch_images"
        }, 
        {
            "location": "/vis.utils.utils/#get_img_shape", 
            "text": "get_img_shape(img)  Returns image shape in a backend agnostic manner.  Args:   img :  An image tensor of shape:  (channels, image_dims...)  if data_format='channels_first' or\n   (image_dims..., channels)  if data_format='channels_last'.   Returns:  Tuple containing image shape information in  (samples, channels, image_dims...)  order.", 
            "title": "get_img_shape"
        }, 
        {
            "location": "/vis.utils.utils/#load_img", 
            "text": "load_img(path, grayscale=False, target_size=None)  Utility function to load an image from disk.  Args:   path :  The image file path.  grayscale :  True to convert to grayscale image (Default value = False)  target_size :  (w, h) to resize. (Default value = None)   Returns:  The loaded numpy image.", 
            "title": "load_img"
        }, 
        {
            "location": "/vis.utils.utils/#get_imagenet_label", 
            "text": "get_imagenet_label(indices, join= ,  )  Utility function to return the image net label for the final  dense  layer output index.  Args:   indices :  Could be a single value or an array of indices whose labels needs looking up.  join :  When multiple indices are passed, the output labels are joined using this value. (Default Value = ', ')   Returns:  Image net label corresponding to the image category.", 
            "title": "get_imagenet_label"
        }, 
        {
            "location": "/vis.utils.utils/#draw_text", 
            "text": "draw_text(img, text, position=(10, 10), font= FreeSans.ttf , font_size=14, color=(0, 0, 0))  Draws text over the image. Requires PIL.  Args:   img :  The image to use.  text :  The text string to overlay.  position :  The text (x, y) position. (Default value = (10, 10))   font :  The ttf or open type font to use. (Default value = 'FreeSans.ttf')  font_size :  The text font size. (Default value = 12)  color :  The (r, g, b) values for text color. (Default value = (0, 0, 0))   Returns: Image overlayed with text.", 
            "title": "draw_text"
        }, 
        {
            "location": "/vis.utils.utils/#bgr2rgb", 
            "text": "bgr2rgb(img)  Converts an RGB image to BGR and vice versa  Args:   img :  Numpy array in RGB or BGR format   Returns: The converted image format", 
            "title": "bgr2rgb"
        }, 
        {
            "location": "/vis.utils.vggnet/", 
            "text": "Source:\n \nvis/utils/vggnet.py#L0\n\n\nGlobal Variables\n\n\n\n\nWEIGHTS_PATH_NO_TOP\n\n\nWEIGHTS_PATH\n\n\n\n\n\n\nVGG16\n\n\nVGG16(include_top=True, weights=\nimagenet\n, input_tensor=None, input_shape=None, pooling=None, \\\n    classes=1000)\n\n\n\n\nInstantiates the VGG16 architecture.\n\n\nOptionally loads weights pre-trained\non ImageNet. Note that when using TensorFlow,\nfor best performance you should set\n\nimage_data_format=\"channels_last\"\n in your Keras config\nat ~/.keras/keras.json.\n\n\nThe model and the weights are compatible with both\nTensorFlow and Theano. The data format\nconvention used by the model is the one\nspecified in your Keras config file.\n\n\nArguments\n\n\n\n\ninclude_top\n:  whether to include the 3 fully-connected\n  layers at the top of the network.\n\n\nweights\n:  one of \nNone\n (random initialization)\n  or \"imagenet\" (pre-training on ImageNet).\n\n\ninput_tensor\n:  optional Keras tensor (i.e. output of \nlayers.Input()\n)\n  to use as image input for the model.\n\n\ninput_shape\n:  optional shape tuple, only to be specified\n  if \ninclude_top\n is False (otherwise the input shape\n  has to be \n(224, 224, 3)\n (with \nchannels_last\n data format)\n  or \n(3, 224, 244)\n (with \nchannels_first\n data format).\n  It should have exactly 3 inputs channels,\n  and width and height should be no smaller than 48.\n  E.g. \n(200, 200, 3)\n would be one valid value.\n\n\npooling\n:  Optional pooling mode for feature extraction\n  when \ninclude_top\n is \nFalse\n.\n\n\nNone\n means that the output of the model will be\n  the 4D tensor output of the\n  last convolutional layer.\n\n\navg\n means that global average pooling\n  will be applied to the output of the\n  last convolutional layer, and thus\n  the output of the model will be a 2D tensor.\n\n\nmax\n means that global max pooling will\n  be applied.\n\n\nclasses\n:  optional number of classes to classify images\n  into, only to be specified if \ninclude_top\n is True, and\n  if no \nweights\n argument is specified.\n\n\n\n\nReturns\n\n\nA Keras model instance.\n\n\nRaises\n\n\n\n\nValueError\n:  in case of invalid argument for \nweights\n,\n  or invalid input shape.", 
            "title": "vggnet"
        }, 
        {
            "location": "/vis.utils.vggnet/#global-variables", 
            "text": "WEIGHTS_PATH_NO_TOP  WEIGHTS_PATH", 
            "title": "Global Variables"
        }, 
        {
            "location": "/vis.utils.vggnet/#vgg16", 
            "text": "VGG16(include_top=True, weights= imagenet , input_tensor=None, input_shape=None, pooling=None, \\\n    classes=1000)  Instantiates the VGG16 architecture.  Optionally loads weights pre-trained\non ImageNet. Note that when using TensorFlow,\nfor best performance you should set image_data_format=\"channels_last\"  in your Keras config\nat ~/.keras/keras.json.  The model and the weights are compatible with both\nTensorFlow and Theano. The data format\nconvention used by the model is the one\nspecified in your Keras config file.", 
            "title": "VGG16"
        }, 
        {
            "location": "/vis.utils.vggnet/#arguments", 
            "text": "include_top :  whether to include the 3 fully-connected\n  layers at the top of the network.  weights :  one of  None  (random initialization)\n  or \"imagenet\" (pre-training on ImageNet).  input_tensor :  optional Keras tensor (i.e. output of  layers.Input() )\n  to use as image input for the model.  input_shape :  optional shape tuple, only to be specified\n  if  include_top  is False (otherwise the input shape\n  has to be  (224, 224, 3)  (with  channels_last  data format)\n  or  (3, 224, 244)  (with  channels_first  data format).\n  It should have exactly 3 inputs channels,\n  and width and height should be no smaller than 48.\n  E.g.  (200, 200, 3)  would be one valid value.  pooling :  Optional pooling mode for feature extraction\n  when  include_top  is  False .  None  means that the output of the model will be\n  the 4D tensor output of the\n  last convolutional layer.  avg  means that global average pooling\n  will be applied to the output of the\n  last convolutional layer, and thus\n  the output of the model will be a 2D tensor.  max  means that global max pooling will\n  be applied.  classes :  optional number of classes to classify images\n  into, only to be specified if  include_top  is True, and\n  if no  weights  argument is specified.", 
            "title": "Arguments"
        }, 
        {
            "location": "/vis.utils.vggnet/#returns", 
            "text": "A Keras model instance.", 
            "title": "Returns"
        }, 
        {
            "location": "/vis.utils.vggnet/#raises", 
            "text": "ValueError :  in case of invalid argument for  weights ,\n  or invalid input shape.", 
            "title": "Raises"
        }
    ]
}