{
    "docs": [
        {
            "location": "/", 
            "text": "Keras Visualization Toolkit\n\n\n\n\n\n\n\n\nkeras-vis is a high-level toolkit for visualizing and debugging your trained keras neural net models. Currently\nsupported visualizations include:\n\n\n\n\nActivation maximization\n\n\nSaliency maps\n\n\nClass activation maps\n\n\n\n\nAll visualizations by default support N-dimensional image inputs. i.e., it generalizes to N-dim image inputs \nto your model.\n\n\nThe toolkit generalizes all of the above as energy minimization problems with a clean, easy to use, \nand extendable interface. Compatible with both theano and tensorflow backends with 'channels_first', 'channels_last' \ndata format.\n\n\nQuick links\n\n\n\n\nRead the documentation at \nhttps://raghakot.github.io/keras-vis\n. \n\n\nJoin the slack \nchannel\n for questions/discussions.\n\n\nWe are tracking new features/tasks in \nwaffle.io\n. Would love it if you lend us \na hand and submit PRs.\n\n\n\n\nGetting Started\n\n\nIn image backprop problems, the goal is to generate an input image that minimizes some loss function.\nSetting up an image backprop problem is easy.\n\n\nDefine weighted loss function\n\n\nVarious useful loss functions are defined in \nlosses\n.\nA custom loss function can be defined by implementing \nLoss.build_loss\n.\n\n\nfrom vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\n\nfilter_indices = [1, 2, 3]\n\n# Tuple consists of (loss_function, weight)\n# Add regularizers as needed.\nlosses = [\n    (ActivationMaximization(keras_layer, filter_indices), 1),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]\n\n\n\n\nConfigure optimizer to minimize weighted loss\n\n\nIn order to generate natural looking images, image search space is constrained using regularization penalties. \nSome common regularizers are defined in \nregularizers\n.\nLike loss functions, custom regularizer can be defined by implementing \n\nLoss.build_loss\n.\n\n\nfrom vis.optimizer import Optimizer\n\noptimizer = Optimizer(model.input, losses)\nopt_img, grads, _ = optimizer.minimize()\n\n\n\n\nConcrete examples of various supported visualizations can be found in \n\nexamples folder\n.\n\n\nInstallation\n\n\n1) Install \nkeras\n \nwith theano or tensorflow backend. Note that this library requires Keras \n 2.0\n\n\n2) Install keras-vis\n\n\n\n\nFrom sources\n\n\n\n\nsudo python setup.py install\n\n\n\n\n\n\nPyPI package\n\n\n\n\nsudo pip install keras-vis\n\n\n\n\nVisualizations\n\n\nNOTE: The links are currently broken and the entire documentation is being reworked.\nPlease see examples/ for samples.\n\n\nNeural nets are black boxes. In the recent years, several approaches for understanding and visualizing Convolutional \nNetworks have been developed in the literature. They give us a way to peer into the black boxes, \ndiagnose mis-classifications, and assess whether the network is over/under fitting. \n\n\nGuided backprop can also be used to create \ntrippy art\n, neural/texture \n\nstyle transfer\n among the list of other growing applications.\n\n\nVarious visualizations, documented in their own pages, are summarized here.\n\n\n\n\n\nConv filter visualization\n\n\n\n\nConvolutional filters learn 'template matching' filters that maximize the output when a similar template \npattern is found in the input image. Visualize those templates via Activation Maximization.\n\n\n\n\n\nDense layer visualization\n\n\n\n\nHow can we assess whether a network is over/under fitting or generalizing well?\n\n\n\n\n\nAttention Maps\n\n\n\n\nHow can we assess whether a network is attending to correct parts of the image in order to generate a decision?\n\n\n\n\n\nGenerating animated gif of optimization progress\n\n\nIt is possible to generate an animated gif of optimization progress by leveraging \n\ncallbacks\n. Following example shows how to visualize the \nactivation maximization for 'ouzel' class (output_index: 20).\n\n\nfrom vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\nfrom vis.modifiers import Jitter\nfrom vis.optimizer import Optimizer\n\nfrom vis.callbacks import GifGenerator\nfrom vis.utils.vggnet import VGG16\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\noutput_class = [20]\n\nlosses = [\n    (ActivationMaximization(layer_dict[layer_name], output_class), 2),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]\nopt = Optimizer(model.input, losses)\nopt.minimize(max_iter=500, verbose=True, image_modifiers=[Jitter()], callbacks=[GifGenerator('opt_progress')])\n\n\n\n\n\nNotice how the output jitters around? This is because we used \nJitter\n, \na kind of \nImageModifier\n that is known to produce \ncrisper activation maximization images. As an exercise, try:\n\n\n\n\nWithout Jitter\n\n\nVarying various loss weights\n\n\n\n\n\n\n\n\n\nCitation\n\n\nPlease cite keras-vis in your publications if it helped your research. Here is an example BibTeX entry:\n\n\n@misc{raghakotkerasvis\n  title={keras-vis},\n  author={Kotikalapudi, Raghavendra and contributors},\n  year={2017},\n  publisher={GitHub},\n  howpublished={\\url{https://github.com/raghakot/keras-vis}},\n}", 
            "title": "Home"
        }, 
        {
            "location": "/#keras-visualization-toolkit", 
            "text": "keras-vis is a high-level toolkit for visualizing and debugging your trained keras neural net models. Currently\nsupported visualizations include:   Activation maximization  Saliency maps  Class activation maps   All visualizations by default support N-dimensional image inputs. i.e., it generalizes to N-dim image inputs \nto your model.  The toolkit generalizes all of the above as energy minimization problems with a clean, easy to use, \nand extendable interface. Compatible with both theano and tensorflow backends with 'channels_first', 'channels_last' \ndata format.", 
            "title": "Keras Visualization Toolkit"
        }, 
        {
            "location": "/#quick-links", 
            "text": "Read the documentation at  https://raghakot.github.io/keras-vis .   Join the slack  channel  for questions/discussions.  We are tracking new features/tasks in  waffle.io . Would love it if you lend us \na hand and submit PRs.", 
            "title": "Quick links"
        }, 
        {
            "location": "/#getting-started", 
            "text": "In image backprop problems, the goal is to generate an input image that minimizes some loss function.\nSetting up an image backprop problem is easy.  Define weighted loss function  Various useful loss functions are defined in  losses .\nA custom loss function can be defined by implementing  Loss.build_loss .  from vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\n\nfilter_indices = [1, 2, 3]\n\n# Tuple consists of (loss_function, weight)\n# Add regularizers as needed.\nlosses = [\n    (ActivationMaximization(keras_layer, filter_indices), 1),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]  Configure optimizer to minimize weighted loss  In order to generate natural looking images, image search space is constrained using regularization penalties. \nSome common regularizers are defined in  regularizers .\nLike loss functions, custom regularizer can be defined by implementing  Loss.build_loss .  from vis.optimizer import Optimizer\n\noptimizer = Optimizer(model.input, losses)\nopt_img, grads, _ = optimizer.minimize()  Concrete examples of various supported visualizations can be found in  examples folder .", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#installation", 
            "text": "1) Install  keras  \nwith theano or tensorflow backend. Note that this library requires Keras   2.0  2) Install keras-vis   From sources   sudo python setup.py install   PyPI package   sudo pip install keras-vis", 
            "title": "Installation"
        }, 
        {
            "location": "/#visualizations", 
            "text": "NOTE: The links are currently broken and the entire documentation is being reworked.\nPlease see examples/ for samples.  Neural nets are black boxes. In the recent years, several approaches for understanding and visualizing Convolutional \nNetworks have been developed in the literature. They give us a way to peer into the black boxes, \ndiagnose mis-classifications, and assess whether the network is over/under fitting.   Guided backprop can also be used to create  trippy art , neural/texture  style transfer  among the list of other growing applications.  Various visualizations, documented in their own pages, are summarized here.", 
            "title": "Visualizations"
        }, 
        {
            "location": "/#conv-filter-visualization", 
            "text": "Convolutional filters learn 'template matching' filters that maximize the output when a similar template \npattern is found in the input image. Visualize those templates via Activation Maximization.", 
            "title": "Conv filter visualization"
        }, 
        {
            "location": "/#dense-layer-visualization", 
            "text": "How can we assess whether a network is over/under fitting or generalizing well?", 
            "title": "Dense layer visualization"
        }, 
        {
            "location": "/#attention-maps", 
            "text": "How can we assess whether a network is attending to correct parts of the image in order to generate a decision?", 
            "title": "Attention Maps"
        }, 
        {
            "location": "/#generating-animated-gif-of-optimization-progress", 
            "text": "It is possible to generate an animated gif of optimization progress by leveraging  callbacks . Following example shows how to visualize the \nactivation maximization for 'ouzel' class (output_index: 20).  from vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\nfrom vis.modifiers import Jitter\nfrom vis.optimizer import Optimizer\n\nfrom vis.callbacks import GifGenerator\nfrom vis.utils.vggnet import VGG16\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\noutput_class = [20]\n\nlosses = [\n    (ActivationMaximization(layer_dict[layer_name], output_class), 2),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]\nopt = Optimizer(model.input, losses)\nopt.minimize(max_iter=500, verbose=True, image_modifiers=[Jitter()], callbacks=[GifGenerator('opt_progress')])  Notice how the output jitters around? This is because we used  Jitter , \na kind of  ImageModifier  that is known to produce \ncrisper activation maximization images. As an exercise, try:   Without Jitter  Varying various loss weights", 
            "title": "Generating animated gif of optimization progress"
        }, 
        {
            "location": "/#citation", 
            "text": "Please cite keras-vis in your publications if it helped your research. Here is an example BibTeX entry:  @misc{raghakotkerasvis\n  title={keras-vis},\n  author={Kotikalapudi, Raghavendra and contributors},\n  year={2017},\n  publisher={GitHub},\n  howpublished={\\url{https://github.com/raghakot/keras-vis}},\n}", 
            "title": "Citation"
        }, 
        {
            "location": "/visualizations/activation_maximization/", 
            "text": "What is Activation Maximization?\n\n\nIn a CNN, each Conv layer has several learned \ntemplate matching\n filters that maximize their output when a similar \ntemplate pattern is found in the input image. First Conv layer is easy to interpret; simply visualize the weights as \nan image. To see what the Conv layer is doing, a simple option is to apply the filter over raw input pixels. \nSubsequent Conv filters operate over the outputs of previous Conv filters (which indicate the presence or absence \nof some templates), making them hard to interpret.\n\n\nThe idea behind activation maximization is simple in hindsight - Generate an input image that maximizes the filter \noutput activations. i.e., we compute \n\n\n\n\n\\frac{\\partial ActivationMaximizationLoss}{\\partial input}\n\n\n\n\nand use that estimate to update the input. \nActivationMaximization\n loss simply \noutputs small values for large filter activations (we are minimizing losses during gradient descent iterations). \nThis allows us to understand what sort of input patterns activate a particular filter. For example, there could be \nan eye filter that activates for the presence of eye within the input image.\n\n\nUsage\n\n\nThere are two APIs exposed to perform activation maximization.\n\n\n\n\nvisualize_activation\n: This is the general purpose API for visualizing\nactivations.\n\n\nvisualize_activation_with_losses\n: This is intended for \nresearch use-cases where some custom weighted losses can be minimized.\n\n\n\n\nScenarios\n\n\nThe API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases\nbelow:\n\n\nCategorical Output \nDense\n layer visualization\n\n\nHow can we assess whether a network is over/under fitting or generalizing well? Given an input image, a CNN can \nclassify whether it is a cat, bird etc. How can we be sure that it is capturing the correct notion of what it means \nto be a bird? \n\n\nOne way to answer these questions is to pose the reverse question:\n\n\n\n\nGenerate an input image that maximizes the final \nDense\n layer output corresponding to bird class. \n\n\n\n\nThis can be done by pointing \nlayer_idx\n to final \nDense\n layer, and setting \nfilter_indices\n to the desired output \ncategory. \n\n\n\n\nFor multi-class classification, \nfilter_indices\n can point to a single class. You could point also point it to \nmultiple categories to see what a cat-fish might look like, as an example.\n\n\nFor multi-label classifier, simply set the appropriate \nfilter_indices\n.\n\n\n\n\nRegression Output \nDense\n layer visualization\n\n\nUnlike class activation visualizations, for regression outputs, we could visualize input that \n\n\n\n\nincreases\n\n\ndecreases\n\n\n\n\nthe regressed \nfilter_indices\n output. For example, if you trained an apple counter model, increasing the regression\noutput should correspond to more apples showing up in the input image. Similarly one could decrease the current output.\nThis can be achieved by using \ngrad_modifier\n option. As the name suggests, it is used to modify the gradient of losses\nwith respect to inputs. By default, \nActivationMaximization\n loss is used to increase the output. By setting \n\ngrad_modifier='negate'\n you can negate the gradients, thus causing output values to decrease. \n\ngradient_modifiers\n are very powerful and show up in other visualization APIs as well. \n\n\nConv\n filter visualization\n\n\nBy pointing \nlayer_idx\n to \nConv\n layer, you can visualize what pattern activates a filter. This might help you discover\nwhat a filter might be computing. Here, \nfilter_indices\n refers to the index of the \nConv\n filter within the layer.\n\n\nAdvanced usage\n\n\nbackprop_modifier\n allows you to modify the backpropagation behavior. For examples, you could tweak backprop to only\npropagate positive gradients by using \nbackprop_modifier='relu'\n. This parameter also accepts a function and can be\nused to implement your crazy research idea :)\n\n\nTips and tricks\n\n\n\n\n\n\nIf you get garbage visualization, try setting \nverbose=True\n to see various losses during gradient descent iterations.\nBy default, \nvisualize_activation\n uses \nTotalVariation\n and \nLpNorm\n regularization to enforce natural image prior. It\nis very likely that you would see \nActivationMax Loss\n bounce back and forth as they are dominated by regularization \nloss weights. Try setting all weights to zero and gradually try increasing values of total variation weight.\n\n\n\n\n\n\nTo get sharper looking images, use \nJitter\n input modifier.\n\n\n\n\n\n\nRegression models usually do not provide enough gradient information to generate meaningful input images. Try seeding\nthe input using \nseed_input\n and see if the modifications to the input make sense.\n\n\n\n\n\n\nConsider submitting a PR to add more tips and tricks that you found useful.", 
            "title": "Activation Maximization"
        }, 
        {
            "location": "/visualizations/activation_maximization/#what-is-activation-maximization", 
            "text": "In a CNN, each Conv layer has several learned  template matching  filters that maximize their output when a similar \ntemplate pattern is found in the input image. First Conv layer is easy to interpret; simply visualize the weights as \nan image. To see what the Conv layer is doing, a simple option is to apply the filter over raw input pixels. \nSubsequent Conv filters operate over the outputs of previous Conv filters (which indicate the presence or absence \nof some templates), making them hard to interpret.  The idea behind activation maximization is simple in hindsight - Generate an input image that maximizes the filter \noutput activations. i.e., we compute    \\frac{\\partial ActivationMaximizationLoss}{\\partial input}   and use that estimate to update the input.  ActivationMaximization  loss simply \noutputs small values for large filter activations (we are minimizing losses during gradient descent iterations). \nThis allows us to understand what sort of input patterns activate a particular filter. For example, there could be \nan eye filter that activates for the presence of eye within the input image.", 
            "title": "What is Activation Maximization?"
        }, 
        {
            "location": "/visualizations/activation_maximization/#usage", 
            "text": "There are two APIs exposed to perform activation maximization.   visualize_activation : This is the general purpose API for visualizing\nactivations.  visualize_activation_with_losses : This is intended for \nresearch use-cases where some custom weighted losses can be minimized.", 
            "title": "Usage"
        }, 
        {
            "location": "/visualizations/activation_maximization/#scenarios", 
            "text": "The API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases\nbelow:", 
            "title": "Scenarios"
        }, 
        {
            "location": "/visualizations/activation_maximization/#categorical-output-dense-layer-visualization", 
            "text": "How can we assess whether a network is over/under fitting or generalizing well? Given an input image, a CNN can \nclassify whether it is a cat, bird etc. How can we be sure that it is capturing the correct notion of what it means \nto be a bird?   One way to answer these questions is to pose the reverse question:   Generate an input image that maximizes the final  Dense  layer output corresponding to bird class.    This can be done by pointing  layer_idx  to final  Dense  layer, and setting  filter_indices  to the desired output \ncategory.    For multi-class classification,  filter_indices  can point to a single class. You could point also point it to \nmultiple categories to see what a cat-fish might look like, as an example.  For multi-label classifier, simply set the appropriate  filter_indices .", 
            "title": "Categorical Output Dense layer visualization"
        }, 
        {
            "location": "/visualizations/activation_maximization/#regression-output-dense-layer-visualization", 
            "text": "Unlike class activation visualizations, for regression outputs, we could visualize input that    increases  decreases   the regressed  filter_indices  output. For example, if you trained an apple counter model, increasing the regression\noutput should correspond to more apples showing up in the input image. Similarly one could decrease the current output.\nThis can be achieved by using  grad_modifier  option. As the name suggests, it is used to modify the gradient of losses\nwith respect to inputs. By default,  ActivationMaximization  loss is used to increase the output. By setting  grad_modifier='negate'  you can negate the gradients, thus causing output values to decrease.  gradient_modifiers  are very powerful and show up in other visualization APIs as well.", 
            "title": "Regression Output Dense layer visualization"
        }, 
        {
            "location": "/visualizations/activation_maximization/#conv-filter-visualization", 
            "text": "By pointing  layer_idx  to  Conv  layer, you can visualize what pattern activates a filter. This might help you discover\nwhat a filter might be computing. Here,  filter_indices  refers to the index of the  Conv  filter within the layer.", 
            "title": "Conv filter visualization"
        }, 
        {
            "location": "/visualizations/activation_maximization/#advanced-usage", 
            "text": "backprop_modifier  allows you to modify the backpropagation behavior. For examples, you could tweak backprop to only\npropagate positive gradients by using  backprop_modifier='relu' . This parameter also accepts a function and can be\nused to implement your crazy research idea :)", 
            "title": "Advanced usage"
        }, 
        {
            "location": "/visualizations/activation_maximization/#tips-and-tricks", 
            "text": "If you get garbage visualization, try setting  verbose=True  to see various losses during gradient descent iterations.\nBy default,  visualize_activation  uses  TotalVariation  and  LpNorm  regularization to enforce natural image prior. It\nis very likely that you would see  ActivationMax Loss  bounce back and forth as they are dominated by regularization \nloss weights. Try setting all weights to zero and gradually try increasing values of total variation weight.    To get sharper looking images, use  Jitter  input modifier.    Regression models usually do not provide enough gradient information to generate meaningful input images. Try seeding\nthe input using  seed_input  and see if the modifications to the input make sense.    Consider submitting a PR to add more tips and tricks that you found useful.", 
            "title": "Tips and tricks"
        }, 
        {
            "location": "/visualizations/saliency/", 
            "text": "What is Saliency?\n\n\nSuppose that all the training images of \nbird\n class contains a tree with leaves. How do we know whether the CNN is \nusing bird-related pixels, as opposed to some other features such as the tree or leaves in the image? This actually \nhappens more often than you think and you should be especially suspicious if you have a small training set. \n\n\nSaliency maps was first introduced in the paper: \n\nDeep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n\n\nThe idea is pretty simple. We compute the gradient of output category with respect to input image. This should tell us\nhow output category value changes with respect to a small change in input image pixels. All the positive values\nin the gradients tell us that a small change to that pixel will increase the output value. \nHence, visualizing these gradients, which are the same shape as the image should provide some intuition of attention.\n\n\nThe idea behind saliency is pretty simple in hindsight. We compute the gradient of output category with respect to \ninput image. \n\n\n\n\n\\frac{\\partial output}{\\partial input}\n\n\n\n\nThis should tell us how the output value changes with respect to a small change in inputs. We can use these gradients \nto highlight input regions that cause the most change in the output. Intuitively this should highlight salient image \nregions that most contribute towards the output. \n\n\nRefer to \nDeep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n \nfor details.\n\n\nUsage\n\n\nThere are two APIs exposed to visualize saliency.\n\n\n\n\nvisualize_saliency\n: This is the general purpose API for visualizing\nsaliency.\n\n\nvisualize_saliency_with_losses\n: This is intended for \nresearch use-cases where some custom weighted loss can be used.\n\n\n\n\nScenarios\n\n\nThe API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases\nbelow:\n\n\nCategorical \nDense\n layer visualization\n\n\nBy setting \nlayer_idx\n to final \nDense\n layer, and \nfilter_indices\n to the desired output category, we can visualize \nparts of the \nseed_input\n that contribute most towards activating the corresponding output nodes,\n\n\n\n\nFor multi-class classification, \nfilter_indices\n can point to a single class.\n\n\nFor multi-label classifier, simply set the appropriate \nfilter_indices\n.\n\n\n\n\nRegression \nDense\n layer visualization\n\n\nFor regression outputs, we could visualize attention over input that \n\n\n\n\nincreases\n\n\ndecreases\n\n\nmaintains\n\n\n\n\nthe regressed \nfilter_indices\n output. For example, consider a self driving model with continuous regression steering \noutput. One could visualize parts of the \nseed_input\n that contributes towards increase, decrease or maintenance of \npredicted output.\n\n\nBy default, saliency tells us how to increase the output activations. For the self driving car case, this only tells\nus parts of the input image that contribute towards steering angle increase. Other use cases can be visualized by \nusing \ngrad_modifier\n option. As the name suggests, it is used to modify the gradient of losses with respect to inputs. \n\n\n\n\n\n\nTo visualize decrease in output, use \ngrad_modifier='negate'\n. By default, \nActivationMaximization\n loss yields \npositive gradients for inputs regions that increase the output. By setting \ngrad_modifier='negate'\n you can treat negative\ngradients (which indicate the decrease) as positive and therefore visualize decrease use case.\n\n\n\n\n\n\nTo visualize what contributed to the predicted output, we want to consider gradients that have very low positive\nor negative values. This can be achieved by performing \ngrads = abs(1 / grads)\n to magnifies small gradients. Equivalently, \nyou can use \ngrad_modifier='small_values'\n, which does the same thing.\n\n\n\n\n\n\ngradient_modifiers\n are very powerful and show up in other visualization APIs as well.\n\n\nGuided / rectified saliency\n\n\nZieler et al. has the idea of clipping negative gradients in the backprop step. i.e., only propagate positive gradient\ninformation that communicates the increase in output. We call this rectified or deconv saliency. Details can be found \nin the paper: \nVisualizing and Understanding Convolutional Networks\n.\n\n\nIn guided saliency, the backprop step is modified to only propagate positive gradients for positive activations.\nFor details see the paper: \nString For Simplicity: The All Convolutional Net\n.\n\n\nFor both these cases, we can use \nbackprop_modifier='relu'\n and \nbackprop_modifier='guided'\n respectively. You \ncan also implement your own \nbackprop_modifier\n to try your crazy research idea :)\n\n\nConv\n filter saliency\n\n\nBy pointing \nlayer_idx\n to \nConv\n layer, you can visualize parts of the image that influence the filter. This might \nhelp you discover what a filter cares about. Here, \nfilter_indices\n refers to the index of the \nConv\n filter within \nthe layer.", 
            "title": "Saliency Maps"
        }, 
        {
            "location": "/visualizations/saliency/#what-is-saliency", 
            "text": "Suppose that all the training images of  bird  class contains a tree with leaves. How do we know whether the CNN is \nusing bird-related pixels, as opposed to some other features such as the tree or leaves in the image? This actually \nhappens more often than you think and you should be especially suspicious if you have a small training set.   Saliency maps was first introduced in the paper:  Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps  The idea is pretty simple. We compute the gradient of output category with respect to input image. This should tell us\nhow output category value changes with respect to a small change in input image pixels. All the positive values\nin the gradients tell us that a small change to that pixel will increase the output value. \nHence, visualizing these gradients, which are the same shape as the image should provide some intuition of attention.  The idea behind saliency is pretty simple in hindsight. We compute the gradient of output category with respect to \ninput image.    \\frac{\\partial output}{\\partial input}   This should tell us how the output value changes with respect to a small change in inputs. We can use these gradients \nto highlight input regions that cause the most change in the output. Intuitively this should highlight salient image \nregions that most contribute towards the output.   Refer to  Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps  \nfor details.", 
            "title": "What is Saliency?"
        }, 
        {
            "location": "/visualizations/saliency/#usage", 
            "text": "There are two APIs exposed to visualize saliency.   visualize_saliency : This is the general purpose API for visualizing\nsaliency.  visualize_saliency_with_losses : This is intended for \nresearch use-cases where some custom weighted loss can be used.", 
            "title": "Usage"
        }, 
        {
            "location": "/visualizations/saliency/#scenarios", 
            "text": "The API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases\nbelow:", 
            "title": "Scenarios"
        }, 
        {
            "location": "/visualizations/saliency/#categorical-dense-layer-visualization", 
            "text": "By setting  layer_idx  to final  Dense  layer, and  filter_indices  to the desired output category, we can visualize \nparts of the  seed_input  that contribute most towards activating the corresponding output nodes,   For multi-class classification,  filter_indices  can point to a single class.  For multi-label classifier, simply set the appropriate  filter_indices .", 
            "title": "Categorical Dense layer visualization"
        }, 
        {
            "location": "/visualizations/saliency/#regression-dense-layer-visualization", 
            "text": "For regression outputs, we could visualize attention over input that    increases  decreases  maintains   the regressed  filter_indices  output. For example, consider a self driving model with continuous regression steering \noutput. One could visualize parts of the  seed_input  that contributes towards increase, decrease or maintenance of \npredicted output.  By default, saliency tells us how to increase the output activations. For the self driving car case, this only tells\nus parts of the input image that contribute towards steering angle increase. Other use cases can be visualized by \nusing  grad_modifier  option. As the name suggests, it is used to modify the gradient of losses with respect to inputs.     To visualize decrease in output, use  grad_modifier='negate' . By default,  ActivationMaximization  loss yields \npositive gradients for inputs regions that increase the output. By setting  grad_modifier='negate'  you can treat negative\ngradients (which indicate the decrease) as positive and therefore visualize decrease use case.    To visualize what contributed to the predicted output, we want to consider gradients that have very low positive\nor negative values. This can be achieved by performing  grads = abs(1 / grads)  to magnifies small gradients. Equivalently, \nyou can use  grad_modifier='small_values' , which does the same thing.    gradient_modifiers  are very powerful and show up in other visualization APIs as well.", 
            "title": "Regression Dense layer visualization"
        }, 
        {
            "location": "/visualizations/saliency/#guided-rectified-saliency", 
            "text": "Zieler et al. has the idea of clipping negative gradients in the backprop step. i.e., only propagate positive gradient\ninformation that communicates the increase in output. We call this rectified or deconv saliency. Details can be found \nin the paper:  Visualizing and Understanding Convolutional Networks .  In guided saliency, the backprop step is modified to only propagate positive gradients for positive activations.\nFor details see the paper:  String For Simplicity: The All Convolutional Net .  For both these cases, we can use  backprop_modifier='relu'  and  backprop_modifier='guided'  respectively. You \ncan also implement your own  backprop_modifier  to try your crazy research idea :)", 
            "title": "Guided / rectified saliency"
        }, 
        {
            "location": "/visualizations/saliency/#conv-filter-saliency", 
            "text": "By pointing  layer_idx  to  Conv  layer, you can visualize parts of the image that influence the filter. This might \nhelp you discover what a filter cares about. Here,  filter_indices  refers to the index of the  Conv  filter within \nthe layer.", 
            "title": "Conv filter saliency"
        }, 
        {
            "location": "/visualizations/class_activation_maps/", 
            "text": "What is a Class Activation Map?\n\n\nClass activation maps or grad-CAM is another way of visualizing attention over input. Instead of using gradients with\nrespect to output (see \nsaliency\n), grad-CAM uses penultimate (pre \nDense\n layer) \nConv\n layer output. The\nintuition is to use the nearest \nConv\n layer to utilize spatial information that gets completely lost in \nDense\n layers.\n\n\nIn keras-vis, we use \ngrad-CAM\n as its considered more general than \n\nClass Activation maps\n.\n\n\nUsage\n\n\nThere are two APIs exposed to visualize grad-CAM and are almost identical to \nsaliency usage\n.\n\n\n\n\nvisualize_cam\n: This is the general purpose API for visualizing\ngrad-CAM.\n\n\nvisualize_cam_with_losses\n: This is intended for \nresearch use-cases where some custom weighted loss can be used.\n\n\n\n\nThe only notable addition is the \npenultimate_layer_idx\n parameter. This can be used to specify the pre-layer\nwhose output gradients are used. By default, keras-vis will search for the nearest layer with filters.\n\n\nScenarios\n\n\nSee \nsaliency scenarios\n. Everything is identical expect the added \npenultimate_layer_idx\n param.\n\n\nGotchas\n\n\ngrad-CAM only works well if the penultimate layer is close to the layer being visualized. This also applies to \nConv\n \nfilter visualizations. You are better off using saliency of this is not the case with your model.", 
            "title": "Class Activation Maps"
        }, 
        {
            "location": "/visualizations/class_activation_maps/#what-is-a-class-activation-map", 
            "text": "Class activation maps or grad-CAM is another way of visualizing attention over input. Instead of using gradients with\nrespect to output (see  saliency ), grad-CAM uses penultimate (pre  Dense  layer)  Conv  layer output. The\nintuition is to use the nearest  Conv  layer to utilize spatial information that gets completely lost in  Dense  layers.  In keras-vis, we use  grad-CAM  as its considered more general than  Class Activation maps .", 
            "title": "What is a Class Activation Map?"
        }, 
        {
            "location": "/visualizations/class_activation_maps/#usage", 
            "text": "There are two APIs exposed to visualize grad-CAM and are almost identical to  saliency usage .   visualize_cam : This is the general purpose API for visualizing\ngrad-CAM.  visualize_cam_with_losses : This is intended for \nresearch use-cases where some custom weighted loss can be used.   The only notable addition is the  penultimate_layer_idx  parameter. This can be used to specify the pre-layer\nwhose output gradients are used. By default, keras-vis will search for the nearest layer with filters.", 
            "title": "Usage"
        }, 
        {
            "location": "/visualizations/class_activation_maps/#scenarios", 
            "text": "See  saliency scenarios . Everything is identical expect the added  penultimate_layer_idx  param.", 
            "title": "Scenarios"
        }, 
        {
            "location": "/visualizations/class_activation_maps/#gotchas", 
            "text": "grad-CAM only works well if the penultimate layer is close to the layer being visualized. This also applies to  Conv  \nfilter visualizations. You are better off using saliency of this is not the case with your model.", 
            "title": "Gotchas"
        }, 
        {
            "location": "/vis.visualization/", 
            "text": "Source:\n \nvis/visualization/\ninit\n.py#L0\n\n\n\n\nvisualize_activation_with_losses\n\n\nvisualize_activation_with_losses(input_tensor, losses, seed_input=None, input_range=(0, 255), \\\n    **optimizer_params)\n\n\n\n\nGenerates the \ninput_tensor\n that minimizes the weighted \nlosses\n. This function is intended for advanced\nuse cases where a custom loss is desired.\n\n\nArgs:\n\n\n\n\ninput_tensor\n:  An input tensor of shape: \n(samples, channels, image_dims...)\n if \nimage_data_format=\n  channels_first\n or \n(samples, image_dims..., channels)\n if \nimage_data_format=channels_last\n.\n\n\nlosses\n:  List of (\nLoss\n, weight) tuples.\n\n\nseed_input\n:  Seeds the optimization with a starting image. Initialized with a random value when set to None.\n  (Default value = None)\n\n\ninput_range\n:  Specifies the input range as a \n(min, max)\n tuple. This is used to rescale the\n  final optimized input to the given range. (Default value=(0, 255))\n\n\noptimizer_params\n:  The **kwargs for optimizer \nparams\n. Will default to\n  reasonable values when required keys are not found.\n\n\n\n\nReturns:\n\n\nThe model input that minimizes the weighted \nlosses\n.\n\n\n\n\nget_num_filters\n\n\nget_num_filters(layer)\n\n\n\n\nDetermines the number of filters within the give \nlayer\n.\n\n\nReturns:\n\n\nTotal number of filters within \nlayer\n.\n  For \nkeras.layers.Dense\n layer, this is the total number of outputs.\n\n\n\n\noverlay\n\n\noverlay(array1, array2, alpha=0.5)\n\n\n\n\nOverlays \narray1\n onto \narray2\n with \nalpha\n blending.\n\n\nArgs:\n\n\n\n\narray1\n:  The first numpy array.\n\n\narray2\n:  The second numpy array.\n\n\nalpha\n:  The alpha value of \narray1\n as overlayed onto \narray2\n. This value needs to be between [0, 1],\n  with 0 being \narray2\n only to 1 being \narray1\n only (Default value = 0.5).\n\n\n\n\nReturns:\n\n\nThe \narray1\n, overlayed with \narray2\n using \nalpha\n blending.\n\n\n\n\nvisualize_saliency_with_losses\n\n\nvisualize_saliency_with_losses(input_tensor, losses, seed_input, grad_modifier=\nabsolute\n)\n\n\n\n\nGenerates an attention heatmap over the \nseed_input\n by using positive gradients of \ninput_tensor\n\nwith respect to weighted \nlosses\n.\n\n\nThis function is intended for advanced use cases where a custom loss is desired. For common use cases,\nrefer to \nvisualize_class_saliency\n or \nvisualize_regression_saliency\n.\n\n\nFor a full description of saliency, see the paper:\n[Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps]\n(https://arxiv.org/pdf/1312.6034v2.pdf)\n\n\nArgs:\n\n\n\n\ninput_tensor\n:  An input tensor of shape: \n(samples, channels, image_dims...)\n if \nimage_data_format=\n  channels_first\n or \n(samples, image_dims..., channels)\n if \nimage_data_format=channels_last\n.\n\n\nlosses\n:  List of (\nLoss\n, weight) tuples.\n\n\nseed_input\n:  The model input for which activation map needs to be visualized.\n\n\n\n\nReturns:\n\n\nThe heatmap image indicating the \nseed_input\n regions whose change would most contribute towards minimizing\nweighted \nlosses\n.\n\n\n\n\nvisualize_activation\n\n\nvisualize_activation(model, layer_idx, filter_indices=None, seed_input=None, input_range=(0, 255), \\\n    backprop_modifier=None, grad_modifier=None, act_max_weight=1, lp_norm_weight=10, \\\n    tv_weight=10, **optimizer_params)\n\n\n\n\nGenerates the model input that maximizes the output of all \nfilter_indices\n in the given \nlayer_idx\n.\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance. The model input shape must be: \n(samples, channels, image_dims...)\n\n  if \nimage_data_format=channels_first\n or \n(samples, image_dims..., channels)\n if\n  \nimage_data_format=channels_last\n.\n\n\nlayer_idx\n:  The layer index within \nmodel.layers\n whose filters needs to be visualized.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  If None, all filters are visualized. (Default value = None)\n\n\n\n\nFor \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\nIf you are visualizing final \nkeras.layers.Dense\n layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nseed_input\n:  Seeds the optimization with a starting input. Initialized with a random value when set to None.\n  (Default value = None)\n\n\ninput_range\n:  Specifies the input range as a \n(min, max)\n tuple. This is used to rescale the\n  final optimized input to the given range. (Default value=(0, 255))\n\n\nact_max_weight\n:  The weight param for \nActivationMaximization\n loss. Not used if 0 or None. (Default value = 1)\n\n\nlp_norm_weight\n:  The weight param for \nLPNorm\n regularization loss. Not used if 0 or None. (Default value = 10)\n\n\ntv_weight\n:  The weight param for \nTotalVariation\n regularization loss. Not used if 0 or None. (Default value = 10)\n\n\noptimizer_params\n:  The **kwargs for optimizer \nparams\n. Will default to\n  reasonable values when required keys are not found.\n\n\n\n\nExample:\n\n\nIf you wanted to visualize the input image that would maximize the output index 22, say on\nfinal \nkeras.layers.Dense\n layer, then, \nfilter_indices = [22]\n, \nlayer_idx = dense_layer_idx\n.\n\n\nIf \nfilter_indices = [22, 23]\n, then it should generate an input image that shows features of both classes.\n\n\nReturns:\n\n\nThe model input that maximizes the output of \nfilter_indices\n in the given \nlayer_idx\n.\n\n\n\n\nvisualize_saliency\n\n\nvisualize_saliency(model, layer_idx, filter_indices, seed_input, backprop_modifier=None, \\\n    grad_modifier=\nabsolute\n)\n\n\n\n\nGenerates an attention heatmap over the \nseed_input\n for maximizing \nfilter_indices\n\noutput in the given \nlayer_idx\n.\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance. The model input shape must be: \n(samples, channels, image_dims...)\n\n  if \nimage_data_format=channels_first\n or \n(samples, image_dims..., channels)\n if\n  \nimage_data_format=channels_last\n.\n\n\nlayer_idx\n:  The layer index within \nmodel.layers\n whose filters needs to be visualized.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  For \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\n\n\nIf you are visualizing final \nkeras.layers.Dense\n layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nseed_input\n:  The model input for which activation map needs to be visualized.\n\n\n\n\nExample:\n\n\nIf you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal \nkeras.layers.Dense\n layer, then, \nfilter_indices = [22]\n, \nlayer = dense_layer\n.\n\n\nOne could also set filter indices to more than one value. For example, \nfilter_indices = [22, 23]\n should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.\n\n\nReturns:\n\n\nThe heatmap image indicating the \nseed_input\n regions whose change would most contribute towards\nmaximizing the output of \nfilter_indices\n.\n\n\n\n\nvisualize_cam_with_losses\n\n\nvisualize_cam_with_losses(input_tensor, losses, seed_input, penultimate_layer, grad_modifier=None)\n\n\n\n\nGenerates a gradient based class activation map (CAM) by using positive gradients of \ninput_tensor\n\nwith respect to weighted \nlosses\n.\n\n\nFor details on grad-CAM, see the paper:\n[Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization]\n(https://arxiv.org/pdf/1610.02391v1.pdf).\n\n\nUnlike \nclass activation mapping\n, which requires minor changes to\nnetwork architecture in some instances, grad-CAM has a more general applicability.\n\n\nCompared to saliency maps, grad-CAM is class discriminative; i.e., the 'cat' explanation exclusively highlights\ncat regions and not the 'dog' region and vice-versa.\n\n\nArgs:\n\n\n\n\ninput_tensor\n:  An input tensor of shape: \n(samples, channels, image_dims...)\n if \nimage_data_format=\n  channels_first\n or \n(samples, image_dims..., channels)\n if \nimage_data_format=channels_last\n.\n\n\nlosses\n:  List of (\nLoss\n, weight) tuples.\n\n\nseed_input\n:  The model input for which activation map needs to be visualized.\n\n\npenultimate_layer\n:  The pre-layer to \nlayer_idx\n whose feature maps should be used to compute gradients\n  with respect to filter output.\n\n\n\n\nNotes:\n\n\nThis technique deprecates occlusion maps as it gives similar results, but with one-pass gradient computation\nas opposed inefficient sliding window approach.\n\n\nReturns:\n\n\nThe heatmap image indicating the \nseed_input\n regions whose change would most contribute towards minimizing the\nweighted \nlosses\n.\n\n\n\n\nvisualize_cam\n\n\nvisualize_cam(model, layer_idx, filter_indices, seed_input, penultimate_layer_idx=None, \\\n    backprop_modifier=None, grad_modifier=None)\n\n\n\n\nGenerates a gradient based class activation map (grad-CAM) that maximizes the outputs of\n\nfilter_indices\n in \nlayer_idx\n.\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance. The model input shape must be: \n(samples, channels, image_dims...)\n\n  if \nimage_data_format=channels_first\n or \n(samples, image_dims..., channels)\n if\n  \nimage_data_format=channels_last\n.\n\n\nlayer_idx\n:  The layer index within \nmodel.layers\n whose filters needs to be visualized.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  For \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\n\n\nIf you are visualizing final \nkeras.layers.Dense\n layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nseed_input\n:  The input image for which activation map needs to be visualized.\n\n\npenultimate_layer_idx\n:  The pre-layer to \nlayer_idx\n whose feature maps should be used to compute gradients\n  wrt filter output. If not provided, it is set to the nearest penultimate \nConv\n or \nPooling\n layer.\n\n\n\n\nExample:\n\n\nIf you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal \nkeras.layers.Dense\n layer, then, \nfilter_indices = [22]\n, \nlayer = dense_layer\n.\n\n\nOne could also set filter indices to more than one value. For example, \nfilter_indices = [22, 23]\n should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.\n\n\nNotes:\n\n\nThis technique deprecates occlusion maps as it gives similar results, but with one-pass gradient computation\nas opposed inefficient sliding window approach.\n\n\nReturns:\n\n\nThe heatmap image indicating the input regions whose change would most contribute towards\nmaximizing the output of \nfilter_indices\n.", 
            "title": "Visualization"
        }, 
        {
            "location": "/vis.visualization/#visualize_activation_with_losses", 
            "text": "visualize_activation_with_losses(input_tensor, losses, seed_input=None, input_range=(0, 255), \\\n    **optimizer_params)  Generates the  input_tensor  that minimizes the weighted  losses . This function is intended for advanced\nuse cases where a custom loss is desired.  Args:   input_tensor :  An input tensor of shape:  (samples, channels, image_dims...)  if  image_data_format=\n  channels_first  or  (samples, image_dims..., channels)  if  image_data_format=channels_last .  losses :  List of ( Loss , weight) tuples.  seed_input :  Seeds the optimization with a starting image. Initialized with a random value when set to None.\n  (Default value = None)  input_range :  Specifies the input range as a  (min, max)  tuple. This is used to rescale the\n  final optimized input to the given range. (Default value=(0, 255))  optimizer_params :  The **kwargs for optimizer  params . Will default to\n  reasonable values when required keys are not found.   Returns:  The model input that minimizes the weighted  losses .", 
            "title": "visualize_activation_with_losses"
        }, 
        {
            "location": "/vis.visualization/#get_num_filters", 
            "text": "get_num_filters(layer)  Determines the number of filters within the give  layer .  Returns:  Total number of filters within  layer .\n  For  keras.layers.Dense  layer, this is the total number of outputs.", 
            "title": "get_num_filters"
        }, 
        {
            "location": "/vis.visualization/#overlay", 
            "text": "overlay(array1, array2, alpha=0.5)  Overlays  array1  onto  array2  with  alpha  blending.  Args:   array1 :  The first numpy array.  array2 :  The second numpy array.  alpha :  The alpha value of  array1  as overlayed onto  array2 . This value needs to be between [0, 1],\n  with 0 being  array2  only to 1 being  array1  only (Default value = 0.5).   Returns:  The  array1 , overlayed with  array2  using  alpha  blending.", 
            "title": "overlay"
        }, 
        {
            "location": "/vis.visualization/#visualize_saliency_with_losses", 
            "text": "visualize_saliency_with_losses(input_tensor, losses, seed_input, grad_modifier= absolute )  Generates an attention heatmap over the  seed_input  by using positive gradients of  input_tensor \nwith respect to weighted  losses .  This function is intended for advanced use cases where a custom loss is desired. For common use cases,\nrefer to  visualize_class_saliency  or  visualize_regression_saliency .  For a full description of saliency, see the paper:\n[Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps]\n(https://arxiv.org/pdf/1312.6034v2.pdf)  Args:   input_tensor :  An input tensor of shape:  (samples, channels, image_dims...)  if  image_data_format=\n  channels_first  or  (samples, image_dims..., channels)  if  image_data_format=channels_last .  losses :  List of ( Loss , weight) tuples.  seed_input :  The model input for which activation map needs to be visualized.   Returns:  The heatmap image indicating the  seed_input  regions whose change would most contribute towards minimizing\nweighted  losses .", 
            "title": "visualize_saliency_with_losses"
        }, 
        {
            "location": "/vis.visualization/#visualize_activation", 
            "text": "visualize_activation(model, layer_idx, filter_indices=None, seed_input=None, input_range=(0, 255), \\\n    backprop_modifier=None, grad_modifier=None, act_max_weight=1, lp_norm_weight=10, \\\n    tv_weight=10, **optimizer_params)  Generates the model input that maximizes the output of all  filter_indices  in the given  layer_idx .  Args:   model :  The  keras.models.Model  instance. The model input shape must be:  (samples, channels, image_dims...) \n  if  image_data_format=channels_first  or  (samples, image_dims..., channels)  if\n   image_data_format=channels_last .  layer_idx :  The layer index within  model.layers  whose filters needs to be visualized.  filter_indices :  filter indices within the layer to be maximized.\n  If None, all filters are visualized. (Default value = None)   For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.  If you are visualizing final  keras.layers.Dense  layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.   seed_input :  Seeds the optimization with a starting input. Initialized with a random value when set to None.\n  (Default value = None)  input_range :  Specifies the input range as a  (min, max)  tuple. This is used to rescale the\n  final optimized input to the given range. (Default value=(0, 255))  act_max_weight :  The weight param for  ActivationMaximization  loss. Not used if 0 or None. (Default value = 1)  lp_norm_weight :  The weight param for  LPNorm  regularization loss. Not used if 0 or None. (Default value = 10)  tv_weight :  The weight param for  TotalVariation  regularization loss. Not used if 0 or None. (Default value = 10)  optimizer_params :  The **kwargs for optimizer  params . Will default to\n  reasonable values when required keys are not found.   Example:  If you wanted to visualize the input image that would maximize the output index 22, say on\nfinal  keras.layers.Dense  layer, then,  filter_indices = [22] ,  layer_idx = dense_layer_idx .  If  filter_indices = [22, 23] , then it should generate an input image that shows features of both classes.  Returns:  The model input that maximizes the output of  filter_indices  in the given  layer_idx .", 
            "title": "visualize_activation"
        }, 
        {
            "location": "/vis.visualization/#visualize_saliency", 
            "text": "visualize_saliency(model, layer_idx, filter_indices, seed_input, backprop_modifier=None, \\\n    grad_modifier= absolute )  Generates an attention heatmap over the  seed_input  for maximizing  filter_indices \noutput in the given  layer_idx .  Args:   model :  The  keras.models.Model  instance. The model input shape must be:  (samples, channels, image_dims...) \n  if  image_data_format=channels_first  or  (samples, image_dims..., channels)  if\n   image_data_format=channels_last .  layer_idx :  The layer index within  model.layers  whose filters needs to be visualized.  filter_indices :  filter indices within the layer to be maximized.\n  For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.   If you are visualizing final  keras.layers.Dense  layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.   seed_input :  The model input for which activation map needs to be visualized.   Example:  If you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal  keras.layers.Dense  layer, then,  filter_indices = [22] ,  layer = dense_layer .  One could also set filter indices to more than one value. For example,  filter_indices = [22, 23]  should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.  Returns:  The heatmap image indicating the  seed_input  regions whose change would most contribute towards\nmaximizing the output of  filter_indices .", 
            "title": "visualize_saliency"
        }, 
        {
            "location": "/vis.visualization/#visualize_cam_with_losses", 
            "text": "visualize_cam_with_losses(input_tensor, losses, seed_input, penultimate_layer, grad_modifier=None)  Generates a gradient based class activation map (CAM) by using positive gradients of  input_tensor \nwith respect to weighted  losses .  For details on grad-CAM, see the paper:\n[Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization]\n(https://arxiv.org/pdf/1610.02391v1.pdf).  Unlike  class activation mapping , which requires minor changes to\nnetwork architecture in some instances, grad-CAM has a more general applicability.  Compared to saliency maps, grad-CAM is class discriminative; i.e., the 'cat' explanation exclusively highlights\ncat regions and not the 'dog' region and vice-versa.  Args:   input_tensor :  An input tensor of shape:  (samples, channels, image_dims...)  if  image_data_format=\n  channels_first  or  (samples, image_dims..., channels)  if  image_data_format=channels_last .  losses :  List of ( Loss , weight) tuples.  seed_input :  The model input for which activation map needs to be visualized.  penultimate_layer :  The pre-layer to  layer_idx  whose feature maps should be used to compute gradients\n  with respect to filter output.   Notes:  This technique deprecates occlusion maps as it gives similar results, but with one-pass gradient computation\nas opposed inefficient sliding window approach.  Returns:  The heatmap image indicating the  seed_input  regions whose change would most contribute towards minimizing the\nweighted  losses .", 
            "title": "visualize_cam_with_losses"
        }, 
        {
            "location": "/vis.visualization/#visualize_cam", 
            "text": "visualize_cam(model, layer_idx, filter_indices, seed_input, penultimate_layer_idx=None, \\\n    backprop_modifier=None, grad_modifier=None)  Generates a gradient based class activation map (grad-CAM) that maximizes the outputs of filter_indices  in  layer_idx .  Args:   model :  The  keras.models.Model  instance. The model input shape must be:  (samples, channels, image_dims...) \n  if  image_data_format=channels_first  or  (samples, image_dims..., channels)  if\n   image_data_format=channels_last .  layer_idx :  The layer index within  model.layers  whose filters needs to be visualized.  filter_indices :  filter indices within the layer to be maximized.\n  For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.   If you are visualizing final  keras.layers.Dense  layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.   seed_input :  The input image for which activation map needs to be visualized.  penultimate_layer_idx :  The pre-layer to  layer_idx  whose feature maps should be used to compute gradients\n  wrt filter output. If not provided, it is set to the nearest penultimate  Conv  or  Pooling  layer.   Example:  If you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal  keras.layers.Dense  layer, then,  filter_indices = [22] ,  layer = dense_layer .  One could also set filter indices to more than one value. For example,  filter_indices = [22, 23]  should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.  Notes:  This technique deprecates occlusion maps as it gives similar results, but with one-pass gradient computation\nas opposed inefficient sliding window approach.  Returns:  The heatmap image indicating the input regions whose change would most contribute towards\nmaximizing the output of  filter_indices .", 
            "title": "visualize_cam"
        }, 
        {
            "location": "/vis.losses/", 
            "text": "Source:\n \nvis/losses.py#L0\n\n\n\n\nLoss\n\n\nAbstract class for defining the loss function to be minimized.\nThe loss function should be built by defining \nbuild_loss\n function.\n\n\nThe attribute \nname\n should be defined to identify loss function with verbose outputs.\nDefaults to 'Unnamed Loss' if not overridden.\n\n\n\n\nLoss.\n__init__\n\n\n__init__(self)\n\n\n\n\n\n\nLoss.build_loss\n\n\nbuild_loss(self)\n\n\n\n\nImplement this function to build the loss function expression.\nAny additional arguments required to build this loss function may be passed in via \n__init__\n.\n\n\nIdeally, the function expression must be compatible with all keras backends and \nchannels_first\n or\n\nchannels_last\n image_data_format(s). \nutils.slicer\n can be used to define data format agnostic slices.\n(just define it in \nchannels_first\n format, it will automatically shuffle indices for tensorflow\nwhich uses \nchannels_last\n format).\n\n\n# theano slice\nconv_layer[:, filter_idx, ...]\n\n# TF slice\nconv_layer[..., filter_idx]\n\n# Backend agnostic slice\nconv_layer[utils.slicer[:, filter_idx, ...]]\n\n\n\n\nutils.get_img_shape\n and\n\nutils.get_img_indices\n are other optional utilities that make this easier.\n\n\nReturns:\n\n\nThe loss expression.\n\n\n\n\nActivationMaximization\n\n\nA loss function that maximizes the activation of a set of filters within a particular layer.\n\n\nTypically this loss is used to ask the reverse question - What kind of input image would increase the networks\nconfidence, for say, dog class. This helps determine what the network might be internalizing as being the 'dog'\nimage space.\n\n\nOne might also use this to generate an input image that maximizes both 'dog' and 'human' outputs on the final\n\nkeras.layers.Dense\n layer.\n\n\n\n\nActivationMaximization.\n__init__\n\n\n__init__(self, layer, filter_indices)\n\n\n\n\nArgs:\n\n\n\n\nlayer\n:  The keras layer whose filters need to be maximized. This can either be a convolutional layer\n  or a dense layer.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  For \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\n\n\nIf you are optimizing final \nkeras.layers.Dense\n layer to maximize class output, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nActivationMaximization.build_loss\n\n\nbuild_loss(self)", 
            "title": "losses"
        }, 
        {
            "location": "/vis.losses/#loss", 
            "text": "Abstract class for defining the loss function to be minimized.\nThe loss function should be built by defining  build_loss  function.  The attribute  name  should be defined to identify loss function with verbose outputs.\nDefaults to 'Unnamed Loss' if not overridden.", 
            "title": "Loss"
        }, 
        {
            "location": "/vis.losses/#loss__init__", 
            "text": "__init__(self)", 
            "title": "Loss.__init__"
        }, 
        {
            "location": "/vis.losses/#lossbuild_loss", 
            "text": "build_loss(self)  Implement this function to build the loss function expression.\nAny additional arguments required to build this loss function may be passed in via  __init__ .  Ideally, the function expression must be compatible with all keras backends and  channels_first  or channels_last  image_data_format(s).  utils.slicer  can be used to define data format agnostic slices.\n(just define it in  channels_first  format, it will automatically shuffle indices for tensorflow\nwhich uses  channels_last  format).  # theano slice\nconv_layer[:, filter_idx, ...]\n\n# TF slice\nconv_layer[..., filter_idx]\n\n# Backend agnostic slice\nconv_layer[utils.slicer[:, filter_idx, ...]]  utils.get_img_shape  and utils.get_img_indices  are other optional utilities that make this easier.  Returns:  The loss expression.", 
            "title": "Loss.build_loss"
        }, 
        {
            "location": "/vis.losses/#activationmaximization", 
            "text": "A loss function that maximizes the activation of a set of filters within a particular layer.  Typically this loss is used to ask the reverse question - What kind of input image would increase the networks\nconfidence, for say, dog class. This helps determine what the network might be internalizing as being the 'dog'\nimage space.  One might also use this to generate an input image that maximizes both 'dog' and 'human' outputs on the final keras.layers.Dense  layer.", 
            "title": "ActivationMaximization"
        }, 
        {
            "location": "/vis.losses/#activationmaximization__init__", 
            "text": "__init__(self, layer, filter_indices)  Args:   layer :  The keras layer whose filters need to be maximized. This can either be a convolutional layer\n  or a dense layer.  filter_indices :  filter indices within the layer to be maximized.\n  For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.   If you are optimizing final  keras.layers.Dense  layer to maximize class output, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.", 
            "title": "ActivationMaximization.__init__"
        }, 
        {
            "location": "/vis.losses/#activationmaximizationbuild_loss", 
            "text": "build_loss(self)", 
            "title": "ActivationMaximization.build_loss"
        }, 
        {
            "location": "/vis.regularizers/", 
            "text": "Source:\n \nvis/regularizers.py#L0\n\n\n\n\nnormalize\n\n\nnormalize(input_tensor, output_tensor)\n\n\n\n\nNormalizes the \noutput_tensor\n with respect to \ninput_tensor\n dimensions.\nThis makes regularizer weight factor more or less uniform across various input image dimensions.\n\n\nArgs:\n\n\n\n\ninput_tensor\n:  An tensor of shape: \n(samples, channels, image_dims...)\n if \nimage_data_format=\n  channels_first\n or \n(samples, image_dims..., channels)\n if \nimage_data_format=channels_last\n.\n\n\noutput_tensor\n:  The tensor to normalize.\n\n\n\n\nReturns:\n\n\nThe normalized tensor.\n\n\n\n\nTotalVariation\n\n\n\n\nTotalVariation.\n__init__\n\n\n__init__(self, img_input, beta=2.0)\n\n\n\n\nTotal variation regularizer encourages blobbier and coherent image structures, akin to natural images.\nSee \nsection 3.2.2\n in\n\nVisualizing deep convolutional neural networks using natural pre-images\n\nfor details.\n\n\nArgs:\n\n\n\n\nimg_input\n:  An image tensor of shape: \n(samples, channels, image_dims...)\n if \nimage_data_format=\nchannels_first\nor\n(samples, image_dims..., channels)\nif\nimage_data_format=channels_last`\n\n\nbeta\n:  Smaller values of beta give sharper but 'spikier' images.\n  Values \n\\in [1.5, 3.0]\n are recommended as a reasonable compromise. (Default value = 2.)\n\n\n\n\n\n\nTotalVariation.build_loss\n\n\nbuild_loss(self)\n\n\n\n\nImplements the N-dim version of function\n\nTV^{\\beta}(x) = \\sum_{whc} \\left ( \\left ( x(h, w+1, c) - x(h, w, c) \\right )^{2} +\n\\left ( x(h+1, w, c) - x(h, w, c) \\right )^{2} \\right )^{\\frac{\\beta}{2}}\n\nto return total variation for all images in the batch.\n\n\n\n\nLPNorm\n\n\n\n\nLPNorm.\n__init__\n\n\n__init__(self, img_input, p=6.0)\n\n\n\n\nBuilds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded.\n  i.e., prevents pixels from taking on very large values.\n\n\nArgs:\n\n\n\n\nimg_input\n:  4D image input tensor to the model of shape: \n(samples, channels, rows, cols)\n\n  if data_format='channels_first' or \n(samples, rows, cols, channels)\n if data_format='channels_last'.\n\n\np\n:  The pth norm to use. If p = float('inf'), infinity-norm will be used.\n\n\n\n\n\n\nLPNorm.build_loss\n\n\nbuild_loss(self)", 
            "title": "regularizers"
        }, 
        {
            "location": "/vis.regularizers/#normalize", 
            "text": "normalize(input_tensor, output_tensor)  Normalizes the  output_tensor  with respect to  input_tensor  dimensions.\nThis makes regularizer weight factor more or less uniform across various input image dimensions.  Args:   input_tensor :  An tensor of shape:  (samples, channels, image_dims...)  if  image_data_format=\n  channels_first  or  (samples, image_dims..., channels)  if  image_data_format=channels_last .  output_tensor :  The tensor to normalize.   Returns:  The normalized tensor.", 
            "title": "normalize"
        }, 
        {
            "location": "/vis.regularizers/#totalvariation", 
            "text": "", 
            "title": "TotalVariation"
        }, 
        {
            "location": "/vis.regularizers/#totalvariation__init__", 
            "text": "__init__(self, img_input, beta=2.0)  Total variation regularizer encourages blobbier and coherent image structures, akin to natural images.\nSee  section 3.2.2  in Visualizing deep convolutional neural networks using natural pre-images \nfor details.  Args:   img_input :  An image tensor of shape:  (samples, channels, image_dims...)  if  image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last`  beta :  Smaller values of beta give sharper but 'spikier' images.\n  Values  \\in [1.5, 3.0]  are recommended as a reasonable compromise. (Default value = 2.)", 
            "title": "TotalVariation.__init__"
        }, 
        {
            "location": "/vis.regularizers/#totalvariationbuild_loss", 
            "text": "build_loss(self)  Implements the N-dim version of function TV^{\\beta}(x) = \\sum_{whc} \\left ( \\left ( x(h, w+1, c) - x(h, w, c) \\right )^{2} +\n\\left ( x(h+1, w, c) - x(h, w, c) \\right )^{2} \\right )^{\\frac{\\beta}{2}} \nto return total variation for all images in the batch.", 
            "title": "TotalVariation.build_loss"
        }, 
        {
            "location": "/vis.regularizers/#lpnorm", 
            "text": "", 
            "title": "LPNorm"
        }, 
        {
            "location": "/vis.regularizers/#lpnorm__init__", 
            "text": "__init__(self, img_input, p=6.0)  Builds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded.\n  i.e., prevents pixels from taking on very large values.  Args:   img_input :  4D image input tensor to the model of shape:  (samples, channels, rows, cols) \n  if data_format='channels_first' or  (samples, rows, cols, channels)  if data_format='channels_last'.  p :  The pth norm to use. If p = float('inf'), infinity-norm will be used.", 
            "title": "LPNorm.__init__"
        }, 
        {
            "location": "/vis.regularizers/#lpnormbuild_loss", 
            "text": "build_loss(self)", 
            "title": "LPNorm.build_loss"
        }, 
        {
            "location": "/vis.optimizer/", 
            "text": "Source:\n \nvis/optimizer.py#L0\n\n\n\n\nOptimizer\n\n\n\n\nOptimizer.\n__init__\n\n\n__init__(self, input_tensor, losses, input_range=(0, 255), wrt_tensor=None, norm_grads=True)\n\n\n\n\nCreates an optimizer that minimizes weighted loss function.\n\n\nArgs:\n\n\n\n\ninput_tensor\n:  An input tensor of shape: \n(samples, channels, image_dims...)\n if \nimage_data_format=\n  channels_first\n or \n(samples, image_dims..., channels)\n if \nimage_data_format=channels_last\n.\n\n\nlosses\n:  List of (\nLoss\n, weight) tuples.\n\n\ninput_range\n:  Specifies the input range as a \n(min, max)\n tuple. This is used to rescale the\n  final optimized input to the given range. (Default value=(0, 255))\n\n\nwrt_tensor\n:  Short for, with respect to. This instructs the optimizer that the aggregate loss from \nlosses\n\n  should be minimized with respect to \nwrt\n. \nwrt\n can be any tensor that is part of the model graph.\n  Default value is set to None which means that loss will simply be minimized with respect to \ninput_tensor\n.\n\n\nnorm_grads\n:  True to normalize gradients. Normalization avoids very small or large gradients and ensures\n  a smooth gradient gradient descent process. If you want the actual gradient\n  (for example, visualizing attention), set this to false.\n\n\n\n\n\n\nOptimizer.minimize\n\n\nminimize(self, seed_input=None, max_iter=200, input_modifiers=None, grad_modifier=None, \\\n    callbacks=None, verbose=True)\n\n\n\n\nPerforms gradient descent on the input image with respect to defined losses.\n\n\nArgs:\n\n\n\n\nseed_input\n:  An N-dim numpy array of shape: \n(samples, channels, image_dims...)\n if \nimage_data_format=\n  channels_first\n or \n(samples, image_dims..., channels)\n if \nimage_data_format=channels_last\n.\n  Seeded with random noise if set to None. (Default value = None)\n\n\nmax_iter\n:  The maximum number of gradient descent iterations. (Default value = 200)\n\n\ninput_modifiers\n:  A list of \n../vis/input_modifier/#InputModifier\n instances specifying\n  how to make \npre\n and \npost\n changes to the optimized input during the optimization process.\n\n\n\n\npre\n is applied in list order while \npost\n is applied in reverse order. For example,\n  \ninput_modifiers = [f, g]\n means that \npre_input = g(f(inp))\n and \npost_input = f(g(inp))\n\n - \ncallbacks\n:  A list of \n../vis/callbacks/#OptimizerCallback\n to trigger during optimization.\n - \nverbose\n:  Logs individual losses at the end of every gradient descent iteration.\n  Very useful to estimate loss weight factor(s). (Default value = True)\n\n\nReturns:\n\n\nThe tuple of \n(optimized input, grads with respect to wrt, wrt_value)\n after gradient descent iterations.", 
            "title": "optimizer"
        }, 
        {
            "location": "/vis.optimizer/#optimizer", 
            "text": "", 
            "title": "Optimizer"
        }, 
        {
            "location": "/vis.optimizer/#optimizer__init__", 
            "text": "__init__(self, input_tensor, losses, input_range=(0, 255), wrt_tensor=None, norm_grads=True)  Creates an optimizer that minimizes weighted loss function.  Args:   input_tensor :  An input tensor of shape:  (samples, channels, image_dims...)  if  image_data_format=\n  channels_first  or  (samples, image_dims..., channels)  if  image_data_format=channels_last .  losses :  List of ( Loss , weight) tuples.  input_range :  Specifies the input range as a  (min, max)  tuple. This is used to rescale the\n  final optimized input to the given range. (Default value=(0, 255))  wrt_tensor :  Short for, with respect to. This instructs the optimizer that the aggregate loss from  losses \n  should be minimized with respect to  wrt .  wrt  can be any tensor that is part of the model graph.\n  Default value is set to None which means that loss will simply be minimized with respect to  input_tensor .  norm_grads :  True to normalize gradients. Normalization avoids very small or large gradients and ensures\n  a smooth gradient gradient descent process. If you want the actual gradient\n  (for example, visualizing attention), set this to false.", 
            "title": "Optimizer.__init__"
        }, 
        {
            "location": "/vis.optimizer/#optimizerminimize", 
            "text": "minimize(self, seed_input=None, max_iter=200, input_modifiers=None, grad_modifier=None, \\\n    callbacks=None, verbose=True)  Performs gradient descent on the input image with respect to defined losses.  Args:   seed_input :  An N-dim numpy array of shape:  (samples, channels, image_dims...)  if  image_data_format=\n  channels_first  or  (samples, image_dims..., channels)  if  image_data_format=channels_last .\n  Seeded with random noise if set to None. (Default value = None)  max_iter :  The maximum number of gradient descent iterations. (Default value = 200)  input_modifiers :  A list of  ../vis/input_modifier/#InputModifier  instances specifying\n  how to make  pre  and  post  changes to the optimized input during the optimization process.   pre  is applied in list order while  post  is applied in reverse order. For example,\n   input_modifiers = [f, g]  means that  pre_input = g(f(inp))  and  post_input = f(g(inp)) \n -  callbacks :  A list of  ../vis/callbacks/#OptimizerCallback  to trigger during optimization.\n -  verbose :  Logs individual losses at the end of every gradient descent iteration.\n  Very useful to estimate loss weight factor(s). (Default value = True)  Returns:  The tuple of  (optimized input, grads with respect to wrt, wrt_value)  after gradient descent iterations.", 
            "title": "Optimizer.minimize"
        }, 
        {
            "location": "/vis.callbacks/", 
            "text": "Source:\n \nvis/callbacks.py#L0\n\n\n\n\nOptimizerCallback\n\n\nAbstract class for defining callbacks for use with \noptimizer.minimize\n.\n\n\n\n\nOptimizerCallback.callback\n\n\ncallback(self, i, named_losses, overall_loss, grads, wrt_value)\n\n\n\n\nThis function will be called within \noptimizer.minimize\n.\n\n\nArgs:\n\n\n\n\ni\n:  The optimizer iteration.\n\n\nnamed_losses\n:  List of \n(loss_name, loss_value)\n tuples.\n\n\noverall_loss\n:  Overall weighted loss.\n\n\ngrads\n:  The gradient of input image with respect to \nwrt_value\n.\n\n\nwrt_value\n:  The current \nwrt_value\n.\n\n\n\n\n\n\nOptimizerCallback.on_end\n\n\non_end(self)\n\n\n\n\nCalled at the end of optimization process. This function is typically used to cleanup / close any\nopened resources at the end of optimization.\n\n\n\n\nPrint\n\n\nCallback to print values during optimization.\n\n\n\n\nPrint.callback\n\n\ncallback(self, i, named_losses, overall_loss, grads, wrt_value)\n\n\n\n\n\n\nPrint.on_end\n\n\non_end(self)\n\n\n\n\nCalled at the end of optimization process. This function is typically used to cleanup / close any\nopened resources at the end of optimization.\n\n\n\n\nGifGenerator\n\n\nCallback to construct gif of optimized image.\n\n\n\n\nGifGenerator.\n__init__\n\n\n__init__(self, path)\n\n\n\n\nArgs:\n\n\n\n\npath\n:  The file path to save gif.\n\n\n\n\n\n\nGifGenerator.callback\n\n\ncallback(self, i, named_losses, overall_loss, grads, wrt_value)\n\n\n\n\n\n\nGifGenerator.on_end\n\n\non_end(self)", 
            "title": "callbacks"
        }, 
        {
            "location": "/vis.callbacks/#optimizercallback", 
            "text": "Abstract class for defining callbacks for use with  optimizer.minimize .", 
            "title": "OptimizerCallback"
        }, 
        {
            "location": "/vis.callbacks/#optimizercallbackcallback", 
            "text": "callback(self, i, named_losses, overall_loss, grads, wrt_value)  This function will be called within  optimizer.minimize .  Args:   i :  The optimizer iteration.  named_losses :  List of  (loss_name, loss_value)  tuples.  overall_loss :  Overall weighted loss.  grads :  The gradient of input image with respect to  wrt_value .  wrt_value :  The current  wrt_value .", 
            "title": "OptimizerCallback.callback"
        }, 
        {
            "location": "/vis.callbacks/#optimizercallbackon_end", 
            "text": "on_end(self)  Called at the end of optimization process. This function is typically used to cleanup / close any\nopened resources at the end of optimization.", 
            "title": "OptimizerCallback.on_end"
        }, 
        {
            "location": "/vis.callbacks/#print", 
            "text": "Callback to print values during optimization.", 
            "title": "Print"
        }, 
        {
            "location": "/vis.callbacks/#printcallback", 
            "text": "callback(self, i, named_losses, overall_loss, grads, wrt_value)", 
            "title": "Print.callback"
        }, 
        {
            "location": "/vis.callbacks/#printon_end", 
            "text": "on_end(self)  Called at the end of optimization process. This function is typically used to cleanup / close any\nopened resources at the end of optimization.", 
            "title": "Print.on_end"
        }, 
        {
            "location": "/vis.callbacks/#gifgenerator", 
            "text": "Callback to construct gif of optimized image.", 
            "title": "GifGenerator"
        }, 
        {
            "location": "/vis.callbacks/#gifgenerator__init__", 
            "text": "__init__(self, path)  Args:   path :  The file path to save gif.", 
            "title": "GifGenerator.__init__"
        }, 
        {
            "location": "/vis.callbacks/#gifgeneratorcallback", 
            "text": "callback(self, i, named_losses, overall_loss, grads, wrt_value)", 
            "title": "GifGenerator.callback"
        }, 
        {
            "location": "/vis.callbacks/#gifgeneratoron_end", 
            "text": "on_end(self)", 
            "title": "GifGenerator.on_end"
        }, 
        {
            "location": "/vis.input_modifiers/", 
            "text": "Source:\n \nvis/input_modifiers.py#L0\n\n\n\n\nInputModifier\n\n\nAbstract class for defining an input modifier. An input modifier can be used with the\n\nOptimizer.minimize\n to make \npre\n and \npost\n changes to the optimized input\nduring the optimization process.\n\n\nmodifier.pre(seed_input)\n# gradient descent update to img\nmodifier.post(seed_input)\n\n\n\n\n\n\nInputModifier.post\n\n\npost(self, inp)\n\n\n\n\nImplement post gradient descent update modification to the input. If post-processing is not desired,\nsimply ignore the implementation. It returns the unmodified \ninp\n by default.\n\n\nArgs:\n\n\n\n\ninp\n:  An N-dim numpy array of shape: \n(samples, channels, image_dims...)\n if \nimage_data_format=\n  channels_first\n or \n(samples, image_dims..., channels)\n if \nimage_data_format=channels_last\n.\n\n\n\n\nReturns:\n\n\nThe modified post input.\n\n\n\n\nInputModifier.pre\n\n\npre(self, inp)\n\n\n\n\nImplement pre gradient descent update modification to the input. If pre-processing is not desired,\nsimply ignore the implementation. It returns the unmodified \ninp\n by default.\n\n\nArgs:\n\n\n\n\ninp\n:  An N-dim numpy array of shape: \n(samples, channels, image_dims...)\n if \nimage_data_format=\n  channels_first\n or \n(samples, image_dims..., channels)\n if \nimage_data_format=channels_last\n.\n\n\n\n\nReturns:\n\n\nThe modified pre input.\n\n\n\n\nJitter\n\n\n\n\nJitter.\n__init__\n\n\n__init__(self, jitter=0.05)\n\n\n\n\nImplements an input modifier that introduces random jitter in \npre\n.\nJitter has been shown to produce crisper activation maximization images.\n\n\nArgs:\n\n\n\n\njitter\n:  The amount of jitter to apply, scalar or sequence.\n  If a scalar, same jitter is applied to all image dims. If sequence, \njitter\n should contain a value\n  per image dim.\n\n\n\n\nA value between \n[0., 1.]\n is interpreted as a percentage of the image dimension. (Default value: 0.05)\n\n\n\n\nJitter.post\n\n\npost(self, inp)\n\n\n\n\nImplement post gradient descent update modification to the input. If post-processing is not desired,\nsimply ignore the implementation. It returns the unmodified \ninp\n by default.\n\n\nArgs:\n\n\n\n\ninp\n:  An N-dim numpy array of shape: \n(samples, channels, image_dims...)\n if \nimage_data_format=\n  channels_first\n or \n(samples, image_dims..., channels)\n if \nimage_data_format=channels_last\n.\n\n\n\n\nReturns:\n\n\nThe modified post input.\n\n\n\n\nJitter.pre\n\n\npre(self, img)", 
            "title": "input_modifiers"
        }, 
        {
            "location": "/vis.input_modifiers/#inputmodifier", 
            "text": "Abstract class for defining an input modifier. An input modifier can be used with the Optimizer.minimize  to make  pre  and  post  changes to the optimized input\nduring the optimization process.  modifier.pre(seed_input)\n# gradient descent update to img\nmodifier.post(seed_input)", 
            "title": "InputModifier"
        }, 
        {
            "location": "/vis.input_modifiers/#inputmodifierpost", 
            "text": "post(self, inp)  Implement post gradient descent update modification to the input. If post-processing is not desired,\nsimply ignore the implementation. It returns the unmodified  inp  by default.  Args:   inp :  An N-dim numpy array of shape:  (samples, channels, image_dims...)  if  image_data_format=\n  channels_first  or  (samples, image_dims..., channels)  if  image_data_format=channels_last .   Returns:  The modified post input.", 
            "title": "InputModifier.post"
        }, 
        {
            "location": "/vis.input_modifiers/#inputmodifierpre", 
            "text": "pre(self, inp)  Implement pre gradient descent update modification to the input. If pre-processing is not desired,\nsimply ignore the implementation. It returns the unmodified  inp  by default.  Args:   inp :  An N-dim numpy array of shape:  (samples, channels, image_dims...)  if  image_data_format=\n  channels_first  or  (samples, image_dims..., channels)  if  image_data_format=channels_last .   Returns:  The modified pre input.", 
            "title": "InputModifier.pre"
        }, 
        {
            "location": "/vis.input_modifiers/#jitter", 
            "text": "", 
            "title": "Jitter"
        }, 
        {
            "location": "/vis.input_modifiers/#jitter__init__", 
            "text": "__init__(self, jitter=0.05)  Implements an input modifier that introduces random jitter in  pre .\nJitter has been shown to produce crisper activation maximization images.  Args:   jitter :  The amount of jitter to apply, scalar or sequence.\n  If a scalar, same jitter is applied to all image dims. If sequence,  jitter  should contain a value\n  per image dim.   A value between  [0., 1.]  is interpreted as a percentage of the image dimension. (Default value: 0.05)", 
            "title": "Jitter.__init__"
        }, 
        {
            "location": "/vis.input_modifiers/#jitterpost", 
            "text": "post(self, inp)  Implement post gradient descent update modification to the input. If post-processing is not desired,\nsimply ignore the implementation. It returns the unmodified  inp  by default.  Args:   inp :  An N-dim numpy array of shape:  (samples, channels, image_dims...)  if  image_data_format=\n  channels_first  or  (samples, image_dims..., channels)  if  image_data_format=channels_last .   Returns:  The modified post input.", 
            "title": "Jitter.post"
        }, 
        {
            "location": "/vis.input_modifiers/#jitterpre", 
            "text": "pre(self, img)", 
            "title": "Jitter.pre"
        }, 
        {
            "location": "/vis.grad_modifiers/", 
            "text": "Source:\n \nvis/grad_modifiers.py#L0\n\n\n\n\nnegate\n\n\nnegate(grads)\n\n\n\n\nNegates the gradients.\n\n\nArgs:\n\n\n\n\ngrads\n:  A numpy array of grads to use.\n\n\n\n\nReturns:\n\n\nThe negated gradients.\n\n\n\n\nabsolute\n\n\nabsolute(grads)\n\n\n\n\nAbsolutes the gradients.\n\n\nArgs:\n\n\n\n\ngrads\n:  A numpy array of grads to use.\n\n\n\n\nReturns:\n\n\nThe absolute gradients.\n\n\n\n\ninvert\n\n\ninvert(grads)\n\n\n\n\nInverts the gradients.\n\n\nArgs:\n\n\n\n\ngrads\n:  A numpy array of grads to use.\n\n\n\n\nReturns:\n\n\nThe inverted gradients.\n\n\n\n\nrelu\n\n\nrelu(grads)\n\n\n\n\nClips negative gradient values.\n\n\nArgs:\n\n\n\n\ngrads\n:  A numpy array of grads to use.\n\n\n\n\nReturns:\n\n\nThe rectified gradients.\n\n\n\n\nsmall_values\n\n\nsmall_values(grads)\n\n\n\n\nCan be used to highlight small gradient values.\n\n\nArgs:\n\n\n\n\ngrads\n:  A numpy array of grads to use.\n\n\n\n\nReturns:\n\n\nThe modified gradients that highlight small values.\n\n\n\n\nget\n\n\nget(identifier)", 
            "title": "grad_modifiers"
        }, 
        {
            "location": "/vis.grad_modifiers/#negate", 
            "text": "negate(grads)  Negates the gradients.  Args:   grads :  A numpy array of grads to use.   Returns:  The negated gradients.", 
            "title": "negate"
        }, 
        {
            "location": "/vis.grad_modifiers/#absolute", 
            "text": "absolute(grads)  Absolutes the gradients.  Args:   grads :  A numpy array of grads to use.   Returns:  The absolute gradients.", 
            "title": "absolute"
        }, 
        {
            "location": "/vis.grad_modifiers/#invert", 
            "text": "invert(grads)  Inverts the gradients.  Args:   grads :  A numpy array of grads to use.   Returns:  The inverted gradients.", 
            "title": "invert"
        }, 
        {
            "location": "/vis.grad_modifiers/#relu", 
            "text": "relu(grads)  Clips negative gradient values.  Args:   grads :  A numpy array of grads to use.   Returns:  The rectified gradients.", 
            "title": "relu"
        }, 
        {
            "location": "/vis.grad_modifiers/#small_values", 
            "text": "small_values(grads)  Can be used to highlight small gradient values.  Args:   grads :  A numpy array of grads to use.   Returns:  The modified gradients that highlight small values.", 
            "title": "small_values"
        }, 
        {
            "location": "/vis.grad_modifiers/#get", 
            "text": "get(identifier)", 
            "title": "get"
        }, 
        {
            "location": "/vis.backprop_modifiers/", 
            "text": "Source:\n \nvis/backprop_modifiers.py#L0\n\n\n\n\nguided\n\n\nguided(model)\n\n\n\n\nModifies backprop to only propagate positive gradients for positive activations.\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance whose gradient computation needs to be overridden.\n\n\n\n\nReferences:\nDetails on guided back propagation can be found in paper: [String For Simplicity: The All Convolutional Net]\n(https://arxiv.org/pdf/1412.6806.pdf)\n\n\n\n\ndeconv, rectified, relu\n\n\nrelu(model)\n\n\n\n\nModifies backprop to only propagate positive gradients.\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance whose gradient computation needs to be overridden.\n\n\n\n\nReferences:\nDetails can be found in the paper: [Visualizing and Understanding Convolutional Networks]\n(https://arxiv.org/pdf/1311.2901.pdf)\n\n\n\n\nget\n\n\nget(identifier)", 
            "title": "backprop_modifiers"
        }, 
        {
            "location": "/vis.backprop_modifiers/#guided", 
            "text": "guided(model)  Modifies backprop to only propagate positive gradients for positive activations.  Args:   model :  The  keras.models.Model  instance whose gradient computation needs to be overridden.   References:\nDetails on guided back propagation can be found in paper: [String For Simplicity: The All Convolutional Net]\n(https://arxiv.org/pdf/1412.6806.pdf)", 
            "title": "guided"
        }, 
        {
            "location": "/vis.backprop_modifiers/#deconv-rectified-relu", 
            "text": "relu(model)  Modifies backprop to only propagate positive gradients.  Args:   model :  The  keras.models.Model  instance whose gradient computation needs to be overridden.   References:\nDetails can be found in the paper: [Visualizing and Understanding Convolutional Networks]\n(https://arxiv.org/pdf/1311.2901.pdf)", 
            "title": "deconv, rectified, relu"
        }, 
        {
            "location": "/vis.backprop_modifiers/#get", 
            "text": "get(identifier)", 
            "title": "get"
        }, 
        {
            "location": "/vis.utils.utils/", 
            "text": "Source:\n \nvis/utils/utils.py#L0\n\n\nGlobal Variables\n\n\n\n\nslicer\n\n\n\n\n\n\nset_random_seed\n\n\nset_random_seed(seed_value=1337)\n\n\n\n\nSets random seed value for reproducibility.\n\n\nArgs:\n\n\n\n\nseed_value\n:  The seed value to use. (Default Value = infamous 1337)\n\n\n\n\n\n\nreverse_enumerate\n\n\nreverse_enumerate(iterable)\n\n\n\n\nEnumerate over an iterable in reverse order while retaining proper indexes, without creating any copies.\n\n\n\n\nlistify\n\n\nlistify(value)\n\n\n\n\nEnsures that the value is a list. If it is not a list, it creates a new list with \nvalue\n as an item.\n\n\n\n\nadd_defaults_to_kwargs\n\n\nadd_defaults_to_kwargs(defaults, **kwargs)\n\n\n\n\nUpdates \nkwargs\n with dict of \ndefaults\n\n\nArgs:\n\n\n\n\ndefaults\n:  A dictionary of keys and values\n**kwargs: The kwargs to update.\n\n\n\n\nReturns:\n\n\nThe updated kwargs.\n\n\n\n\nget_identifier\n\n\nget_identifier(identifier, module_globals, module_name)\n\n\n\n\nHelper utility to retrieve the callable function associated with a string identifier.\n\n\nArgs:\n\n\n\n\nidentifier\n:  The identifier. Could be a string or function.\n\n\nmodule_globals\n:  The global objects of the module.\n\n\nmodule_name\n:  The module name\n\n\n\n\nReturns:\n\n\nThe callable associated with the identifier.\n\n\n\n\napply_modifications\n\n\napply_modifications(model)\n\n\n\n\nApplies modifications to the model layers to create a new Graph. For example, simply changing\n\nmodel.layers[idx].activation = new activation\n does not change the graph. The entire graph needs to be updated\nwith modified inbound and outbound tensors because of change in layer building function.\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance.\n\n\n\n\nReturns:\n\n\nThe modified model with changes applied. Does not mutate the original \nmodel\n.\n\n\n\n\nrandom_array\n\n\nrandom_array(shape, mean=128.0, std=20.0)\n\n\n\n\nCreates a uniformly distributed random array with the given \nmean\n and \nstd\n.\n\n\nArgs:\n\n\n\n\nshape\n:  The desired shape\n\n\nmean\n:  The desired mean (Default value = 128)\n\n\nstd\n:  The desired std (Default value = 20)\n\n\n\n\nReturns: Random numpy array of given \nshape\n uniformly distributed with desired \nmean\n and \nstd\n.\n\n\n\n\nfind_layer_idx\n\n\nfind_layer_idx(model, layer_name)\n\n\n\n\nLooks up the layer index corresponding to \nlayer_name\n from \nmodel\n.\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance.\n\n\nlayer_name\n:  The name of the layer to lookup.\n\n\n\n\nReturns:\n\n\nThe layer index if found. Raises an exception otherwise.\n\n\n\n\ndeprocess_input\n\n\ndeprocess_input(input_array, input_range=(0, 255))\n\n\n\n\nUtility function to scale the \ninput_array\n to \ninput_range\n throwing away high frequency artifacts.\n\n\nArgs:\n\n\n\n\ninput_array\n:  An N-dim numpy array.\n\n\ninput_range\n:  Specifies the input range as a \n(min, max)\n tuple to rescale the \ninput_array\n.\n\n\n\n\nReturns:\n\n\nThe rescaled \ninput_array\n.\n\n\n\n\nstitch_images\n\n\nstitch_images(images, margin=5, cols=5)\n\n\n\n\nUtility function to stitch images together with a \nmargin\n.\n\n\nArgs:\n\n\n\n\nimages\n:  The array of 2D images to stitch.\n\n\nmargin\n:  The black border margin size between images (Default value = 5)\n\n\ncols\n:  Max number of image cols. New row is created when number of images exceed the column size.\n  (Default value = 5)\n\n\n\n\nReturns:\n\n\nA single numpy image array comprising of input images.\n\n\n\n\nget_img_shape\n\n\nget_img_shape(img)\n\n\n\n\nReturns image shape in a backend agnostic manner.\n\n\nArgs:\n\n\n\n\nimg\n:  An image tensor of shape: \n(channels, image_dims...)\n if data_format='channels_first' or\n  \n(image_dims..., channels)\n if data_format='channels_last'.\n\n\n\n\nReturns:\n\n\nTuple containing image shape information in \n(samples, channels, image_dims...)\n order.\n\n\n\n\nload_img\n\n\nload_img(path, grayscale=False, target_size=None)\n\n\n\n\nUtility function to load an image from disk.\n\n\nArgs:\n\n\n\n\npath\n:  The image file path.\n\n\ngrayscale\n:  True to convert to grayscale image (Default value = False)\n\n\ntarget_size\n:  (w, h) to resize. (Default value = None)\n\n\n\n\nReturns:\n\n\nThe loaded numpy image.\n\n\n\n\nget_imagenet_label\n\n\nget_imagenet_label(indices, join=\n, \n)\n\n\n\n\nUtility function to return the image net label for the final \ndense\n layer output index.\n\n\nArgs:\n\n\n\n\nindices\n:  Could be a single value or an array of indices whose labels needs looking up.\n\n\njoin\n:  When multiple indices are passed, the output labels are joined using this value. (Default Value = ', ')\n\n\n\n\nReturns:\n\n\nImage net label corresponding to the image category.\n\n\n\n\ndraw_text\n\n\ndraw_text(img, text, position=(10, 10), font=\nFreeSans.ttf\n, font_size=14, color=(0, 0, 0))\n\n\n\n\nDraws text over the image. Requires PIL.\n\n\nArgs:\n\n\n\n\nimg\n:  The image to use.\n\n\ntext\n:  The text string to overlay.\n\n\nposition\n:  The text (x, y) position. (Default value = (10, 10))\n\n\nfont\n:  The ttf or open type font to use. (Default value = 'FreeSans.ttf')\n\n\nfont_size\n:  The text font size. (Default value = 12)\n\n\ncolor\n:  The (r, g, b) values for text color. (Default value = (0, 0, 0))\n\n\n\n\nReturns: Image overlayed with text.\n\n\n\n\nbgr2rgb\n\n\nbgr2rgb(img)\n\n\n\n\nConverts an RGB image to BGR and vice versa\n\n\nArgs:\n\n\n\n\nimg\n:  Numpy array in RGB or BGR format\n\n\n\n\nReturns: The converted image format\n\n\n\n\nnormalize\n\n\nnormalize(array, min_value=0.0, max_value=1.0)\n\n\n\n\nNormalizes the numpy array to (min_value, max_value)\n\n\nArgs:\n\n\n\n\narray\n:  The numpy array\n\n\nmin_value\n:  The min value in normalized array (Default value = 0)\n\n\nmax_value\n:  The max value in normalized array (Default value = 1)\n\n\n\n\nReturns:\n\n\nThe array normalized to range between (min_value, max_value)", 
            "title": "utils"
        }, 
        {
            "location": "/vis.utils.utils/#global-variables", 
            "text": "slicer", 
            "title": "Global Variables"
        }, 
        {
            "location": "/vis.utils.utils/#set_random_seed", 
            "text": "set_random_seed(seed_value=1337)  Sets random seed value for reproducibility.  Args:   seed_value :  The seed value to use. (Default Value = infamous 1337)", 
            "title": "set_random_seed"
        }, 
        {
            "location": "/vis.utils.utils/#reverse_enumerate", 
            "text": "reverse_enumerate(iterable)  Enumerate over an iterable in reverse order while retaining proper indexes, without creating any copies.", 
            "title": "reverse_enumerate"
        }, 
        {
            "location": "/vis.utils.utils/#listify", 
            "text": "listify(value)  Ensures that the value is a list. If it is not a list, it creates a new list with  value  as an item.", 
            "title": "listify"
        }, 
        {
            "location": "/vis.utils.utils/#add_defaults_to_kwargs", 
            "text": "add_defaults_to_kwargs(defaults, **kwargs)  Updates  kwargs  with dict of  defaults  Args:   defaults :  A dictionary of keys and values\n**kwargs: The kwargs to update.   Returns:  The updated kwargs.", 
            "title": "add_defaults_to_kwargs"
        }, 
        {
            "location": "/vis.utils.utils/#get_identifier", 
            "text": "get_identifier(identifier, module_globals, module_name)  Helper utility to retrieve the callable function associated with a string identifier.  Args:   identifier :  The identifier. Could be a string or function.  module_globals :  The global objects of the module.  module_name :  The module name   Returns:  The callable associated with the identifier.", 
            "title": "get_identifier"
        }, 
        {
            "location": "/vis.utils.utils/#apply_modifications", 
            "text": "apply_modifications(model)  Applies modifications to the model layers to create a new Graph. For example, simply changing model.layers[idx].activation = new activation  does not change the graph. The entire graph needs to be updated\nwith modified inbound and outbound tensors because of change in layer building function.  Args:   model :  The  keras.models.Model  instance.   Returns:  The modified model with changes applied. Does not mutate the original  model .", 
            "title": "apply_modifications"
        }, 
        {
            "location": "/vis.utils.utils/#random_array", 
            "text": "random_array(shape, mean=128.0, std=20.0)  Creates a uniformly distributed random array with the given  mean  and  std .  Args:   shape :  The desired shape  mean :  The desired mean (Default value = 128)  std :  The desired std (Default value = 20)   Returns: Random numpy array of given  shape  uniformly distributed with desired  mean  and  std .", 
            "title": "random_array"
        }, 
        {
            "location": "/vis.utils.utils/#find_layer_idx", 
            "text": "find_layer_idx(model, layer_name)  Looks up the layer index corresponding to  layer_name  from  model .  Args:   model :  The  keras.models.Model  instance.  layer_name :  The name of the layer to lookup.   Returns:  The layer index if found. Raises an exception otherwise.", 
            "title": "find_layer_idx"
        }, 
        {
            "location": "/vis.utils.utils/#deprocess_input", 
            "text": "deprocess_input(input_array, input_range=(0, 255))  Utility function to scale the  input_array  to  input_range  throwing away high frequency artifacts.  Args:   input_array :  An N-dim numpy array.  input_range :  Specifies the input range as a  (min, max)  tuple to rescale the  input_array .   Returns:  The rescaled  input_array .", 
            "title": "deprocess_input"
        }, 
        {
            "location": "/vis.utils.utils/#stitch_images", 
            "text": "stitch_images(images, margin=5, cols=5)  Utility function to stitch images together with a  margin .  Args:   images :  The array of 2D images to stitch.  margin :  The black border margin size between images (Default value = 5)  cols :  Max number of image cols. New row is created when number of images exceed the column size.\n  (Default value = 5)   Returns:  A single numpy image array comprising of input images.", 
            "title": "stitch_images"
        }, 
        {
            "location": "/vis.utils.utils/#get_img_shape", 
            "text": "get_img_shape(img)  Returns image shape in a backend agnostic manner.  Args:   img :  An image tensor of shape:  (channels, image_dims...)  if data_format='channels_first' or\n   (image_dims..., channels)  if data_format='channels_last'.   Returns:  Tuple containing image shape information in  (samples, channels, image_dims...)  order.", 
            "title": "get_img_shape"
        }, 
        {
            "location": "/vis.utils.utils/#load_img", 
            "text": "load_img(path, grayscale=False, target_size=None)  Utility function to load an image from disk.  Args:   path :  The image file path.  grayscale :  True to convert to grayscale image (Default value = False)  target_size :  (w, h) to resize. (Default value = None)   Returns:  The loaded numpy image.", 
            "title": "load_img"
        }, 
        {
            "location": "/vis.utils.utils/#get_imagenet_label", 
            "text": "get_imagenet_label(indices, join= ,  )  Utility function to return the image net label for the final  dense  layer output index.  Args:   indices :  Could be a single value or an array of indices whose labels needs looking up.  join :  When multiple indices are passed, the output labels are joined using this value. (Default Value = ', ')   Returns:  Image net label corresponding to the image category.", 
            "title": "get_imagenet_label"
        }, 
        {
            "location": "/vis.utils.utils/#draw_text", 
            "text": "draw_text(img, text, position=(10, 10), font= FreeSans.ttf , font_size=14, color=(0, 0, 0))  Draws text over the image. Requires PIL.  Args:   img :  The image to use.  text :  The text string to overlay.  position :  The text (x, y) position. (Default value = (10, 10))  font :  The ttf or open type font to use. (Default value = 'FreeSans.ttf')  font_size :  The text font size. (Default value = 12)  color :  The (r, g, b) values for text color. (Default value = (0, 0, 0))   Returns: Image overlayed with text.", 
            "title": "draw_text"
        }, 
        {
            "location": "/vis.utils.utils/#bgr2rgb", 
            "text": "bgr2rgb(img)  Converts an RGB image to BGR and vice versa  Args:   img :  Numpy array in RGB or BGR format   Returns: The converted image format", 
            "title": "bgr2rgb"
        }, 
        {
            "location": "/vis.utils.utils/#normalize", 
            "text": "normalize(array, min_value=0.0, max_value=1.0)  Normalizes the numpy array to (min_value, max_value)  Args:   array :  The numpy array  min_value :  The min value in normalized array (Default value = 0)  max_value :  The max value in normalized array (Default value = 1)   Returns:  The array normalized to range between (min_value, max_value)", 
            "title": "normalize"
        }, 
        {
            "location": "/vis.backend/", 
            "text": "Source:\n \nvis/backend/\ninit\n.py#L0\n\n\n\n\nmodify_model_backprop\n\n\nmodify_model_backprop(model, backprop_modifier)\n\n\n\n\nCreates a copy of model by modifying all activations to custom op that clips the negative gradients in the\nbackward pass.\n\n\nArgs:\n\n\n\n\nmodel\n:   The \nkeras.models.Model\n instance.\n\n\nbackprop_modifier\n:  One of \n{'guided', 'rectified'}\n\n\n\n\nReturns:\n\n\nA copy of model with modified activations for backwards pass.\n\n\n\n\nset_random_seed\n\n\nset_random_seed(seed_value=1337)\n\n\n\n\nSets random seed value for reproducibility.\n\n\nArgs:\n\n\n\n\nseed_value\n:  The seed value to use. (Default Value = infamous 1337)", 
            "title": "backend"
        }, 
        {
            "location": "/vis.backend/#modify_model_backprop", 
            "text": "modify_model_backprop(model, backprop_modifier)  Creates a copy of model by modifying all activations to custom op that clips the negative gradients in the\nbackward pass.  Args:   model :   The  keras.models.Model  instance.  backprop_modifier :  One of  {'guided', 'rectified'}   Returns:  A copy of model with modified activations for backwards pass.", 
            "title": "modify_model_backprop"
        }, 
        {
            "location": "/vis.backend/#set_random_seed", 
            "text": "set_random_seed(seed_value=1337)  Sets random seed value for reproducibility.  Args:   seed_value :  The seed value to use. (Default Value = infamous 1337)", 
            "title": "set_random_seed"
        }
    ]
}