{
    "docs": [
        {
            "location": "/", 
            "text": "Keras Visualization Toolkit\n\n\n\n\n\n\nkeras-vis is a high-level toolkit for visualizing input images via guided backprop. \nThere are several repositories out there to visualize: \n\n\n\n\nActivation maximization\n\n\nSaliency and class activation maps\n\n\nCaricaturization (deep dream)\n\n\nTexture/Artistic style transfer\n\n\nAny other guided image backprop\n\n\n\n\nThis toolkit generalizes all of the above as energy minimization problems. \nCompatible with both theano and tensorflow backends with 'th'/'tf' image dim orderings. \n\n\nRead the documentation at \nhttps://raghakot.github.io/keras-vis\n. \nJoin the slack \nchannel\n for questions/discussions.\n\n\nGetting Started\n\n\nIn image backprop problems, the goal is to generate an input image that minimizes some loss function.\nSetting up an image backprop problem is easy.\n\n\nDefine weighted loss function\n\n\nVarious useful loss functions are defined in \nlosses\n.\nA custom loss function can be defined by implementing \nLoss.build_loss\n.\n\n\nfrom vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\n\nfilter_indices = [1, 2, 3]\n\n# Tuple consists of (loss_function, weight)\n# Add regularizers as needed.\nlosses = [\n    (ActivationMaximization(keras_layer, filter_indices), 1),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]\n\n\n\n\nConfigure optimizer to minimize weighted loss\n\n\nIn order to generate natural looking images, image search space is constrained using regularization penalties. \nSome common regularizers are defined in \nregularizers\n.\nLike loss functions, custom regularizer can be defined by implementing \n\nLoss.build_loss\n.\n\n\nfrom vis.optimizer import Optimizer\n\noptimizer = Optimizer(model.input, losses)\nopt_img, grads, _ = optimizer.minimize()\n\n\n\n\nConcrete examples of various visualizations can be found in \n\nexamples folder\n.\n\n\nInstallation\n\n\n1) Install \nkeras\n \nwith theano or tensorflow backend\n\n\n2) Install OpenCV \n\n\nsudo apt-get install python-opencv\n\n\n\n\n3) Install keras-vis\n\n\n\n\nFrom sources\n\n\n\n\nsudo python setup.py install\n\n\n\n\n\n\nPyPI package\n\n\n\n\nsudo pip install keras-vis\n\n\n\n\nVisualizations\n\n\nNeural nets are black boxes. In the recent years, several approaches for understanding and visualizing Convolutional \nNetworks have been developed in the literature. They give us a way to peer into the black boxes, \ndiagnose mis-classifications, and assess whether the network is over/under fitting. \n\n\nGuided backprop can also be used to create \ntrippy art\n, neural/texture \n\nstyle transfer\n among the list of other growing applications.\n\n\nVarious visualizations, documented in their own pages, are summarized here.\n\n\n\n\n\nConv filter visualization\n\n\n\n\nConvolutional filters learn 'template matching' filters that maximize the output when a similar template \npattern is found in the input image. Visualize those templates via Activation Maximization.\n\n\n\n\n\nDense layer visualization\n\n\n\n\nHow can we assess whether a network is over/under fitting or generalizing well?\n\n\n\n\n\nAttention Maps\n\n\n\n\nHow can we assess whether a network is attending to correct parts of the image in order to generate a decision?\n\n\n\n\n\nCaricaturization (deep dream)\n\n\nTODO\n\n\n\n\n\nNeural Style Transfer\n\n\nTODO\n\n\n\n\n\nGenerating animated gif of optimization progress\n\n\nIt is possible to generate an animated gif of optimization progress. Below is an example for activation maximization\nof 'ouzel' class (output_index: 20). This example also shows how to use the optimizer directly.\n\n\nfrom vis.utils.vggnet import VGG16\nfrom vis.optimizer import Optimizer\nfrom vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\noutput_class = [20]\n\nlosses = [\n    (ActivationMaximization(layer_dict[layer_name], output_class), 2),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]\nopt = Optimizer(model.input, losses)\n\n# Jitter is used as a regularizer to create crisper images, but it makes gif animation ugly.\nopt.minimize(max_iter=500, verbose=True, jitter=0,\n             progress_gif_path='opt_progress')", 
            "title": "Home"
        }, 
        {
            "location": "/#keras-visualization-toolkit", 
            "text": "keras-vis is a high-level toolkit for visualizing input images via guided backprop. \nThere are several repositories out there to visualize:    Activation maximization  Saliency and class activation maps  Caricaturization (deep dream)  Texture/Artistic style transfer  Any other guided image backprop   This toolkit generalizes all of the above as energy minimization problems. \nCompatible with both theano and tensorflow backends with 'th'/'tf' image dim orderings.   Read the documentation at  https://raghakot.github.io/keras-vis . \nJoin the slack  channel  for questions/discussions.", 
            "title": "Keras Visualization Toolkit"
        }, 
        {
            "location": "/#getting-started", 
            "text": "In image backprop problems, the goal is to generate an input image that minimizes some loss function.\nSetting up an image backprop problem is easy.  Define weighted loss function  Various useful loss functions are defined in  losses .\nA custom loss function can be defined by implementing  Loss.build_loss .  from vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\n\nfilter_indices = [1, 2, 3]\n\n# Tuple consists of (loss_function, weight)\n# Add regularizers as needed.\nlosses = [\n    (ActivationMaximization(keras_layer, filter_indices), 1),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]  Configure optimizer to minimize weighted loss  In order to generate natural looking images, image search space is constrained using regularization penalties. \nSome common regularizers are defined in  regularizers .\nLike loss functions, custom regularizer can be defined by implementing  Loss.build_loss .  from vis.optimizer import Optimizer\n\noptimizer = Optimizer(model.input, losses)\nopt_img, grads, _ = optimizer.minimize()  Concrete examples of various visualizations can be found in  examples folder .", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#installation", 
            "text": "1) Install  keras  \nwith theano or tensorflow backend  2) Install OpenCV   sudo apt-get install python-opencv  3) Install keras-vis   From sources   sudo python setup.py install   PyPI package   sudo pip install keras-vis", 
            "title": "Installation"
        }, 
        {
            "location": "/#visualizations", 
            "text": "Neural nets are black boxes. In the recent years, several approaches for understanding and visualizing Convolutional \nNetworks have been developed in the literature. They give us a way to peer into the black boxes, \ndiagnose mis-classifications, and assess whether the network is over/under fitting.   Guided backprop can also be used to create  trippy art , neural/texture  style transfer  among the list of other growing applications.  Various visualizations, documented in their own pages, are summarized here.", 
            "title": "Visualizations"
        }, 
        {
            "location": "/#conv-filter-visualization", 
            "text": "Convolutional filters learn 'template matching' filters that maximize the output when a similar template \npattern is found in the input image. Visualize those templates via Activation Maximization.", 
            "title": "Conv filter visualization"
        }, 
        {
            "location": "/#dense-layer-visualization", 
            "text": "How can we assess whether a network is over/under fitting or generalizing well?", 
            "title": "Dense layer visualization"
        }, 
        {
            "location": "/#attention-maps", 
            "text": "How can we assess whether a network is attending to correct parts of the image in order to generate a decision?", 
            "title": "Attention Maps"
        }, 
        {
            "location": "/#caricaturization-deep-dream", 
            "text": "TODO", 
            "title": "Caricaturization (deep dream)"
        }, 
        {
            "location": "/#neural-style-transfer", 
            "text": "TODO", 
            "title": "Neural Style Transfer"
        }, 
        {
            "location": "/#generating-animated-gif-of-optimization-progress", 
            "text": "It is possible to generate an animated gif of optimization progress. Below is an example for activation maximization\nof 'ouzel' class (output_index: 20). This example also shows how to use the optimizer directly.  from vis.utils.vggnet import VGG16\nfrom vis.optimizer import Optimizer\nfrom vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\noutput_class = [20]\n\nlosses = [\n    (ActivationMaximization(layer_dict[layer_name], output_class), 2),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]\nopt = Optimizer(model.input, losses)\n\n# Jitter is used as a regularizer to create crisper images, but it makes gif animation ugly.\nopt.minimize(max_iter=500, verbose=True, jitter=0,\n             progress_gif_path='opt_progress')", 
            "title": "Generating animated gif of optimization progress"
        }, 
        {
            "location": "/visualizations/conv_filters/", 
            "text": "Overview\n\n\nEach conv layer has several learned 'template matching' filters that maximize their output when a similar template \npattern is found in the input image. This makes the first conv net layer highly interpretable by simply visualizing \ntheir weights as it is operating over raw pixels.\n\n\nSubsequent conv filters operate over the outputs of previous conv filters (which indicate the presence or absence of \nsome templates), so visualizing them directly is not very interpretable.\n\n\nOne way of interpreting them is to generate an input image that maximizes the filter output. With keras-vis, setting\nthis up is easy. Lets visualize the second conv layer of vggnet (named as 'block1_conv2').\n\n\nimport cv2\nimport numpy as np\n\nfrom vis.utils.utils import stitch_images\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_activation, get_num_filters\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'block1_conv2'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Visualize all filters in this layer.\nfilters = np.arange(get_num_filters(model.layers[layer_idx]))\n\n# Generate input image for each filter. Here `text` field is used to overlay `filter_value` on top of the image.\nvis_images = [visualize_activation(model, layer_idx, filter_indices=idx, text=str(idx))\n              for idx in filters]\n\n# Generate stitched image pallette with 10 cols.\ncv2.imshow(layer_name, stitch_images(vis_images, cols=8))\ncv2.waitKey(0)\n\n\n\n\n\nThis generates the following stitched image representing input image(s) that maximize the filter_idx output.\nThey mostly seem to match for specific color and directional patterns.\n\n\n\n\nLets visualize the remaining conv filters (first few) by iterating over different \nlayer_name\n values.\n\n\nblock2_conv2: random sample of the 128 filters\n\n\n\n\nblock3_conv3: random sample of the 256 filters\n\n\n\n\nblock3_conv4: random sample of the 512 filters\n\n\n\n\nblock3_conv5: random sample of the 512 filters\n\n\n\n\nSome of the 'block5_conv3' filters failed to converge. This is because regularization losses (total variation and \nLP norm) are overtaking activation maximization loss (set \nverbose=True\n to observe). \n\n\nWhenever activation maximization fails to converge, total variation regularization is the typical culprit. \nIt is easier to minimize total variation from a random image (just have to create blobbier color structures), \nand this sets the input image in a bad local minima that makes it difficult to optimize for activation maximization. \nWe can turn off total variation by setting \ntv_weight=0\n. This generates most of the previously unconverged filters.\n\n\n\n\nBy this layer, we can clearly notice templates for complex patterns such as flower buds / corals \n(filters 67, 84 respectively). Notice that images are not as coherent due to lack of total variation loss.\n\n\nA good strategy in these situations might be to seed the optimization with image output generated via tv_weight=0\nand add the tv_weight back. Lets specifically look at filter 67.\n\n\nlayer_name = 'block5_conv3'\n\nno_tv_seed_img = visualize_activation(model, layer_idx, filter_indices=[67],\n                                      tv_weight=0, verbose=True)\npost_tv_img = visualize_activation(model, layer_idx, filter_indices=[67],\n                                   tv_weight=1, seed_img=no_tv_seed_img, verbose=True, max_iter=300)\ncv2.imshow(layer_name, post_tv_img)\ncv2.waitKey(0)\n\n\n\n\nAs expected, this generates a blobbier and smoother image:", 
            "title": "Convolutional Filters"
        }, 
        {
            "location": "/visualizations/conv_filters/#overview", 
            "text": "Each conv layer has several learned 'template matching' filters that maximize their output when a similar template \npattern is found in the input image. This makes the first conv net layer highly interpretable by simply visualizing \ntheir weights as it is operating over raw pixels.  Subsequent conv filters operate over the outputs of previous conv filters (which indicate the presence or absence of \nsome templates), so visualizing them directly is not very interpretable.  One way of interpreting them is to generate an input image that maximizes the filter output. With keras-vis, setting\nthis up is easy. Lets visualize the second conv layer of vggnet (named as 'block1_conv2').  import cv2\nimport numpy as np\n\nfrom vis.utils.utils import stitch_images\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_activation, get_num_filters\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'block1_conv2'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Visualize all filters in this layer.\nfilters = np.arange(get_num_filters(model.layers[layer_idx]))\n\n# Generate input image for each filter. Here `text` field is used to overlay `filter_value` on top of the image.\nvis_images = [visualize_activation(model, layer_idx, filter_indices=idx, text=str(idx))\n              for idx in filters]\n\n# Generate stitched image pallette with 10 cols.\ncv2.imshow(layer_name, stitch_images(vis_images, cols=8))\ncv2.waitKey(0)  This generates the following stitched image representing input image(s) that maximize the filter_idx output.\nThey mostly seem to match for specific color and directional patterns.   Lets visualize the remaining conv filters (first few) by iterating over different  layer_name  values.", 
            "title": "Overview"
        }, 
        {
            "location": "/visualizations/conv_filters/#block2_conv2-random-sample-of-the-128-filters", 
            "text": "", 
            "title": "block2_conv2: random sample of the 128 filters"
        }, 
        {
            "location": "/visualizations/conv_filters/#block3_conv3-random-sample-of-the-256-filters", 
            "text": "", 
            "title": "block3_conv3: random sample of the 256 filters"
        }, 
        {
            "location": "/visualizations/conv_filters/#block3_conv4-random-sample-of-the-512-filters", 
            "text": "", 
            "title": "block3_conv4: random sample of the 512 filters"
        }, 
        {
            "location": "/visualizations/conv_filters/#block3_conv5-random-sample-of-the-512-filters", 
            "text": "Some of the 'block5_conv3' filters failed to converge. This is because regularization losses (total variation and \nLP norm) are overtaking activation maximization loss (set  verbose=True  to observe).   Whenever activation maximization fails to converge, total variation regularization is the typical culprit. \nIt is easier to minimize total variation from a random image (just have to create blobbier color structures), \nand this sets the input image in a bad local minima that makes it difficult to optimize for activation maximization. \nWe can turn off total variation by setting  tv_weight=0 . This generates most of the previously unconverged filters.   By this layer, we can clearly notice templates for complex patterns such as flower buds / corals \n(filters 67, 84 respectively). Notice that images are not as coherent due to lack of total variation loss.  A good strategy in these situations might be to seed the optimization with image output generated via tv_weight=0\nand add the tv_weight back. Lets specifically look at filter 67.  layer_name = 'block5_conv3'\n\nno_tv_seed_img = visualize_activation(model, layer_idx, filter_indices=[67],\n                                      tv_weight=0, verbose=True)\npost_tv_img = visualize_activation(model, layer_idx, filter_indices=[67],\n                                   tv_weight=1, seed_img=no_tv_seed_img, verbose=True, max_iter=300)\ncv2.imshow(layer_name, post_tv_img)\ncv2.waitKey(0)  As expected, this generates a blobbier and smoother image:", 
            "title": "block3_conv5: random sample of the 512 filters"
        }, 
        {
            "location": "/visualizations/dense/", 
            "text": "Overview\n\n\nHow can we assess whether a network is over/under fitting or generalizing well? Given an input image, \nconv net can classify whether it is a cat, bird etc. How can we be sure that it is capturing the correct notion of \nwhat it means to be a bird?\n\n\nOne way to answer these questions is to pose the reverse question:\n\n\n\n\nGenerate an input image that maximizes the final \nDense\n layer output corresponding to bird class. \n\n\n\n\nLets try this for 'ouzel' (imagenet output category: 20)\n\n\nimport cv2\n\nfrom vis.utils.utils import stitch_images\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_activation, get_num_filters\n\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Generate three different images of the same output index.\nvis_images = [visualize_activation(model, layer_idx, filter_indices=idx, text=str(idx), max_iter=500)\n              for idx in [20, 20, 20]]\ncv2.imshow(layer_name, stitch_images(vis_images))\ncv2.waitKey(0)\n\n\n\n\n\nand out comes this..\n\n\n\n\nNot only has the conv net captured what it means to be an ouzel, but it also seems to encode for different orientations \nand scales, a further proof of rotational and scale invariance. \n\n\nLets do this for a few more random imagenet categories.\n\n\n\n\nIf you squint really hard, we can sort of see that most images are more or less accurate representations of the \ncorresponding class.\n\n\nGenerating more natural looking images\n\n\nYou might notice that in most of these visualizations, the same pattern tends to repeat all over the image \nwith different orientations and scales. Why is this the case? If you think about it, it is essentially the consequence\nof using \nActivationMaximization\n loss. Multiple copies of 'template pattern' \nall over the image would certainly maximize the output value.\n\n\nIf we want more natural looking images, we need a better \nnatural image prior\n. A natural image prior is something that\ncaptures the degree of naturalness. By default \nvisualize_activation\n uses:\n\n\n\n\nTotalVariation\n regularizer to Prefer blobbier images. i.e., not bobby images emit higher loss values.\n\n\nLPNorm\n regularizer to limit the color range.\n\n\n\n\nIf we set \ntv_weight=0\n, i.e., turn off total variation regularization, the following is generated:\n\n\n\n\nTotal variation regularizer definitely helps, but in order to get even more natural looking images, we need a better \nimage prior that penalizes unnatural images. Instead of hand-crafting these losses, perhaps the best approach is to \nuse a generative adversarial network (GAN) discriminator. A GAN discriminator is trained to emit probability that the \ninput image is real/fake. To learn more about GANs in general, read: \nUnsupervised Representation Learning with Deep Convolutional \nGenerative Adversarial Networks\n\n\nI am currently in the process of building a GAN regularizer. Stay tuned!", 
            "title": "Dense Layers"
        }, 
        {
            "location": "/visualizations/dense/#overview", 
            "text": "How can we assess whether a network is over/under fitting or generalizing well? Given an input image, \nconv net can classify whether it is a cat, bird etc. How can we be sure that it is capturing the correct notion of \nwhat it means to be a bird?  One way to answer these questions is to pose the reverse question:   Generate an input image that maximizes the final  Dense  layer output corresponding to bird class.    Lets try this for 'ouzel' (imagenet output category: 20)  import cv2\n\nfrom vis.utils.utils import stitch_images\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_activation, get_num_filters\n\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Generate three different images of the same output index.\nvis_images = [visualize_activation(model, layer_idx, filter_indices=idx, text=str(idx), max_iter=500)\n              for idx in [20, 20, 20]]\ncv2.imshow(layer_name, stitch_images(vis_images))\ncv2.waitKey(0)  and out comes this..   Not only has the conv net captured what it means to be an ouzel, but it also seems to encode for different orientations \nand scales, a further proof of rotational and scale invariance.   Lets do this for a few more random imagenet categories.   If you squint really hard, we can sort of see that most images are more or less accurate representations of the \ncorresponding class.", 
            "title": "Overview"
        }, 
        {
            "location": "/visualizations/dense/#generating-more-natural-looking-images", 
            "text": "You might notice that in most of these visualizations, the same pattern tends to repeat all over the image \nwith different orientations and scales. Why is this the case? If you think about it, it is essentially the consequence\nof using  ActivationMaximization  loss. Multiple copies of 'template pattern' \nall over the image would certainly maximize the output value.  If we want more natural looking images, we need a better  natural image prior . A natural image prior is something that\ncaptures the degree of naturalness. By default  visualize_activation  uses:   TotalVariation  regularizer to Prefer blobbier images. i.e., not bobby images emit higher loss values.  LPNorm  regularizer to limit the color range.   If we set  tv_weight=0 , i.e., turn off total variation regularization, the following is generated:   Total variation regularizer definitely helps, but in order to get even more natural looking images, we need a better \nimage prior that penalizes unnatural images. Instead of hand-crafting these losses, perhaps the best approach is to \nuse a generative adversarial network (GAN) discriminator. A GAN discriminator is trained to emit probability that the \ninput image is real/fake. To learn more about GANs in general, read:  Unsupervised Representation Learning with Deep Convolutional \nGenerative Adversarial Networks  I am currently in the process of building a GAN regularizer. Stay tuned!", 
            "title": "Generating more natural looking images"
        }, 
        {
            "location": "/visualizations/attention/", 
            "text": "Overview\n\n\nSuppose that all the training images of 'bird' class contains a tree with leaves. How do we know that the conv net is \nindeed leveraging bird-related pixels as opposed to some other features such as the tree or leaves in the image. \n\n\nAttention maps are a family of methods that try to answer these questions by generating a heatmap over input \nimage that most contributed towards maximizing the probability of an output class.\n\n\nSaliency maps\n\n\nSaliency maps was first introduced in the paper: \n\nDeep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n\n\nThe idea is pretty simple. We compute the gradient of output category with respect to input image. This should tell us\nhow output category value changes with respect to a small change in input image pixels. All the positive values\nin the gradients tell us that a small change to that pixel will increase the output value. \nHence, visualizing these gradients, which are the same shape as the image should provide some intuition of attention.\n\n\nkeras-vis abstracts all of this under the hood with \nvisualize_saliency\n. Lets\ntry to visualize attention over images with: \ntiger, penguin, dumbbell, speedboat, spider\n. Note there is no guarantee\nthat these image urls haven't expired. Update them as needed.\n\n\nimport cv2\nimport numpy as np\n\nfrom keras.preprocessing.image import img_to_array\nfrom vis.utils import utils\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_saliency\n\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Images corresponding to tiger, penguin, dumbbell, speedboat, spider\nimage_paths = [\n    \nhttp://www.tigerfdn.com/wp-content/uploads/2016/05/How-Much-Does-A-Tiger-Weigh.jpg\n,\n    \nhttp://www.slate.com/content/dam/slate/articles/health_and_science/wild_things/2013/10/131025_WILD_AdeliePenguin.jpg.CROP.promo-mediumlarge.jpg\n,\n    \nhttps://www.kshs.org/cool2/graphics/dumbbell1lg.jpg\n,\n    \nhttp://tampaspeedboatadventures.com/wp-content/uploads/2010/10/DSC07011.jpg\n,\n    \nhttp://ichef-1.bbci.co.uk/news/660/cpsprodpb/1C24/production/_85540270_85540265.jpg\n\n]\n\nheatmaps = []\nfor path in image_paths:\n    # Predict the corresponding class for use in `visualize_saliency`.\n    seed_img = utils.load_img(path, target_size=(224, 224))\n    pred_class = np.argmax(model.predict(np.array([img_to_array(seed_img)])))\n\n    # Here we are asking it to show attention such that prob of `pred_class` is maximized.\n    heatmap = visualize_saliency(model, layer_idx, [pred_class], seed_img, text=utils.get_imagenet_label(pred_class))\n    heatmaps.append(heatmap)\n\ncv2.imshow(\nSaliency map\n, utils.stitch_images(heatmaps))\ncv2.waitKey(0)\n\n\n\n\n\nThis generates heatmaps overlayed on top of images. overlay can be turned off using \noverlay=False\n.\n\n\n\n\nThis mostly looks pretty accurate! Note that the heatmap looks pretty sparse as the \nDense\n layers destroys a lot of spatial \ninformation. This visualization should look a lot better if we used 1 X 1 convolutions to \n\nmimic\n the dense layer.\n\n\nClass activation maps\n\n\nAs you might expect, since the inception of saliency maps by Simonyan et al, various other techniques have been developed \nto improve upon these visualizations. One problem with saliency maps is that it is not class discriminative; i.e., there\nis some overlap in heatmaps between, say the 'dog' and 'cat' class. Notable methods to solve this problem includes:  \n\n\n\n\nOcculusion maps\n\n\nClass Activation maps\n\n\n\n\nIn keras-vis, we however adopt the \ngrad-CAM\n method as it solves the inefficiency\nproblem with occlusion maps and architectural constraint problem with CAM.\n\n\nGenerating grad-CAM visualization is simple, just replace \nvisualize_saliency\n with \n\nvisualize_cam\n in the above code. This generates the following:\n\n\n\n\nCompared to saliency, notice how this excludes the spider the in \nspider_web\n prediction. I personally feel that grad-CAM \nis more helpful in diagnosing issues with conv-nets, especially for Kaggle competitions.", 
            "title": "Attention Maps"
        }, 
        {
            "location": "/visualizations/attention/#overview", 
            "text": "Suppose that all the training images of 'bird' class contains a tree with leaves. How do we know that the conv net is \nindeed leveraging bird-related pixels as opposed to some other features such as the tree or leaves in the image.   Attention maps are a family of methods that try to answer these questions by generating a heatmap over input \nimage that most contributed towards maximizing the probability of an output class.", 
            "title": "Overview"
        }, 
        {
            "location": "/visualizations/attention/#saliency-maps", 
            "text": "Saliency maps was first introduced in the paper:  Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps  The idea is pretty simple. We compute the gradient of output category with respect to input image. This should tell us\nhow output category value changes with respect to a small change in input image pixels. All the positive values\nin the gradients tell us that a small change to that pixel will increase the output value. \nHence, visualizing these gradients, which are the same shape as the image should provide some intuition of attention.  keras-vis abstracts all of this under the hood with  visualize_saliency . Lets\ntry to visualize attention over images with:  tiger, penguin, dumbbell, speedboat, spider . Note there is no guarantee\nthat these image urls haven't expired. Update them as needed.  import cv2\nimport numpy as np\n\nfrom keras.preprocessing.image import img_to_array\nfrom vis.utils import utils\nfrom vis.utils.vggnet import VGG16\nfrom vis.visualization import visualize_saliency\n\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights='imagenet', include_top=True)\nprint('Model loaded.')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\n# Images corresponding to tiger, penguin, dumbbell, speedboat, spider\nimage_paths = [\n     http://www.tigerfdn.com/wp-content/uploads/2016/05/How-Much-Does-A-Tiger-Weigh.jpg ,\n     http://www.slate.com/content/dam/slate/articles/health_and_science/wild_things/2013/10/131025_WILD_AdeliePenguin.jpg.CROP.promo-mediumlarge.jpg ,\n     https://www.kshs.org/cool2/graphics/dumbbell1lg.jpg ,\n     http://tampaspeedboatadventures.com/wp-content/uploads/2010/10/DSC07011.jpg ,\n     http://ichef-1.bbci.co.uk/news/660/cpsprodpb/1C24/production/_85540270_85540265.jpg \n]\n\nheatmaps = []\nfor path in image_paths:\n    # Predict the corresponding class for use in `visualize_saliency`.\n    seed_img = utils.load_img(path, target_size=(224, 224))\n    pred_class = np.argmax(model.predict(np.array([img_to_array(seed_img)])))\n\n    # Here we are asking it to show attention such that prob of `pred_class` is maximized.\n    heatmap = visualize_saliency(model, layer_idx, [pred_class], seed_img, text=utils.get_imagenet_label(pred_class))\n    heatmaps.append(heatmap)\n\ncv2.imshow( Saliency map , utils.stitch_images(heatmaps))\ncv2.waitKey(0)  This generates heatmaps overlayed on top of images. overlay can be turned off using  overlay=False .   This mostly looks pretty accurate! Note that the heatmap looks pretty sparse as the  Dense  layers destroys a lot of spatial \ninformation. This visualization should look a lot better if we used 1 X 1 convolutions to  mimic  the dense layer.", 
            "title": "Saliency maps"
        }, 
        {
            "location": "/visualizations/attention/#class-activation-maps", 
            "text": "As you might expect, since the inception of saliency maps by Simonyan et al, various other techniques have been developed \nto improve upon these visualizations. One problem with saliency maps is that it is not class discriminative; i.e., there\nis some overlap in heatmaps between, say the 'dog' and 'cat' class. Notable methods to solve this problem includes:     Occulusion maps  Class Activation maps   In keras-vis, we however adopt the  grad-CAM  method as it solves the inefficiency\nproblem with occlusion maps and architectural constraint problem with CAM.  Generating grad-CAM visualization is simple, just replace  visualize_saliency  with  visualize_cam  in the above code. This generates the following:   Compared to saliency, notice how this excludes the spider the in  spider_web  prediction. I personally feel that grad-CAM \nis more helpful in diagnosing issues with conv-nets, especially for Kaggle competitions.", 
            "title": "Class activation maps"
        }, 
        {
            "location": "/vis.losses/", 
            "text": "Source:\n \nvis/losses.py#L0\n\n\n\n\nLoss\n\n\nAbstract class for defining the loss function to be minimized.\nThe loss function should be built by defining \nbuild_loss\n function.\n\n\nThe attribute \nname\n should be defined to identify loss function with verbose outputs.\nDefaults to 'Unnamed Loss' if not overridden.\n\n\n\n\nLoss.\n__init__\n\n\n__init__(self)\n\n\n\n\n\n\nLoss.build_loss\n\n\nbuild_loss(self)\n\n\n\n\nImplement this function to build the loss function expression.\nAny additional arguments required to build this loss function may be passed in via \n__init__\n.\n\n\nIdeally, the function expression must be compatible with both theano/tensorflow backends with\n'th' or 'tf' image dim ordering. \nutils.slicer\n can be used to define backend agnostic slices\n(just define it for theano, it will automatically shuffle indices for tensorflow).\n\n\n# theano slice\nconv_layer[:, filter_idx, :, :]\n\n# TF slice\nconv_layer[:, :, :, filter_idx]\n\n# Backend agnostic slice\nconv_layer[utils.slicer[:, filter_idx, :, :]]\n\n\n\n\nutils.get_img_shape\n and\n\nutils.get_img_indices\n are other optional utilities that make this easier.\n\n\nReturns:\n\n\nThe loss expression.\n\n\n\n\nActivationMaximization\n\n\nA loss function that maximizes the activation of a set of filters within a particular layer.\n\n\nTypically this loss is used to ask the reverse question - What kind of input image would increase the networks\nconfidence, for say, dog class. This helps determine what the network might be internalizing as being the 'dog'\nimage space.\n\n\nOne might also use this to generate an input image that maximizes both 'dog' and 'human' outputs on the final\n\nkeras.layers.Dense\n layer.\n\n\n\n\nActivationMaximization.\n__init__\n\n\n__init__(self, layer, filter_indices)\n\n\n\n\nArgs:\n\n\n\n\nlayer\n:  The keras layer whose filters need to be maximized. This can either be a convolutional layer\n  or a dense layer.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  For \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\n\n\nIf you are optimizing final \nkeras.layers.Dense\n layer to maximize class output, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nActivationMaximization.build_loss\n\n\nbuild_loss(self)", 
            "title": "losses"
        }, 
        {
            "location": "/vis.losses/#loss", 
            "text": "Abstract class for defining the loss function to be minimized.\nThe loss function should be built by defining  build_loss  function.  The attribute  name  should be defined to identify loss function with verbose outputs.\nDefaults to 'Unnamed Loss' if not overridden.", 
            "title": "Loss"
        }, 
        {
            "location": "/vis.losses/#loss__init__", 
            "text": "__init__(self)", 
            "title": "Loss.__init__"
        }, 
        {
            "location": "/vis.losses/#lossbuild_loss", 
            "text": "build_loss(self)  Implement this function to build the loss function expression.\nAny additional arguments required to build this loss function may be passed in via  __init__ .  Ideally, the function expression must be compatible with both theano/tensorflow backends with\n'th' or 'tf' image dim ordering.  utils.slicer  can be used to define backend agnostic slices\n(just define it for theano, it will automatically shuffle indices for tensorflow).  # theano slice\nconv_layer[:, filter_idx, :, :]\n\n# TF slice\nconv_layer[:, :, :, filter_idx]\n\n# Backend agnostic slice\nconv_layer[utils.slicer[:, filter_idx, :, :]]  utils.get_img_shape  and utils.get_img_indices  are other optional utilities that make this easier.  Returns:  The loss expression.", 
            "title": "Loss.build_loss"
        }, 
        {
            "location": "/vis.losses/#activationmaximization", 
            "text": "A loss function that maximizes the activation of a set of filters within a particular layer.  Typically this loss is used to ask the reverse question - What kind of input image would increase the networks\nconfidence, for say, dog class. This helps determine what the network might be internalizing as being the 'dog'\nimage space.  One might also use this to generate an input image that maximizes both 'dog' and 'human' outputs on the final keras.layers.Dense  layer.", 
            "title": "ActivationMaximization"
        }, 
        {
            "location": "/vis.losses/#activationmaximization__init__", 
            "text": "__init__(self, layer, filter_indices)  Args:   layer :  The keras layer whose filters need to be maximized. This can either be a convolutional layer\n  or a dense layer.  filter_indices :  filter indices within the layer to be maximized.\n  For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.   If you are optimizing final  keras.layers.Dense  layer to maximize class output, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.", 
            "title": "ActivationMaximization.__init__"
        }, 
        {
            "location": "/vis.losses/#activationmaximizationbuild_loss", 
            "text": "build_loss(self)", 
            "title": "ActivationMaximization.build_loss"
        }, 
        {
            "location": "/vis.regularizers/", 
            "text": "Source:\n \nvis/regularizers.py#L0\n\n\n\n\nnormalize\n\n\nnormalize(img, value)\n\n\n\n\nNormalizes the value with respect to image dimensions. This makes regularizer weight factor more or less\nuniform across various input image dimensions.\n\n\nArgs:\n\n\n\n\nimg\n:  4D tensor with shape: \n(samples, channels, rows, cols)\n if dim_ordering='th' or\n  \n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nvalue\n:  The function to normalize\n\n\n\n\nReturns:\n\n\nThe normalized expression.\n\n\n\n\nTotalVariation\n\n\n\n\nTotalVariation.\n__init__\n\n\n__init__(self, img_input, beta=2.0)\n\n\n\n\nTotal variation regularizer encourages blobbier and coherent image structures, akin to natural images.\nSee \nsection 3.2.2\n in\n\nVisualizing deep convolutional neural networks using natural pre-images\n\nfor details.\n\n\nArgs:\n\n\n\n\nimg_input\n:  4D image input tensor to the model of shape: \n(samples, channels, rows, cols)\n\n  if dim_ordering='th' or \n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nbeta\n:  Smaller values of beta give sharper but 'spikier' images.\n  Values \n\\in [1.5, 2.0]\n are recommended as a reasonable compromise.\n\n\n\n\n\n\nTotalVariation.build_loss\n\n\nbuild_loss(self)\n\n\n\n\nImplements the function\n\nTV^{\\beta}(x) = \\sum_{whc} \\left ( \\left ( x(h, w+1, c) - x(h, w, c) \\right )^{2} +\n\\left ( x(h+1, w, c) - x(h, w, c) \\right )^{2} \\right )^{\\frac{\\beta}{2}}\n\n\n\n\n\n\nLPNorm\n\n\n\n\nLPNorm.\n__init__\n\n\n__init__(self, img_input, p=6.0)\n\n\n\n\nBuilds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded.\n  i.e., prevents pixels from taking on very large values.\n\n\nArgs:\n\n\n\n\nimg_input\n:  4D image input tensor to the model of shape: \n(samples, channels, rows, cols)\n\n  if dim_ordering='th' or \n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\np\n:  The pth norm to use. If p = float('inf'), infinity-norm will be used.\n\n\n\n\n\n\nLPNorm.build_loss\n\n\nbuild_loss(self)", 
            "title": "regularizers"
        }, 
        {
            "location": "/vis.regularizers/#normalize", 
            "text": "normalize(img, value)  Normalizes the value with respect to image dimensions. This makes regularizer weight factor more or less\nuniform across various input image dimensions.  Args:   img :  4D tensor with shape:  (samples, channels, rows, cols)  if dim_ordering='th' or\n   (samples, rows, cols, channels)  if dim_ordering='tf'.  value :  The function to normalize   Returns:  The normalized expression.", 
            "title": "normalize"
        }, 
        {
            "location": "/vis.regularizers/#totalvariation", 
            "text": "", 
            "title": "TotalVariation"
        }, 
        {
            "location": "/vis.regularizers/#totalvariation__init__", 
            "text": "__init__(self, img_input, beta=2.0)  Total variation regularizer encourages blobbier and coherent image structures, akin to natural images.\nSee  section 3.2.2  in Visualizing deep convolutional neural networks using natural pre-images \nfor details.  Args:   img_input :  4D image input tensor to the model of shape:  (samples, channels, rows, cols) \n  if dim_ordering='th' or  (samples, rows, cols, channels)  if dim_ordering='tf'.  beta :  Smaller values of beta give sharper but 'spikier' images.\n  Values  \\in [1.5, 2.0]  are recommended as a reasonable compromise.", 
            "title": "TotalVariation.__init__"
        }, 
        {
            "location": "/vis.regularizers/#totalvariationbuild_loss", 
            "text": "build_loss(self)  Implements the function TV^{\\beta}(x) = \\sum_{whc} \\left ( \\left ( x(h, w+1, c) - x(h, w, c) \\right )^{2} +\n\\left ( x(h+1, w, c) - x(h, w, c) \\right )^{2} \\right )^{\\frac{\\beta}{2}}", 
            "title": "TotalVariation.build_loss"
        }, 
        {
            "location": "/vis.regularizers/#lpnorm", 
            "text": "", 
            "title": "LPNorm"
        }, 
        {
            "location": "/vis.regularizers/#lpnorm__init__", 
            "text": "__init__(self, img_input, p=6.0)  Builds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded.\n  i.e., prevents pixels from taking on very large values.  Args:   img_input :  4D image input tensor to the model of shape:  (samples, channels, rows, cols) \n  if dim_ordering='th' or  (samples, rows, cols, channels)  if dim_ordering='tf'.  p :  The pth norm to use. If p = float('inf'), infinity-norm will be used.", 
            "title": "LPNorm.__init__"
        }, 
        {
            "location": "/vis.regularizers/#lpnormbuild_loss", 
            "text": "build_loss(self)", 
            "title": "LPNorm.build_loss"
        }, 
        {
            "location": "/vis.optimizer/", 
            "text": "Source:\n \nvis/optimizer.py#L0\n\n\n\n\nOptimizer\n\n\n\n\nOptimizer.\n__init__\n\n\n__init__(self, img_input, losses, wrt=None)\n\n\n\n\nCreates an optimizer that minimizes weighted loss function.\n\n\nArgs:\n\n\n\n\nimg_input\n:  4D image input tensor to the model of shape: \n(samples, channels, rows, cols)\n\n  if dim_ordering='th' or \n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nlosses\n:  List of (\nLoss\n, weight) tuples.\n\n\nwrt\n:  Short for, with respect to. This instructs the optimizer that the aggregate loss from \nlosses\n\n  should be minimized with respect to \nwrt\n. \nwrt\n can be any tensor that is part of the model graph.\n  Default value is set to None which means that loss will simply be minimized with respect to \nimg_input\n.\n\n\n\n\n\n\nOptimizer.eval_losses\n\n\neval_losses(self, img)\n\n\n\n\nEvaluates losses with respect to numpy input image.\n\n\nArgs:\n\n\n\n\nimg\n:  4D numpy array with shape: \n(samples, channels, rows, cols)\n if dim_ordering='th' or\n  \n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\n\n\nReturns:\n\n\nA dictionary of (\nLoss\n.name, loss_value) values for various losses.\n\n\n\n\nOptimizer.get_seed_img\n\n\nget_seed_img(self, seed_img)\n\n\n\n\nCreates the seed_img, along with other sanity checks.\n\n\n\n\nOptimizer.jitter\n\n\njitter(self, img, jitter=32)\n\n\n\n\nJitters the numpy input image randomly in width and height dimensions.\nThis kind of regularization is known to produce crisper images via guided backprop.\n\n\nArgs:\n\n\n\n\nimg\n:  4D numpy array with shape: \n(samples, channels, rows, cols)\n if dim_ordering='th' or\n  \n(samples, rows, cols, channels)\n if dim_ordering='tf'\n\n\njitter\n:  Number of pixels to jitter in width and height directions.\n\n\n\n\nReturns:\n\n\nThe jittered numpy image array.\n\n\n\n\nOptimizer.minimize\n\n\nminimize(self, seed_img=None, max_iter=200, jitter=8, verbose=True, progress_gif_path=None)\n\n\n\n\nPerforms gradient descent on the input image with respect to defined losses.\n\n\nArgs:\n\n\n\n\nseed_img\n:  3D numpy array with shape: \n(channels, rows, cols)\n if dim_ordering='th' or\n  \n(rows, cols, channels)\n if dim_ordering='tf'.\n  Seeded with random noise if set to None. (Default value = None)\n\n\nmax_iter\n:  The maximum number of gradient descent iterations. (Default value = 200)\n\n\njitter\n:  The number of pixels to jitter between subsequent gradient descent iterations.\n  Jitter is known to generate crisper images. (Default value = 8)\n\n\nverbose\n:  Logs individual losses at the end of every gradient descent iteration.\n  Very useful to estimate loss weight factor. (Default value = True)\n\n\nprogress_gif_path\n:  Saves a gif of \nseed_img\n being optimized.\n  This slows down perf quite a bit, use with caution.\n\n\n\n\nReturns:\n\n\nThe tuple of \n(optimized_image, grads with respect to wrt, wrt_value)\n after gradient descent iterations.\n\n\n\n\nOptimizer.rmsprop\n\n\nrmsprop(self, grads, cache=None, decay_rate=0.95)\n\n\n\n\nUses RMSProp to compute step from gradients.\n\n\nArgs:\n\n\n\n\ngrads\n:  numpy array of gradients.\n\n\ncache\n:  numpy array of same shape as \ngrads\n as RMSProp cache\n\n\ndecay_rate\n:  How fast to decay cache\n\n\n\n\nReturns:\n\n\nA tuple of\n - \nstep\n:  numpy array of the same shape as \ngrads\n giving the step.\n  Note that this does not yet take the learning rate into account.\n - \ncache\n:  Updated RMSProp cache.", 
            "title": "optimizer"
        }, 
        {
            "location": "/vis.optimizer/#optimizer", 
            "text": "", 
            "title": "Optimizer"
        }, 
        {
            "location": "/vis.optimizer/#optimizer__init__", 
            "text": "__init__(self, img_input, losses, wrt=None)  Creates an optimizer that minimizes weighted loss function.  Args:   img_input :  4D image input tensor to the model of shape:  (samples, channels, rows, cols) \n  if dim_ordering='th' or  (samples, rows, cols, channels)  if dim_ordering='tf'.  losses :  List of ( Loss , weight) tuples.  wrt :  Short for, with respect to. This instructs the optimizer that the aggregate loss from  losses \n  should be minimized with respect to  wrt .  wrt  can be any tensor that is part of the model graph.\n  Default value is set to None which means that loss will simply be minimized with respect to  img_input .", 
            "title": "Optimizer.__init__"
        }, 
        {
            "location": "/vis.optimizer/#optimizereval_losses", 
            "text": "eval_losses(self, img)  Evaluates losses with respect to numpy input image.  Args:   img :  4D numpy array with shape:  (samples, channels, rows, cols)  if dim_ordering='th' or\n   (samples, rows, cols, channels)  if dim_ordering='tf'.   Returns:  A dictionary of ( Loss .name, loss_value) values for various losses.", 
            "title": "Optimizer.eval_losses"
        }, 
        {
            "location": "/vis.optimizer/#optimizerget_seed_img", 
            "text": "get_seed_img(self, seed_img)  Creates the seed_img, along with other sanity checks.", 
            "title": "Optimizer.get_seed_img"
        }, 
        {
            "location": "/vis.optimizer/#optimizerjitter", 
            "text": "jitter(self, img, jitter=32)  Jitters the numpy input image randomly in width and height dimensions.\nThis kind of regularization is known to produce crisper images via guided backprop.  Args:   img :  4D numpy array with shape:  (samples, channels, rows, cols)  if dim_ordering='th' or\n   (samples, rows, cols, channels)  if dim_ordering='tf'  jitter :  Number of pixels to jitter in width and height directions.   Returns:  The jittered numpy image array.", 
            "title": "Optimizer.jitter"
        }, 
        {
            "location": "/vis.optimizer/#optimizerminimize", 
            "text": "minimize(self, seed_img=None, max_iter=200, jitter=8, verbose=True, progress_gif_path=None)  Performs gradient descent on the input image with respect to defined losses.  Args:   seed_img :  3D numpy array with shape:  (channels, rows, cols)  if dim_ordering='th' or\n   (rows, cols, channels)  if dim_ordering='tf'.\n  Seeded with random noise if set to None. (Default value = None)  max_iter :  The maximum number of gradient descent iterations. (Default value = 200)  jitter :  The number of pixels to jitter between subsequent gradient descent iterations.\n  Jitter is known to generate crisper images. (Default value = 8)  verbose :  Logs individual losses at the end of every gradient descent iteration.\n  Very useful to estimate loss weight factor. (Default value = True)  progress_gif_path :  Saves a gif of  seed_img  being optimized.\n  This slows down perf quite a bit, use with caution.   Returns:  The tuple of  (optimized_image, grads with respect to wrt, wrt_value)  after gradient descent iterations.", 
            "title": "Optimizer.minimize"
        }, 
        {
            "location": "/vis.optimizer/#optimizerrmsprop", 
            "text": "rmsprop(self, grads, cache=None, decay_rate=0.95)  Uses RMSProp to compute step from gradients.  Args:   grads :  numpy array of gradients.  cache :  numpy array of same shape as  grads  as RMSProp cache  decay_rate :  How fast to decay cache   Returns:  A tuple of\n -  step :  numpy array of the same shape as  grads  giving the step.\n  Note that this does not yet take the learning rate into account.\n -  cache :  Updated RMSProp cache.", 
            "title": "Optimizer.rmsprop"
        }, 
        {
            "location": "/vis.visualization/", 
            "text": "Source:\n \nvis/visualization.py#L0\n\n\n\n\nget_num_filters\n\n\nget_num_filters(layer)\n\n\n\n\nReturns: Total number of filters within \nlayer\n.\n\n\nFor \nkeras.layers.Dense\n layer, this is the total number of outputs.\n\n\n\n\nvisualize_activation\n\n\nvisualize_activation(model, layer_idx, filter_indices=None, seed_img=None, text=None, \\\n    act_max_weight=1, lp_norm_weight=10, tv_weight=10, **optimizer_params)\n\n\n\n\nGenerates stitched input image(s) over all \nfilter_indices\n in the given \nlayer\n that maximize\nthe filter output activation.\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance. Model input is expected to be a 4D image input of shape:\n  \n(samples, channels, rows, cols)\n if dim_ordering='th' or \n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nlayer_idx\n:  The layer index within \nmodel.layers\n whose filters needs to be visualized.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  For \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\n\n\nIf you are visualizing final \nkeras.layers.Dense\n layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\nfilter indices within the layer to be maximized.\n  If None, all filters are visualized. (Default value = None)\n\n\nAn input image is generated for each entry in \nfilter_indices\n. The entry can also be an array.\n  For example, \nfilter_indices = [[1, 2], 3, [4, 5, 6]]\n would generate three input images. The first one\n  would maximize output of filters 1, 2, 3 jointly. A fun use of this might be to generate a dog-fish\n  image by maximizing 'dog' and 'fish' output in final \nDense\n layer.\n\n\nFor \nkeras.layers.Dense\n layers, \nfilter_idx\n is interpreted as the output index.\n\n\nIf you are visualizing final \nkeras.layers.Dense\n layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nseed_img\n:  Seeds the optimization with a starting image. Initialized with a random value when set to None.\n  (Default value = None)\n\n\ntext\n:  The text to overlay on top of the generated image. (Default Value = None)\n\n\nact_max_weight\n:  The weight param for \nActivationMaximization\n loss. Not used if 0 or None. (Default value = 1)\n\n\nlp_norm_weight\n:  The weight param for \nLPNorm\n regularization loss. Not used if 0 or None. (Default value = 10)\n\n\ntv_weight\n:  The weight param for \nTotalVariation\n regularization loss. Not used if 0 or None. (Default value = 10)\n\n\noptimizer_params\n:  The **kwargs for optimizer \nparams\n. Will default to\n  reasonable values when required keys are not found.\n\n\n\n\nExample:\n\n\nIf you wanted to visualize the input image that would maximize the output index 22, say on\nfinal \nkeras.layers.Dense\n layer, then, \nfilter_indices = [22]\n, \nlayer = dense_layer\n.\n\n\nIf \nfilter_indices = [22, 23]\n, then it should generate an input image that shows features of both classes.\n\n\nReturns:\n\n\nStitched image output visualizing input images that maximize the filter output(s). (Default value = 10)\n\n\n\n\nvisualize_saliency\n\n\nvisualize_saliency(model, layer_idx, filter_indices, seed_img, text=None, overlay=True)\n\n\n\n\nGenerates an attention heatmap over the \nseed_img\n for maximizing \nfilter_indices\n output in the given \nlayer\n.\nFor a full description of saliency, see the paper:\n\nDeep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance. Model input is expected to be a 4D image input of shape:\n  \n(samples, channels, rows, cols)\n if dim_ordering='th' or \n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nlayer_idx\n:  The layer index within \nmodel.layers\n whose filters needs to be visualized.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  For \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\n\n\nIf you are visualizing final \nkeras.layers.Dense\n layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nseed_img\n:  The input image for which activation map needs to be visualized.\n\n\ntext\n:  The text to overlay on top of the generated image. (Default Value = None)\n\n\noverlay\n:  If true, overlays the heatmap over the original image (Default value = True)\n\n\n\n\nExample:\n\n\nIf you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal \nkeras.layers.Dense\n layer, then, \nfilter_indices = [22]\n, \nlayer = dense_layer\n.\n\n\nOne could also set filter indices to more than one value. For example, \nfilter_indices = [22, 23]\n should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.\n\n\nReturns:\n\n\nThe heatmap image indicating image regions that, when changed, would contribute the most towards maximizing\na the filter output.\n\n\n\n\nvisualize_cam\n\n\nvisualize_cam(model, layer_idx, filter_indices, seed_img, penultimate_layer_idx=None, text=None, \\\n    overlay=True)\n\n\n\n\nGenerates a gradient based class activation map (CAM) as described in paper\n\nGrad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization\n.\nUnlike \nclass activation mapping\n, which requires minor changes to\nnetwork architecture in some instances, grad-CAM has a more general applicability.\n\n\nCompared to saliency maps, grad-CAM is class discriminative; i.e., the 'cat' explanation exclusively highlights\ncat regions and not the 'dog' region and vice-versa.\n\n\nArgs:\n\n\n\n\nmodel\n:  The \nkeras.models.Model\n instance. Model input is expected to be a 4D image input of shape:\n  \n(samples, channels, rows, cols)\n if dim_ordering='th' or \n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nlayer_idx\n:  The layer index within \nmodel.layers\n whose filters needs to be visualized.\n\n\nfilter_indices\n:  filter indices within the layer to be maximized.\n  For \nkeras.layers.Dense\n layer, \nfilter_idx\n is interpreted as the output index.\n\n\n\n\nIf you are visualizing final \nkeras.layers.Dense\n layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.\n\n\n\n\nseed_img\n:  The input image for which activation map needs to be visualized.\n\n\npenultimate_layer_idx\n:  The pre-layer to \nlayer_idx\n whose feature maps should be used to compute gradients\n  wrt filter output. If not provided, it is set to the nearest penultimate \nConvolutional\n or \nPooling\n layer.\n\n\ntext\n:  The text to overlay on top of the generated image. (Default Value = None)\n\n\noverlay\n:  If true, overlays the heatmap over the original image (Default value = True)\n\n\n\n\nExample:\n\n\nIf you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal \nkeras.layers.Dense\n layer, then, \nfilter_indices = [22]\n, \nlayer = dense_layer\n.\n\n\nOne could also set filter indices to more than one value. For example, \nfilter_indices = [22, 23]\n should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.\n\n\nNotes:\n\n\nThis technique deprecates occlusion maps as it gives similar results, but with one-pass gradient computation\nas opposed inefficient sliding window approach.\n\n\nReturns:\n\n\nThe heatmap image indicating image regions that, when changed, would contribute the most towards maximizing\na the filter output.", 
            "title": "visualization"
        }, 
        {
            "location": "/vis.visualization/#get_num_filters", 
            "text": "get_num_filters(layer)  Returns: Total number of filters within  layer .  For  keras.layers.Dense  layer, this is the total number of outputs.", 
            "title": "get_num_filters"
        }, 
        {
            "location": "/vis.visualization/#visualize_activation", 
            "text": "visualize_activation(model, layer_idx, filter_indices=None, seed_img=None, text=None, \\\n    act_max_weight=1, lp_norm_weight=10, tv_weight=10, **optimizer_params)  Generates stitched input image(s) over all  filter_indices  in the given  layer  that maximize\nthe filter output activation.  Args:   model :  The  keras.models.Model  instance. Model input is expected to be a 4D image input of shape:\n   (samples, channels, rows, cols)  if dim_ordering='th' or  (samples, rows, cols, channels)  if dim_ordering='tf'.  layer_idx :  The layer index within  model.layers  whose filters needs to be visualized.  filter_indices :  filter indices within the layer to be maximized.\n  For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.   If you are visualizing final  keras.layers.Dense  layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.  filter indices within the layer to be maximized.\n  If None, all filters are visualized. (Default value = None)  An input image is generated for each entry in  filter_indices . The entry can also be an array.\n  For example,  filter_indices = [[1, 2], 3, [4, 5, 6]]  would generate three input images. The first one\n  would maximize output of filters 1, 2, 3 jointly. A fun use of this might be to generate a dog-fish\n  image by maximizing 'dog' and 'fish' output in final  Dense  layer.  For  keras.layers.Dense  layers,  filter_idx  is interpreted as the output index.  If you are visualizing final  keras.layers.Dense  layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.   seed_img :  Seeds the optimization with a starting image. Initialized with a random value when set to None.\n  (Default value = None)  text :  The text to overlay on top of the generated image. (Default Value = None)  act_max_weight :  The weight param for  ActivationMaximization  loss. Not used if 0 or None. (Default value = 1)  lp_norm_weight :  The weight param for  LPNorm  regularization loss. Not used if 0 or None. (Default value = 10)  tv_weight :  The weight param for  TotalVariation  regularization loss. Not used if 0 or None. (Default value = 10)  optimizer_params :  The **kwargs for optimizer  params . Will default to\n  reasonable values when required keys are not found.   Example:  If you wanted to visualize the input image that would maximize the output index 22, say on\nfinal  keras.layers.Dense  layer, then,  filter_indices = [22] ,  layer = dense_layer .  If  filter_indices = [22, 23] , then it should generate an input image that shows features of both classes.  Returns:  Stitched image output visualizing input images that maximize the filter output(s). (Default value = 10)", 
            "title": "visualize_activation"
        }, 
        {
            "location": "/vis.visualization/#visualize_saliency", 
            "text": "visualize_saliency(model, layer_idx, filter_indices, seed_img, text=None, overlay=True)  Generates an attention heatmap over the  seed_img  for maximizing  filter_indices  output in the given  layer .\nFor a full description of saliency, see the paper: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps  Args:   model :  The  keras.models.Model  instance. Model input is expected to be a 4D image input of shape:\n   (samples, channels, rows, cols)  if dim_ordering='th' or  (samples, rows, cols, channels)  if dim_ordering='tf'.  layer_idx :  The layer index within  model.layers  whose filters needs to be visualized.  filter_indices :  filter indices within the layer to be maximized.\n  For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.   If you are visualizing final  keras.layers.Dense  layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.   seed_img :  The input image for which activation map needs to be visualized.  text :  The text to overlay on top of the generated image. (Default Value = None)  overlay :  If true, overlays the heatmap over the original image (Default value = True)   Example:  If you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal  keras.layers.Dense  layer, then,  filter_indices = [22] ,  layer = dense_layer .  One could also set filter indices to more than one value. For example,  filter_indices = [22, 23]  should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.  Returns:  The heatmap image indicating image regions that, when changed, would contribute the most towards maximizing\na the filter output.", 
            "title": "visualize_saliency"
        }, 
        {
            "location": "/vis.visualization/#visualize_cam", 
            "text": "visualize_cam(model, layer_idx, filter_indices, seed_img, penultimate_layer_idx=None, text=None, \\\n    overlay=True)  Generates a gradient based class activation map (CAM) as described in paper Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization .\nUnlike  class activation mapping , which requires minor changes to\nnetwork architecture in some instances, grad-CAM has a more general applicability.  Compared to saliency maps, grad-CAM is class discriminative; i.e., the 'cat' explanation exclusively highlights\ncat regions and not the 'dog' region and vice-versa.  Args:   model :  The  keras.models.Model  instance. Model input is expected to be a 4D image input of shape:\n   (samples, channels, rows, cols)  if dim_ordering='th' or  (samples, rows, cols, channels)  if dim_ordering='tf'.  layer_idx :  The layer index within  model.layers  whose filters needs to be visualized.  filter_indices :  filter indices within the layer to be maximized.\n  For  keras.layers.Dense  layer,  filter_idx  is interpreted as the output index.   If you are visualizing final  keras.layers.Dense  layer, you tend to get\n  better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax'\n  output can be maximized by minimizing scores for other classes.   seed_img :  The input image for which activation map needs to be visualized.  penultimate_layer_idx :  The pre-layer to  layer_idx  whose feature maps should be used to compute gradients\n  wrt filter output. If not provided, it is set to the nearest penultimate  Convolutional  or  Pooling  layer.  text :  The text to overlay on top of the generated image. (Default Value = None)  overlay :  If true, overlays the heatmap over the original image (Default value = True)   Example:  If you wanted to visualize attention over 'bird' category, say output index 22 on the\nfinal  keras.layers.Dense  layer, then,  filter_indices = [22] ,  layer = dense_layer .  One could also set filter indices to more than one value. For example,  filter_indices = [22, 23]  should\n(hopefully) show attention map that corresponds to both 22, 23 output categories.  Notes:  This technique deprecates occlusion maps as it gives similar results, but with one-pass gradient computation\nas opposed inefficient sliding window approach.  Returns:  The heatmap image indicating image regions that, when changed, would contribute the most towards maximizing\na the filter output.", 
            "title": "visualize_cam"
        }, 
        {
            "location": "/vis.utils.utils/", 
            "text": "Source:\n \nvis/utils/utils.py#L0\n\n\nGlobal Variables\n\n\n\n\nslicer\n\n\n\n\n\n\nset_random_seed\n\n\nset_random_seed(seed_value=1337)\n\n\n\n\nSets random seed value for reproducibility.\n\n\nArgs:\n\n\n\n\nseed_value\n:  The seed value to use. (Default Value = infamous 1337)\n\n\n\n\n\n\nreverse_enumerate\n\n\nreverse_enumerate(iterable)\n\n\n\n\nEnumerate over an iterable in reverse order while retaining proper indexes, without creating any copies.\n\n\n\n\nlistify\n\n\nlistify(value)\n\n\n\n\nEnsures that the value is a list. If it is not a list, it creates a new list with \nvalue\n as an item within it.\n\n\n\n\ndeprocess_image\n\n\ndeprocess_image(img)\n\n\n\n\nUtility function to convert optimized image output into a valid image.\n\n\nArgs:\n\n\n\n\nimg\n:  3D numpy array with shape: \n(channels, rows, cols)\n if dim_ordering='th' or\n  \n(rows, cols, channels)\n if dim_ordering='tf'.\n\n\n\n\nReturns:\n\n\nA valid image output.\n\n\n\n\nstitch_images\n\n\nstitch_images(images, margin=5, cols=5)\n\n\n\n\nUtility function to stitch images together with a \nmargin\n.\n\n\nArgs:\n\n\n\n\nimages\n:  The array of images to stitch.\n\n\nmargin\n:  The black border margin size between images (Default value = 5)\n\n\ncols\n:  Max number of image cols. New row is created when number of images exceed the column size.\n  (Default value = 5)\n\n\n\n\nReturns:\n\n\nA single numpy image array comprising of input images.\n\n\n\n\ngenerate_rand_img\n\n\ngenerate_rand_img(ch, rows, cols)\n\n\n\n\nGenerates a random image.\n\n\nArgs:\n\n\n\n\nch\n:  image channels\n\n\nrows\n:  image rows or height\n\n\ncols\n:  image cols or width\n\n\n\n\nReturns:\n\n\nA numpy array of shape: \n(channels, rows, cols)\n if dim_ordering='th' or\n  \n(rows, cols, channels)\n if dim_ordering='tf'.\n\n\n\n\nget_img_shape\n\n\nget_img_shape(img)\n\n\n\n\nReturns image shape in a backend agnostic manner.\n\n\nArgs:\n\n\n\n\nimg\n:  The image tensor in 'th' or 'tf' dim ordering.\n\n\n\n\nReturns:\n\n\nTuple containing image shape information in \n(samples, channels, rows, cols)\n order.\n\n\n\n\nget_img_indices\n\n\nget_img_indices()\n\n\n\n\nReturns image indices in a backend agnostic manner.\n\n\nReturns:\n\n\nA tuple representing indices for image in \n(samples, channels, rows, cols)\n order.\n\n\n\n\nload_img\n\n\nload_img(path, grayscale=False, target_size=None)\n\n\n\n\nUtility function to load an image from disk.\n\n\nArgs:\n\n\n\n\npath\n:  The image file path.\n\n\ngrayscale\n:  True to convert to grayscale image (Default value = False)\n\n\ntarget_size\n:  (w, h) to resize. (Default value = None)\n\n\n\n\nReturns:\n\n\nThe loaded numpy image.\n\n\n\n\nget_imagenet_label\n\n\nget_imagenet_label(indices, join=\n, \n)\n\n\n\n\nUtility function to return the image net label for the final \ndense\n layer output index.\n\n\nArgs:\n\n\n\n\nindices\n:  Could be a single value or an array of indices whose labels needs looking up.\n\n\njoin\n:  When multiple indices are passed, the output labels are joined using this value. (Default Value = ', ')\n\n\n\n\nReturns:\n\n\nImage net label corresponding to the image category.", 
            "title": "utils"
        }, 
        {
            "location": "/vis.utils.utils/#global-variables", 
            "text": "slicer", 
            "title": "Global Variables"
        }, 
        {
            "location": "/vis.utils.utils/#set_random_seed", 
            "text": "set_random_seed(seed_value=1337)  Sets random seed value for reproducibility.  Args:   seed_value :  The seed value to use. (Default Value = infamous 1337)", 
            "title": "set_random_seed"
        }, 
        {
            "location": "/vis.utils.utils/#reverse_enumerate", 
            "text": "reverse_enumerate(iterable)  Enumerate over an iterable in reverse order while retaining proper indexes, without creating any copies.", 
            "title": "reverse_enumerate"
        }, 
        {
            "location": "/vis.utils.utils/#listify", 
            "text": "listify(value)  Ensures that the value is a list. If it is not a list, it creates a new list with  value  as an item within it.", 
            "title": "listify"
        }, 
        {
            "location": "/vis.utils.utils/#deprocess_image", 
            "text": "deprocess_image(img)  Utility function to convert optimized image output into a valid image.  Args:   img :  3D numpy array with shape:  (channels, rows, cols)  if dim_ordering='th' or\n   (rows, cols, channels)  if dim_ordering='tf'.   Returns:  A valid image output.", 
            "title": "deprocess_image"
        }, 
        {
            "location": "/vis.utils.utils/#stitch_images", 
            "text": "stitch_images(images, margin=5, cols=5)  Utility function to stitch images together with a  margin .  Args:   images :  The array of images to stitch.  margin :  The black border margin size between images (Default value = 5)  cols :  Max number of image cols. New row is created when number of images exceed the column size.\n  (Default value = 5)   Returns:  A single numpy image array comprising of input images.", 
            "title": "stitch_images"
        }, 
        {
            "location": "/vis.utils.utils/#generate_rand_img", 
            "text": "generate_rand_img(ch, rows, cols)  Generates a random image.  Args:   ch :  image channels  rows :  image rows or height  cols :  image cols or width   Returns:  A numpy array of shape:  (channels, rows, cols)  if dim_ordering='th' or\n   (rows, cols, channels)  if dim_ordering='tf'.", 
            "title": "generate_rand_img"
        }, 
        {
            "location": "/vis.utils.utils/#get_img_shape", 
            "text": "get_img_shape(img)  Returns image shape in a backend agnostic manner.  Args:   img :  The image tensor in 'th' or 'tf' dim ordering.   Returns:  Tuple containing image shape information in  (samples, channels, rows, cols)  order.", 
            "title": "get_img_shape"
        }, 
        {
            "location": "/vis.utils.utils/#get_img_indices", 
            "text": "get_img_indices()  Returns image indices in a backend agnostic manner.  Returns:  A tuple representing indices for image in  (samples, channels, rows, cols)  order.", 
            "title": "get_img_indices"
        }, 
        {
            "location": "/vis.utils.utils/#load_img", 
            "text": "load_img(path, grayscale=False, target_size=None)  Utility function to load an image from disk.  Args:   path :  The image file path.  grayscale :  True to convert to grayscale image (Default value = False)  target_size :  (w, h) to resize. (Default value = None)   Returns:  The loaded numpy image.", 
            "title": "load_img"
        }, 
        {
            "location": "/vis.utils.utils/#get_imagenet_label", 
            "text": "get_imagenet_label(indices, join= ,  )  Utility function to return the image net label for the final  dense  layer output index.  Args:   indices :  Could be a single value or an array of indices whose labels needs looking up.  join :  When multiple indices are passed, the output labels are joined using this value. (Default Value = ', ')   Returns:  Image net label corresponding to the image category.", 
            "title": "get_imagenet_label"
        }, 
        {
            "location": "/vis.utils.vggnet/", 
            "text": "Source:\n \nvis/utils/vggnet.py#L0\n\n\nGlobal Variables\n\n\n\n\nTH_WEIGHTS_PATH\n\n\nTF_WEIGHTS_PATH\n\n\nTH_WEIGHTS_PATH_NO_TOP\n\n\nTF_WEIGHTS_PATH_NO_TOP\n\n\n\n\n\n\nVGG16\n\n\nVGG16(include_top=True, weights=\nimagenet\n, input_tensor=None)\n\n\n\n\nInstantiate the VGG16 architecture,\noptionally loading weights pre-trained\non ImageNet. Note that when using TensorFlow,\nfor best performance you should set\n\nimage_dim_ordering=\"tf\"\n in your Keras config\nat ~/.keras/keras.json.\n\n\nThe model and the weights are compatible with both\nTensorFlow and Theano. The dimension ordering\nconvention used by the model is the one\nspecified in your Keras config file.\n\n\nArgs:\n\n\n\n\ninclude_top\n:  whether to include the 3 fully-connected\n  layers at the top of the network.\n\n\nweights\n:  one of \nNone\n (random initialization)\n  or \"imagenet\" (pre-training on ImageNet).\n\n\ninput_tensor\n:  optional Keras tensor (i.e. output of \nlayers.Input()\n)\n  to use as image input for the model.\n\n\n\n\nReturns:\n\n\nA Keras model instance.", 
            "title": "vggnet"
        }, 
        {
            "location": "/vis.utils.vggnet/#global-variables", 
            "text": "TH_WEIGHTS_PATH  TF_WEIGHTS_PATH  TH_WEIGHTS_PATH_NO_TOP  TF_WEIGHTS_PATH_NO_TOP", 
            "title": "Global Variables"
        }, 
        {
            "location": "/vis.utils.vggnet/#vgg16", 
            "text": "VGG16(include_top=True, weights= imagenet , input_tensor=None)  Instantiate the VGG16 architecture,\noptionally loading weights pre-trained\non ImageNet. Note that when using TensorFlow,\nfor best performance you should set image_dim_ordering=\"tf\"  in your Keras config\nat ~/.keras/keras.json.  The model and the weights are compatible with both\nTensorFlow and Theano. The dimension ordering\nconvention used by the model is the one\nspecified in your Keras config file.  Args:   include_top :  whether to include the 3 fully-connected\n  layers at the top of the network.  weights :  one of  None  (random initialization)\n  or \"imagenet\" (pre-training on ImageNet).  input_tensor :  optional Keras tensor (i.e. output of  layers.Input() )\n  to use as image input for the model.   Returns:  A Keras model instance.", 
            "title": "VGG16"
        }
    ]
}